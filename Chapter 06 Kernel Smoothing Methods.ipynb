{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 6.2 \n",
    "\n",
    "Define the vector-valued function $b(x)^T=(1,\\,x,\\,\\dots\\,x^k)$. Let $\\mathbf{B}$ be the matrix with $i$th row $b(x_i)^T$.\n",
    "\n",
    "Let's expand the polynomial: $(x_i-x_0)^j$:\n",
    "\n",
    "\\begin{align*}\n",
    "(x_{i}-x_{0})^{j} & =\\Sigma_{m=0}^{j}\\alpha_{n}\\left(x_{0}\\right)x_{i}^{m}\\\\\n",
    " & =b\\left(x_{i}\\right)^{T}\\alpha\\left(x_{0}\\right),\\\\\n",
    "\\mbox{where }\\alpha\\left(x_{0}\\right) & =\\left[\\alpha_{0}\\left(x_{0}\\right),\\,\\dots,\\,\\alpha_{j}\\left(x_{0}\\right)\\right],^{T}\\\\\n",
    "\\alpha_{m}\\left(x_{0}\\right) & ={j \\choose j-m}\\left(-x_{0}\\right)^{j-m}.\n",
    "\\end{align*}\n",
    "\n",
    "Let's define the vector-value: $\\left(x-x_{0}\\right)^{j}=\\left[\\left(x_{1}-x_{0}\\right)^{j},\\,\\dots,\\,\\left(x_{n}-x_{0}\\right)^{j}\\right]^{T}.$ \n",
    "\n",
    "Then it is obvious that:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{B}\\alpha\\left(x_{0}\\right) & = & \\left[b\\left(x_{1}\\right),\\,\\dots,\\,b\\left(x_{n}\\right)\\right]^{T}\\alpha\\left(x_{0}\\right)\\\\\n",
    " & = & \\left(\\alpha\\left(x_{0}\\right)^{T}\\left[b\\left(x_{1}\\right),\\,\\dots,\\,b\\left(x_{n}\\right)\\right]\\right)^{T}\\\\\n",
    " & = & \\left[\\alpha\\left(x_{0}\\right)^{T}b\\left(x_{1}\\right),\\,\\dots,\\,\\alpha\\left(x_{0}\\right)^{T}b\\left(x_{n}\\right)\\right]^{T}\\\\\n",
    " & = & \\left[\\left(x_{1}-x_{0}\\right)^{j},\\,\\dots,\\,\\left(x_{n}-x_{0}\\right)^{j}\\right]^{T}.\\\\\n",
    " & = & \\left(x-x_{0}\\right)^{j}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "For this $k$th order polynomial regression, we also have:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\hat{f}\\left(x_{0}\\right) & = & b\\left(x_{0}\\right)^{T}\\left(\\mathbf{B}^{T}\\mathbf{WB}\\right)^{-1}\\mathbf{B}^{T}\\mathbf{Wy}\\\\\n",
    " & = & l\\left(x_{0}\\right)\\mathbf{y},\\\\\n",
    "\\mbox{where }l\\left(x_{0}\\right) & = & b\\left(x_{0}\\right)^{T}\\left(\\mathbf{B}^{T}\\mathbf{WB}\\right)^{-1}\\mathbf{B}^{T}\\mathbf{W}\\\\\n",
    "\\mathbf{B}\\alpha\\left(x_{0}\\right) & = & \\left(x-x_{0}\\right)^{j}\\\\\n",
    "\\Rightarrow\\mathbf{B}^{T}\\mathbf{W}\\mathbf{B}\\alpha\\left(x_{0}\\right) & = & \\mathbf{B}^{T}\\mathbf{W}\\left(x-x_{0}\\right)^{j}\\\\\n",
    "\\Rightarrow\\alpha\\left(x_{0}\\right) & = & \\left(\\mathbf{B}^{T}\\mathbf{WB}\\right)^{-1}\\mathbf{B}^{T}\\mathbf{W}\\left(x-x_{0}\\right)^{j}\\\\\n",
    "\\Rightarrow l\\left(x_{0}\\right)\\left(x-x_{0}\\right)^{j} & = & b\\left(x_{0}\\right)^{T}\\left(\\mathbf{B}^{T}\\mathbf{WB}\\right)^{-1}\\mathbf{B}^{T}\\mathbf{W}\\left(x-x_{0}\\right)^{j}\\\\\n",
    " & = & b\\left(x_{0}\\right)^{T}\\alpha\\left(x_{0}\\right)\\\\\n",
    " & = & \\left(x_{0}-x_{0}\\right)^{j}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "This completes the proof that $b_0(x_0)=1$ and $b_j(x_0)=0$ for all $j\\in \\{1,\\,\\dots,\\,k\\}$.\n",
    "\n",
    "By the Taylor expanse (6.10) in the book, we know that for local polynormial regresion of order $k$, the bias is in the order of the $k+1$ order derivative of the true model $f$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
