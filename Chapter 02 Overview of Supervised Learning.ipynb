{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.1\n",
    "\n",
    "First we need to assume that the norm here is Euclidean norm $||\\cdot||_2$, we also believe this is the case for any norm appearing in this book. The statement in this problem is true under Euclidean norm, but not necessarily true for other norms. Let's consider two vectors in $v_1, v_2 \\in \\mathbb{R}^2 $, where $ v_1 = [4,4]^T, v_2 = [6,1]^T $. We can see that:\n",
    "\n",
    "\\begin{align*}\n",
    "  ||v_1||_1 >& \\,||v_2||_1 \\\\ \n",
    "  ||v_1||_2 <& \\,||v_2||_2 .\n",
    "\\end{align*}\n",
    "\n",
    "This indicates that for a given point, the nearest neighbor is not invariant with different choices of norms.\n",
    "  \n",
    "Let's come back to prove the statment in this question:\n",
    "$$\\mathrm{argmin}_k\\left(\\left\\Vert t_k - \\hat{y}\\right\\Vert_2\\right) = \\mathrm{argmax}_k\\left(\\hat{y}_k\\right)$$\n",
    "**Proof**\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathrm{argmin}_k\\left(\\left\\Vert t_k - \\hat{y}\\right\\Vert_2\\right) \n",
    "  =&\\mathrm{argmin}_k\\left(\\left\\Vert t_k - \\hat{y}\\right\\Vert_2^2\\right)\\\\\n",
    "  =&\\mathrm{argmin}_k\\sum_{i=1}^K\\left( t_{k\\,i} - \\hat{y}_i\\right)^2 \\\\\n",
    "  =&\\mathrm{argmin}_k\\sum_{i=1}^K\\left(t_{k\\,i}^2 + \\hat{y}_i^2 - 2t_{k\\,i}\\hat{y}_i\\right)\\\\\n",
    "  =&\\mathrm{argmin}_k\\left(1+\\left\\Vert\\hat{y}\\right\\Vert_2^2-2\\sum_{i=1}^Kt_{k\\,i}\\hat{y}_i\\right)\\\\\n",
    "  =&\\mathrm{argmin}_k\\left(1+\\left\\Vert\\hat{y}\\right\\Vert_2^2-2\\hat{y}_k\\right) \\\\\n",
    "  =&\\mathrm{argmin}_k\\left(-2\\hat{y}_k\\right) \\\\\n",
    "  =&\\mathrm{argmax}_k\\left(\\hat{y}_k\\right)\n",
    "\\end{align*}∎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.2\n",
    "\n",
    "The Bayes decision boundary is atainable because we know how the two classes (Blue and Organge) are generated. Let's first review on how these two classes are generated. We define a Gaussian bivariate distribution for each of the two classes, with the covariance matrix being the same $2\\times2$ identity matrix $\\mathbf{I}$, and mean vectors different. For the Blue class, the mean vector is $[1,0]^T$, and for the Organge $[0,1]^T$. Namely $\\mathbf{Z}_1 = N\\left([1,0]^T, \\mathbf{I}\\right), \\mathbf{Z}_2 = N\\left([0,1]^T, \\mathbf{I}\\right)$. For each of the two classes, we sample 10 times from its corresponding bivariate Gaussian vector. We denote the two set of samples as: $\\mathbf{p}=\\{p_i\\}_{i=1}^{10}$ for the Blue and $\\mathbf{q}=\\{q_i\\}_{i=1}^{10}$ for the Orange. Next, for each of the two classes, we first uniformly pick an element form $\\mathbf{p}$ (or $\\mathbf{q}$) to be the mean vector, and use $\\frac15\\mathbf{I}$ as the covariance matrix to form another bivariate Gaussian vector to generate a single realization. Such process is repeated for 100 times for each class.\n",
    "\n",
    "Now let's first look for the Bayes decision boundary conditional on $\\mathbf{p}$ and $\\mathbf{q}$, $\\Gamma(\\mathbf{p}, \\mathbf{q})\\in \\mathbb{R}^2$. For any $x\\in\\mathbb{R}^2$. The probability of $x$ being blue is:\n",
    "$$P(x\\,\\mathrm{being\\,blue})= \\sum_{i=1}^{10}\\frac1{10}\\phi(x,p_i), $$\n",
    "where $\\phi(x,p_i)$ is the probability density function at $x$ of a bivariate Guassian with mean $p_i$ and covariance matrix $\\frac15\\mathbf{I}$. Then the Bayes decision boundary conditional on $\\mathbf{p}$ and $\\mathbf{q}$ should be those points where being blue and orange are equally likely:\n",
    "$$\\Gamma(\\mathbf{p}, \\mathbf{q})\n",
    "=\\{x\\in\\mathbb{R}^2\\vert \\sum_{i=1}^{10}\\phi(x, p_i)=\\sum_{i=1}^{10}\\phi(x, q_i)\\}$$\n",
    "\n",
    "The final Bayes decision boundary should only be conditinal on given paramters $\\mathbf{Z}_1$ and $\\mathbf{Z}_2$, which should be the expectation of $\\Gamma(\\mathbf{p}, \\mathbf{q})$ over all possible $\\mathbf{p}$ or $\\mathbf{q}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\Gamma(\\mathbf{Z}_1, \\mathbf{Z}_2) =& \\Gamma(\\mathrm{E}[\\mathbf{p}],\\mathrm{E}[\\mathbf{q}])\\\\\n",
    "=&\\{x \\in \\mathbb{R}^2\\,\\vert\\,\\phi(x,[1,0]^T)=\\phi(x,[0,1]^T)\\}\\\\\n",
    "=&\\{x \\in \\mathbb{R}^2\\,\\vert\\, x_1 = x_2\\}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.3\n",
    "\n",
    "This excercise is related to ordered statistics, where the simplest case minimum is encountered. The volume of a $p$ dimmensional ball is proportional to $r^p$, where $r$ is its radius. Let's denote the random variable of the distance of any sampled point in this $p$ dimmentional unit ball to the origin as $Z$. Then,\n",
    "\n",
    "\\begin{align*}\n",
    "P(Z < x)=&x^p, \\\\\n",
    "P(Z > x)=&1-x^p, \\\\\n",
    "P(Z_{(1)} > x) =& P(\\prod_{i=1}^N Z_i > x)=\\left(1-x^p\\right)^N,\\\\\n",
    "P(Z_{(1)} < x) =& 1 - \\left(1-x^p\\right)^N.\n",
    "\\end{align*}\n",
    "The median is the $\\frac12$ quantile of $Z_{(1)}$, solve for $1-\\left(1-x^p\\right)^N=\\frac12$ gives us:\n",
    "$$x=\\left(1-\\frac12^{1/N}\\right)^{1/p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.4\n",
    "\n",
    "$z_i$ is a linear combination of all the elements of any realization of $\\mathbf{X}$, so it must be one dimensional normal. $\\mathbf{X} \\sim N(0, \\mathbf{I})$, therefore we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{E}[z_i]=& 0,\\\\\n",
    "\\mathrm{Var}(z_i)=&\\sum_{i=1}^p\\left(a_i^2\\cdot1\\right)=1.\n",
    "\\end{align*}\n",
    "\n",
    "The expected squared distance of $z_i$ is:\n",
    "$$\\mathrm{E}[\\mathrm{d}(z_i,0)^2]=\\mathrm{E}[z_i^2]=1.$$\n",
    "\n",
    "The expected squared distance for $x_0$ is:\n",
    "$$\\mathrm{E}[\\mathrm{d}(x_0,0)^2]=\\mathrm{E}[x_0^2]=p,$$\n",
    "since $x_0^2 \\sim \\chi^2(p).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.5\n",
    "\n",
    "## (a)\n",
    "This decomposition of expected prediction error is a general law for any prediction, and should be memorized:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{E}\\left[\\left(y_{0}-\\hat{y}_{0}\\right)^{2}\\right] & =\\mathrm{E}\\left[\\left(y_{0}-\\mathrm{E}[\\hat{y}_{0}]+\\mathrm{E}[\\hat{y}_{0}]-\\hat{y}_{0}\\right)^{2}\\right]\\\\\n",
    " & =\\mathrm{E}\\left[\\left(y_{0}-\\mathrm{E}[\\hat{y}_{0}]\\right)^{2}\\right]+\\mathrm{E}\\left[\\left(\\mathrm{E}[\\hat{y}_{0}]-\\hat{y}_{0}\\right)^{2}\\right]+0\\\\\n",
    " & =\\mathrm{E}\\left[\\left(y_{0}-\\mathrm{E}[y_{0}]+\\mathrm{E}[y_{0}]-\\mathrm{E}[\\hat{y}_{0}]\\right)^{2}\\right]+\\mathrm{Var}[\\hat{y}_{0}]\\\\\n",
    " & =\\mathrm{E}\\left[\\left(y_{0}-\\mathrm{E}[y_{0}]\\right)^{2}\\right]+\\mathrm{E}\\left[\\left(\\mathrm{E}[y_{0}]-\\mathrm{E}[\\hat{y}_{0}]\\right)^{2}\\right]+0+\\mathrm{Var}[\\hat{y}_{0}]\\\\\n",
    " & =\\mathrm{Var}[y_{0}]+\\left(\\mathrm{E}[y_{0}]-\\mathrm{E}[\\hat{y}_{0}]\\right)^{2}+\\mathrm{Var}[\\hat{y}_{0}]\\\\\n",
    " & =\\mathrm{Var}[y_{0}]+\\mathrm{Bias}\\left(\\hat{y}_{0}\\right)^{2}+\\mathrm{Var}[\\hat{y}_{0}]\n",
    "\\end{align*}\n",
    "Following the notion in (2.4), where $\\mathbf{X}$ is an $N\\times p$ matrix with each row an input vector of the training set, and $\\mathbf{y}$ is a $N$-vector of the output of the training set. The least square estimate of $\\beta$ is given by:\n",
    "$$\\hat{\\beta}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y}.$$\n",
    "Then we have:\n",
    "\n",
    "\\begin{align*}\n",
    "    y_0 - \\hat{y}_0 =& x_0^T\\beta+\\varepsilon_0-x_0^T\\hat{\\beta} \\\\\n",
    "    =& x_0^T\\beta+\\varepsilon_0\n",
    "    -x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n",
    "    =& x_0^T\\beta+\\varepsilon_0 - x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\n",
    "    \\mathbf{X}^T\\left(\\mathbf{X}\\beta+\\varepsilon\\right) \\\\\n",
    "    =& \\varepsilon_0 - x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\varepsilon.\n",
    "\\end{align*}\n",
    "Knowing that $\\varepsilon_0$ and $\\varepsilon$ are independent:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathrm{EPE}(x_0)=&\\mathrm{E}[(y_0 - \\hat{y}_0)^2]\\\\\n",
    "    =&\\mathrm{E}\\left[\\left(\\varepsilon_0 - x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\n",
    "    \\mathbf{X}^T\\varepsilon\\right)^2\\right] \\\\\n",
    "    =&\\mathrm{E}\\left[\\varepsilon_0^2\\right] + \\mathrm{E}\\left[\n",
    "    x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\varepsilon \n",
    "    \\varepsilon^T\\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}x_0\\right]+0 \\\\\n",
    "    =&\\mathrm{E}\\left[\\varepsilon_0^2\\right] + \\mathrm{E}\\left[\n",
    "    x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathrm{E}\\left[\\varepsilon \n",
    "    \\varepsilon^T\\right]\\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}x_0\\right]+0 \\\\\n",
    "    =&\\sigma^2+\\sigma^2\\mathrm{E}\\left[\n",
    "    x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\cdot\\mathbf{I}\\cdot\n",
    "    \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}x_0\\right] \\\\\n",
    "    =&\\sigma^2+\\sigma^2\\mathrm{E}\\left[x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}x_0\\right].\n",
    "\\end{align*}\n",
    "According to the decomposition of expected prediction error:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{Var}[y_{0}] & =\\sigma^{2}\\\\\n",
    "\\mathrm{Bias}\\left(\\hat{y}_{0}\\right)^{2} & =0\\\\\n",
    "\\mathrm{Var}[\\hat{y}_{0}] & =\\sigma^{2}\\mathrm{E}\\left[x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}\\right]\n",
    "\\end{align*}\n",
    "In the fourth line of the above equations, I assumed the following:\n",
    "$$\\mathrm{E}[\\mathbf{XYZ}]=\\mathrm{E}[\\mathbf{X}\\cdot\\mathrm{E}[\\mathbf{Y}]\\cdot\\mathbf{Z}],$$\n",
    "where $\\mathbf{X}$,$\\mathbf{Y}$ and $\\mathbf{Z}$ are random matrices, and $\\mathbf{Y}$ is independent of both $\\mathbf{X}$ and $\\mathbf{Z}$. Let's take a step back to first prove for the scalar case: \n",
    "$$\\mathrm{E}[\\mathbf{xyz}]=\\mathrm{E}[\\mathbf{x}\\cdot\\mathrm{E}[\\mathbf{y}]\\cdot\\mathbf{z}].$$\n",
    "\n",
    "By law of total expectation (or tower rule):\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{E}[\\mathbf{xyz}] & =\\mathrm{E}[\\mathrm{E}[\\mathbf{xyz}|\\mathbf{y}]]\\\\\n",
    " & =\\mathrm{E}[\\mathrm{E}[\\mathbf{y}|\\mathbf{y}]\\cdot\\mathrm{E}[\\mathbf{xz}|\\mathbf{y}]]\\\\\n",
    " & =\\mathrm{E}[\\mathbf{y}\\cdot\\mathrm{E}[\\mathbf{xz}|\\mathbf{y}]]\\\\\n",
    " & =\\mathrm{E}[\\mathbf{y}\\cdot\\mathrm{E}[\\mathbf{xz}]]\\qquad\\mbox{by independence of}\\,\\,\\mathbf{y}\\\\\n",
    " & =\\mathrm{E}[\\mathbf{y}]\\cdot\\mathrm{E}[\\mathbf{xz}]\\\\\n",
    " & =\\mathrm{E}[\\mathbf{x}\\cdot\\mathrm{E}[\\mathbf{y}]\\cdot\\mathbf{z}]\n",
    "\\end{align*}\n",
    "\n",
    "The matrix case is straight forward by definition of matrix mutiplication:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{E}[\\mathbf{XYZ}]_{i,j} & =\\mathrm{E}\\left[\\sum_{k}\\sum_{l}\\mathbf{X}_{i,k}\\mathbf{Y}_{k,l}\\mathbf{Z}_{l,j}\\right]\\\\\n",
    " & =\\sum_{k}\\sum_{l}\\mathrm{E}[\\mathbf{X}_{i,k}\\mathbf{Y}_{k,l}\\mathbf{Z}_{l,j}]\\\\\n",
    " & =\\sum_{k}\\sum_{l}\\mathrm{E}\\left[\\mathbf{X}_{i,k}\\mathrm{E}[\\mathbf{Y}_{k,l}]\\mathbf{Z}_{l,j}\\right]\\\\\n",
    " & =\\sum_{k}\\sum_{l}\\mathrm{E}\\left[\\mathbf{X}_{i,k}\\mathrm{E}[\\mathbf{Y}]_{k,l}\\mathbf{Z}_{l,j}\\right]\\\\\n",
    " & =\\mathrm{E}\\left[\\sum_{k}\\sum_{l}\\mathbf{X}_{i,k}\\mathrm{E}[\\mathbf{Y}]_{k,l}\\mathbf{Z}_{l,j}\\right]\\\\\n",
    " & =\\mathrm{E}[\\mathbf{X}\\cdot\\mathrm{E}[\\mathbf{Y}]\\cdot\\mathbf{Z}]_{i,j}\n",
    "\\end{align*}\n",
    "\n",
    "## (b)\n",
    "\n",
    "Assuming $\\mathrm{E}[\\mathbf{X}]=0$, by law of large number we have \n",
    "$$\\frac{\\mathbf{X}^T\\mathbf{X}}{N}\\rightarrow \\mathrm{Cov}(\\mathbf{x})\\, \n",
    "\\mathrm{when}\\, N\\rightarrow\\infty.$$\n",
    "Further more, \n",
    "$$\\frac{\\mathrm{Cov}(\\mathbf{x})^{-1}\\mathbf{X}^T\\mathbf{X}}{N}\\rightarrow \\mathbf{I}\\, \n",
    "\\mathrm{when}\\, N\\rightarrow\\infty.$$\n",
    "$x_0$ has the same distribution as any row of $\\mathbf{X}$:\n",
    "$$\\mathrm{E}[x_0x_0^T]=\\mathrm{Cov}(x_0)=\\mathrm{Cov}(\\mathbf{x}).$$\n",
    "Knowing that $x_0$ and $\\mathbf{X}$ are independent, and that both $\\mathrm{Cov}(\\mathbf{x})$ and $\\mathbf{X}^T\\mathbf{X}$ are symmetric and invertible (for the latter, almost surely):\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{E}\\left[x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}x_0\\right] =& \n",
    "\\mathrm{E}\\left[\\mathrm{Tr}\\left(x_0x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\right)\\right]\\\\\n",
    "=&\\mathrm{Tr}\\left(\\mathrm{E}\\left[x_0x_0^T\\right]\\cdot\n",
    "\\mathrm{E}\\left[\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\right]\\right)\\\\\n",
    "=&\\mathrm{Tr}\\left(\\mathrm{Cov}(\\mathbf{x})\\cdot\n",
    "\\mathrm{E}\\left[\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\right]\\right)\\\\\n",
    "=&\\mathrm{Tr}\\left(\\mathrm{E}\\left[\\mathrm{Cov}(\\mathbf{x})\n",
    "\\cdot\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\right]\\right)\\\\\n",
    "=& \\mathrm{Tr}\\left(\\mathrm{E}\\left[\\left(\\mathrm{Cov}(\\mathbf{x})^{-1}\n",
    "\\cdot\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\right]\\right).\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The matrix inverse operator is a continuous operator on the space of $p\\times p$ invertible matrices, as a result, we have:\n",
    "$$\\left(\\mathrm{Cov}(\\mathbf{x})^{-1}\n",
    "\\cdot\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\sim \\left(N\\cdot\\mathbf{I}\\right)^{-1}\\,\n",
    "\\mathrm{when}\\, N\\rightarrow\\infty.$$\n",
    "One may notice that the matrix inverse operator is continuous but not uniformly continuous, so the above limit behavior holds true almost surely, since the uninvertible $p\\times p$ matrices consists a negligible subset of the set of all $p\\times p$ matrices. \n",
    "\n",
    "Finally we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{Var}[\\hat{y}_{0}]=\n",
    "\\sigma^2\\mathrm{E}\\left[x_0^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}x_0\\right] \\sim&\n",
    "\\sigma^2\\mathrm{Tr}\\left(\\mathrm{E}\\left[\\left(N\\cdot\\mathbf{I}\\right)^{-1}\\right]\\right)\\\\\n",
    "=& \\sigma^2N^{-1}\\mathrm{Tr}(\\mathbf{I})\\\\\n",
    "=& \\sigma^2p/N.\n",
    "\\end{align*}\n",
    "This completes the derivation of (2.28). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.6\n",
    "\n",
    "This problem asks us to prove that the simple least square estimator of $\\theta$, equals some form of weighted least square estimator of $\\theta$ when some values of $x_i$ are repeated. Suppose we have $N_i$ repeatitions of $x_i$, with outputs $y_{ij}$ where $j=1,\\dots,N_i$. For input $x_i$ natrually we would guess that the weight for this data entry to be $N_i$, and the output to be the everage of all $N_j$ numbers of $y_{ij}$, let's denote it as $\\bar{y_i}$. Now if we are lucky, the following can be proved:\n",
    "$$\\mathrm{argmin}_\\theta\\sum_{i=1}^N\\sum_{j=1}^{N_i}(f_\\theta(x_i)-y_{ij})^2\n",
    "=\\mathrm{argmin}_\\theta\\sum_{i=1}^NN_i(f_\\theta(x_i)-\\bar{y_i})^2$$\n",
    "**Proof**\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{argmin}_\\theta\\sum_{i=1}^N\\sum_{j=1}^{N_i}(f_\\theta(x_i)-y_{ij})^2\n",
    "=&\\mathrm{argmin}_\\theta\\sum_{i=1}^N\\sum_{j=1}^{N_i}\n",
    "\\left(f_\\theta(x_i)^2-2y_{ij}f_\\theta(x_i)+y_{ij}^2\\right)\\\\\n",
    "=&\\mathrm{argmin}_\\theta\\sum_{i=1}^NN_i\\left(f_\\theta(x_i)^2-2f_\\theta(x_i)\\bar{y_i}\n",
    "+\\bar{y_i}^2-\\bar{y_i}^2+\\frac1{N_i}\\sum_{j=1}^{N_i}y_{ij}^2\\right)\\\\\n",
    "=&\\mathrm{argmin}_\\theta\\left(\\sum_{i=1}^NN_i\\left(f_\\theta(x_i)-\\bar{y_i}\\right)^2\n",
    "+\\sum_{i=1}^N\\left(\\sum_{j=1}^{N_i}y_{ij}^2-N_i\\bar{y_i}^2\\right)\\right)\\\\\n",
    "=&\\mathrm{argmin}_\\theta\\sum_{i=1}^NN_i\\left(f_\\theta(x_i)-\\bar{y_i}\\right)^2.\n",
    "\\end{align*}∎\n",
    "\n",
    "The last line is based on the fact that $\\sum_{i=1}^N\\left(\\sum_{j=1}^{N_i}y_{ij}^2-N_i\\bar{y_i}^2\\right)$ is not a function of $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.7\n",
    "\n",
    "Without loss of generality, let's assume $\\mathbb{R}^p\\overset{f}{\\rightarrow}\\mathbb{R}$. Namely, let $\\mathbf{x}$ be the p dimensional random vector with $\\{x_i | i=1,\\dots,N\\}$ as its $N$ realizations. \n",
    "## (a)\n",
    "In the case of linear regression, $f(\\mathbf{x})=\\mathbf{x}^T\\beta$. $\\mathcal{X}$ will a $N\\times p$ dimensional matrix with its $i$th row being $x_i^T$. The least square esimator of $\\beta$ is:\n",
    "$$\\hat{\\beta}=\\left(\\mathcal{X}^T\\mathcal{X}\\right)^{-1}\\mathcal{X}^T\\mathcal{Y}.$$\n",
    "Therefore:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{f}(x_0)=&x_0^T\\hat{\\beta}\\\\\n",
    "=&x_0^T\\left(\\mathcal{X}^T\\mathcal{X}\\right)^{-1}\\mathcal{X}^T\\mathcal{Y}\\\\\n",
    "=&\\sum_{i=1}^Nl_i(x_0;\\mathcal{X})y_i,\n",
    "\\end{align*}\n",
    "where $l_i(x_0;\\mathcal{X})$ is the $i$th element of the row vector $x_0^T\\left(\\mathcal{X}^T\\mathcal{X}\\right)^{-1}\\mathcal{X}^T$, and $y_i$ is the $i$th element of the column vector of $\\mathcal{Y}$.\n",
    "\n",
    "In the case of $k$-nearest-neighbor, $l_i(x_0;\\mathcal{X})$ is either $1/k$, if the $x_i$ is in the set of $k$-nearest-neighbor of $x_0$, or $0$, if otherwise.\n",
    "\n",
    "In both cases $l_i(x_0;\\mathcal{X})$ doesn't depend on $y_i$.\n",
    "## (b)\n",
    "This is my first serious study in statistics and was confused about the notation at first. With help from a friend I learned that $\\mathbf{E}_{X,Y|Z}[\\cdot]$ means the same thing as $\\mathbf{E}[\\cdot|Z]$ in probability. It seems that statisticians love to keep themselves reminded about what the random variables are, and I can imagine that for regression there can be a lot of them. Weirdly, (but with all due respet) the unconditinal epxectation $\\mathbf{E}[\\cdot]$ in probability becomes $\\mathbf{E}_{X,Y}[\\cdot]$. Since we are learning statistics here, let's follow the notation.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{E}_{\\mathcal{Y|X}}\\left(f(x_0)-\\hat{f}(x_0)\\right)^2=&\n",
    "\\mathbf{E}_{\\mathcal{Y|X}}\\left(f(x_0) - \\mathbf{E}_{\\mathcal{Y|X}}\\hat{f}(x_0)\n",
    "+\\mathbf{E}_{\\mathcal{Y|X}}\\hat{f}(x_0) -\\hat{f}(x_0)\\right)^2\\\\\n",
    "=&\\mathbf{E}_{\\mathcal{Y|X}}\\left(f(x_0)\n",
    "-\\mathbf{E}_{\\mathcal{Y|X}}\\hat{f}(x_0)\\right)^2\n",
    "+\\mathbf{E}_{\\mathcal{Y|X}}\\left(\\mathbf{E}_{\\mathcal{Y|X}}\\hat{f}(x_0)-\\hat{f}(x_0)\\right)^2\\\\\n",
    "&+2\\mathbf{E}_{\\mathcal{Y|X}}\\left[\\left(f(x_0)-\\mathbf{E}_{\\mathcal{Y|X}}\\hat{f}(x_0)\\right)\n",
    "\\left(\\mathbf{E}_{\\mathcal{Y|X}}\\hat{f}(x_0)-\\hat{f}(x_0)\\right)\\right]\n",
    "\\end{align*}\n",
    "In the last line, the first term is just $\\left(f(x_0)-\\mathbf{E}_{\\mathcal{Y|X}}\\hat{f}(x_0)\\right)^2$, because it is not random given $\\mathcal{X}$. The difference between the expection of an estimator and the true value is the bias of the estimator, so the first term is the square of the bias of $\\hat{f}(x_0)$. The second term is the variance of $\\hat{f}(x_0)$ by definition. Let's take a look at the third term. Since $f(x_{0})-\\mathbf{E}_{\\mathcal{Y|X}}\\hat{f}(x_{0})$ is non-random, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{E}_{\\mathcal{Y}|\\mathcal{X}}\\left[\\left(f(x_{0})-\\mathbf{E}_{\\mathcal{Y}|\\mathcal{X}}\\hat{f}(x_{0})\\right)\\left(\\mathbf{E}_{\\mathcal{Y}|\\mathcal{X}}\\hat{f}(x_{0})-\\hat{f}(x_{0})\\right)\\right] & =\\left(f(x_{0})-\\mathbf{E}_{\\mathcal{Y}|\\mathcal{X}}\\hat{f}(x_{0})\\right)\\mathbf{E}_{\\mathcal{Y}|\\mathcal{X}}\\left[\\mathbf{E}_{\\mathcal{Y}|\\mathcal{X}}\\hat{f}(x_{0})-\\hat{f}(x_{0})\\right]\\\\\n",
    " & =\\left(f(x_{0})-\\mathbf{E}_{\\mathcal{Y}|\\mathcal{X}}\\hat{f}(x_{0})\\right)\\left(\\mathbf{E}_{\\mathcal{Y}|\\mathcal{X}}\\hat{f}(x_{0})-\\mathbf{E}_{\\mathcal{Y}|\\mathcal{X}}\\hat{f}(x_{0})\\right)\\\\\n",
    " & =\\left(f(x_{0})-\\mathbf{E}_{\\mathcal{Y}|\\mathcal{X}}\\hat{f}(x_{0})\\right)\\cdot0=0\n",
    "\\end{align*}\n",
    "\n",
    "Finally, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{E}_{\\mathcal{Y|X}}\\left(f(x_0)-\\hat{f}(x_0)\\right)^2\n",
    "=&\\mathrm{Bias}(\\hat{f}(x_{0})|\\mathcal{X})^2+\\mathrm{Var}(\\hat{f}(x_{0})|\\mathcal{X})\\\\\n",
    "=&\\left(f(x_0)-\\mathbf{E}_{\\mathcal{Y|X}}\\hat{f}(x_0)\\right)^2 \n",
    "+ \\mathbf{E}_{\\mathcal{Y|X}}\\left(\\mathbf{E}_{\\mathcal{Y|X}}\\hat{f}(x_0)-\\hat{f}(x_0)\\right)^2\n",
    "\\end{align*}\n",
    "\n",
    "## (c)\n",
    "The derivation is the same for unconditional expected prediction error and the result is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{E}_{\\mathcal{X},\\mathcal{Y}}\\left(f(x_{0})-\\hat{f}(x_{0})\\right)^{2} & =\\mathrm{Bias}(\\hat{f}(x_{0}))^{2}+\\mathrm{Var}(\\hat{f}(x_{0}))\\\\\n",
    " & =\\left(\\mathbf{E}_{\\mathcal{X},\\mathcal{Y}}\\hat{f}(x_{0})-f(x_{0})\\right)^{2}+\\mathbf{E}_{\\mathcal{X},\\mathcal{Y}}\\left(\\hat{f}(x_{0})-\\mathbf{E}_{\\mathcal{X},\\mathcal{Y}}\\left[\\hat{f}(x_{0})\\right]\\right)^{2}\n",
    "\\end{align*}\n",
    "\n",
    "## (d)\n",
    "I don't quite understand what this problem is asking for. I'm writing down the results for linear regression when the true model is indeed a linear model. \n",
    "For the linear regression, the bias is zero regardless of conditioning. For the variance conditional on $\\mathcal{X}$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{Var}[\\hat{f}(x_{0})|\\mathcal{X}] & =\\mathrm{E}\\left[x_{0}^{T}\\left(\\mathcal{X}^{T}\\mathcal{X}\\right)^{-1}\\mathcal{X}^{T}\\varepsilon\\varepsilon^{T}\\mathcal{X}\\left(\\mathcal{X}^{T}\\mathcal{X}\\right)^{-1}x_{0}|\\mathcal{X}\\right]\\\\\n",
    " & =x_{0}^{T}\\left(\\mathcal{X}^{T}\\mathcal{X}\\right)^{-1}\\mathcal{X}^{T}\\mathrm{E}\\left[\\varepsilon\\varepsilon^{T}|\\mathcal{X}\\right]\\mathcal{X}\\left(\\mathcal{X}^{T}\\mathcal{X}\\right)^{-1}x_{0}\\\\\n",
    " & =x_{0}^{T}\\left(\\mathcal{X}^{T}\\mathcal{X}\\right)^{-1}\\mathcal{X}^{T}\\sigma^{2}\\mathbf{I}\\mathcal{X}\\left(\\mathcal{X}^{T}\\mathcal{X}\\right)^{-1}x_{0}\\\\\n",
    " & =\\sigma^{2}x_{0}^{T}\\left(\\mathcal{X}^{T}\\mathcal{X}\\right)^{-1}\\mathcal{X}^{T}\\mathcal{X}\\left(\\mathcal{X}^{T}\\mathcal{X}\\right)^{-1}x_{0}\\\\\n",
    " & =\\sigma^{2}x_{0}^{T}\\left(\\mathcal{X}^{T}\\mathcal{X}\\right)^{-1}x_{0}\n",
    "\\end{align*}\n",
    "For the unconditionl variance, following Ex. 2.5 (a):\n",
    "$$\\mathrm{Var}[\\hat{f}(x_{0})] =\\sigma^{2}\\mathrm{E}\\left[x_{0}^{T}\\left(\\mathcal{X}^{T}\\mathcal{X}\\right)^{-1}x_{0}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.8\n",
    "The data files are download from the book website: www-stat.stanford.edu/ElemStatLearn, and stored at 'data/Chapter02'. Let's first load the data as pandas data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import getcwd\n",
    "fname_train = getcwd()+'/data/Chapter02/zip.train'\n",
    "fname_train2 = getcwd()+'/data/Chapter02/train.2'\n",
    "fname_train3 = getcwd()+'/data/Chapter02/train.3'\n",
    "fname_test = getcwd()+'/data/Chapter02/zip.test'\n",
    "colnames = ['Digit']+['Grayvalue'+str(i) for i in range(0,256)]\n",
    "\n",
    "train_dat=pd.read_table(fname_train,sep=' ',header=None,names=colnames,index_col=False)\n",
    "train2_dat=pd.read_table(fname_train2,sep=',',header=None,names=colnames[1:],index_col=False)\n",
    "train3_dat=pd.read_table(fname_train3,sep=',',header=None,names=colnames[1:],index_col=False)\n",
    "test_dat=pd.read_table(fname_test,sep=' ',header=None,names=colnames,index_col=False)\n",
    "\n",
    "num_train = train_dat.shape[0]\n",
    "num_train2 = train2_dat.shape[0]\n",
    "num_train3 = train3_dat.shape[0]\n",
    "num_test = test_dat.shape[0]\n",
    "# add exogenenous variable to fit the intercept\n",
    "train_dat.insert(1,'intercept',pd.Series(np.ones(num_train),index=train_dat.index))\n",
    "train2_dat.insert(0,'intercept',pd.Series(np.ones(num_train2),index=train2_dat.index))\n",
    "train3_dat.insert(0,'intercept',pd.Series(np.ones(num_train3),index=train3_dat.index))\n",
    "test_dat.insert(1,'intercept',pd.Series(np.ones(num_test),index=test_dat.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's firt try the ordinary linear least square with no intercept. In this case, a picture with gray scale value of 0 everywhere means 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>Digit</td>      <th>  R-squared:         </th> <td>   0.872</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.868</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   187.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 27 Mar 2016</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:19:59</td>     <th>  Log-Likelihood:    </th> <td> -14463.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  7291</td>      <th>  AIC:               </th> <td>2.944e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  7035</td>      <th>  BIC:               </th> <td>3.120e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   256</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Grayvalue0</th> <td>   -2.2494</td> <td>    0.632</td> <td>   -3.560</td> <td> 0.000</td> <td>   -3.488    -1.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Grayvalue1</th> <td>    0.1097</td> <td>    0.319</td> <td>    0.344</td> <td> 0.731</td> <td>   -0.516     0.735</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>98.271</td> <th>  Durbin-Watson:     </th> <td>   1.942</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 110.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.242</td> <th>  Prob(JB):          </th> <td>1.14e-24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.358</td> <th>  Cond. No.          </th> <td>    379.</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  Digit   R-squared:                       0.872\n",
       "Model:                            OLS   Adj. R-squared:                  0.868\n",
       "Method:                 Least Squares   F-statistic:                     187.6\n",
       "Date:                Sun, 27 Mar 2016   Prob (F-statistic):               0.00\n",
       "Time:                        11:19:59   Log-Likelihood:                -14463.\n",
       "No. Observations:                7291   AIC:                         2.944e+04\n",
       "Df Residuals:                    7035   BIC:                         3.120e+04\n",
       "Df Model:                         256                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Grayvalue0    -2.2494      0.632     -3.560      0.000        -3.488    -1.011\n",
       "Grayvalue1     0.1097      0.319      0.344      0.731        -0.516     0.735\n",
       "==============================================================================\n",
       "Omnibus:                       98.271   Durbin-Watson:                   1.942\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              110.263\n",
       "Skew:                           0.242   Prob(JB):                     1.14e-24\n",
       "Kurtosis:                       3.358   Cond. No.                         379.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.api import OLS\n",
    "\n",
    "result_ols = OLS(endog=train_dat['Digit'], exog=train_dat[colnames[1:]]).fit()\n",
    "result_ols.summary(xname=colnames[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we examine the OLS model fitting results by testing on both the training set and test set. We round the prediction to the nearest integer, with a floor of 0 and a cap of 9. The we count the percetage of predictions that is correct. We achieve roughly 20~30% success rate for digit recognition by using OLS with no intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate on test set:\n",
      "0.254608868959\n",
      "Correct rate on entire train set:\n",
      "0.269784666027\n",
      "Correct rate on training set with the digit being 2:\n",
      "0.243502051984\n",
      "Correct rate on training set with the digit being 3:\n",
      "0.306990881459\n"
     ]
    }
   ],
   "source": [
    "# prediction using the entire test set\n",
    "pred_test = result_ols.predict(test_dat[colnames[1:]])\n",
    "pred_test=np.round(pred_test)\n",
    "pred_test[pred_test<0]=0\n",
    "pred_test[pred_test>9]=9\n",
    "print(\"Correct rate on test set:\")\n",
    "print(sum(pred_test==test_dat['Digit'])/num_test)\n",
    "# prediction using the entire training set\n",
    "pred_train = result_ols.predict(train_dat[colnames[1:]])\n",
    "pred_train=np.round(pred_train)\n",
    "pred_train[pred_train<0]=0\n",
    "pred_train[pred_train>9]=9\n",
    "print(\"Correct rate on entire train set:\")\n",
    "print(sum(pred_train==train_dat['Digit'])/num_train)\n",
    "# prediction using the training set with the digit being 2\n",
    "pred_train2 = result_ols.predict(train2_dat[colnames[1:]])\n",
    "pred_train2=np.round(pred_train2)\n",
    "pred_train2[pred_train2<0]=0\n",
    "pred_train2[pred_train2>9]=9\n",
    "print(\"Correct rate on training set with the digit being 2:\")\n",
    "print(sum(pred_train2==2)/num_train2)\n",
    "# prediction using the training set with the digit being 3\n",
    "pred_train3 = result_ols.predict(train3_dat[colnames[1:]])\n",
    "pred_train3=np.round(pred_train2)\n",
    "pred_train3[pred_train3<0]=0\n",
    "pred_train3[pred_train3>9]=9\n",
    "print(\"Correct rate on training set with the digit being 3:\")\n",
    "print(sum(pred_train3==3)/num_train3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try the OLS with intercept. In this case zero grey scale value every where represents a digit specified by the intercept rounded up to the nearest integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>Digit</td>      <th>  R-squared:         </th> <td>   0.655</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.643</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   52.25</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 27 Mar 2016</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:20:00</td>     <th>  Log-Likelihood:    </th> <td> -14463.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  7291</td>      <th>  AIC:               </th> <td>2.944e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  7034</td>      <th>  BIC:               </th> <td>3.121e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   256</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Grayvalue0</th> <td>    0.6094</td> <td>    1.000</td> <td>    0.610</td> <td> 0.542</td> <td>   -1.350     2.569</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Grayvalue1</th> <td>   -2.1020</td> <td>    0.677</td> <td>   -3.107</td> <td> 0.002</td> <td>   -3.428    -0.776</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>98.456</td> <th>  Durbin-Watson:     </th> <td>   1.942</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 110.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.243</td> <th>  Prob(JB):          </th> <td>1.05e-24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.358</td> <th>  Cond. No.          </th> <td>    562.</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  Digit   R-squared:                       0.655\n",
       "Model:                            OLS   Adj. R-squared:                  0.643\n",
       "Method:                 Least Squares   F-statistic:                     52.25\n",
       "Date:                Sun, 27 Mar 2016   Prob (F-statistic):               0.00\n",
       "Time:                        11:20:00   Log-Likelihood:                -14463.\n",
       "No. Observations:                7291   AIC:                         2.944e+04\n",
       "Df Residuals:                    7034   BIC:                         3.121e+04\n",
       "Df Model:                         256                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Grayvalue0     0.6094      1.000      0.610      0.542        -1.350     2.569\n",
       "Grayvalue1    -2.1020      0.677     -3.107      0.002        -3.428    -0.776\n",
       "==============================================================================\n",
       "Omnibus:                       98.456   Durbin-Watson:                   1.942\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              110.435\n",
       "Skew:                           0.243   Prob(JB):                     1.05e-24\n",
       "Kurtosis:                       3.358   Cond. No.                         562.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.api import OLS\n",
    "\n",
    "result_ols_intercept = OLS(endog=train_dat['Digit'], exog=train_dat[['intercept']+colnames[1:]]).fit()\n",
    "result_ols_intercept.summary(xname=colnames[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flollowing result showed no much difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct rate on test set:\n",
      "0.254608868959\n",
      "Correct rate on entire train set:\n",
      "0.268824578247\n",
      "Correct rate on training set with the digit being 2:\n",
      "0.236662106703\n",
      "Correct rate on training set with the digit being 3:\n",
      "0.316109422492\n"
     ]
    }
   ],
   "source": [
    "# prediction using the entire test set\n",
    "pred_test = result_ols_intercept.predict(test_dat[['intercept']+colnames[1:]])\n",
    "pred_test=np.round(pred_test)\n",
    "pred_test[pred_test<0]=0\n",
    "pred_test[pred_test>9]=9\n",
    "print(\"Correct rate on test set:\")\n",
    "print(sum(pred_test==test_dat['Digit'])/num_test)\n",
    "# prediction using the entire training set\n",
    "pred_train = result_ols_intercept.predict(train_dat[['intercept']+colnames[1:]])\n",
    "pred_train=np.round(pred_train)\n",
    "pred_train[pred_train<0]=0\n",
    "pred_train[pred_train>9]=9\n",
    "print(\"Correct rate on entire train set:\")\n",
    "print(sum(pred_train==train_dat['Digit'])/num_train)\n",
    "# prediction using the training set with the digit being 2\n",
    "pred_train2 = result_ols_intercept.predict(train2_dat[['intercept']+colnames[1:]])\n",
    "pred_train2=np.round(pred_train2)\n",
    "pred_train2[pred_train2<0]=0\n",
    "pred_train2[pred_train2>9]=9\n",
    "print(\"Correct rate on training set with the digit being 2:\")\n",
    "print(sum(pred_train2==2)/num_train2)\n",
    "# prediction using the training set with the digit being 3\n",
    "pred_train3 = result_ols_intercept.predict(train3_dat[['intercept']+colnames[1:]])\n",
    "pred_train3=np.round(pred_train2)\n",
    "pred_train3[pred_train3<0]=0\n",
    "pred_train3[pred_train3>9]=9\n",
    "print(\"Correct rate on training set with the digit being 3:\")\n",
    "print(sum(pred_train3==3)/num_train3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement $k$-nearest-neighbor by brute force. We use a max heap to store $k$ nodes nearest to the test point and update it iteratively with all the training points. The time complexity should be $O(MN)$, where $M$ is the size of the training set and $N$ is the test set. Another simple algorithm is k-d-tree, and cannot be used here. k-d-tree performs poorly and is almost the same as brute force as the dimmension here is 256, and $2^{256} >> 7000$.\n",
    "\n",
    "On my machine, one test will take about 7 seconds. Let's test for only 100 points and print the success rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost in minutes:\n",
      "12.415090866883595\n",
      "Success rate:\n",
      "0.84\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import heapq\n",
    "gray_scale_col = colnames[1:]\n",
    "def knn(k, p_test, training_set):\n",
    "    def distance(p_test, p_train):\n",
    "        return np.square(p_test[gray_scale_col].values-p_train[gray_scale_col].values).sum()\n",
    "    minhp = [] # python only have min heap, use the negative of distance as priority value\n",
    "    train_len = training_set.shape[0]\n",
    "    if train_len <= k:\n",
    "        pred = np.mean(training_set['Digit'])\n",
    "    else:\n",
    "        for i in range(k):\n",
    "            heapq.heappush(minhp, (-distance(p_test, training_set.iloc[i]), i))\n",
    "        for i in range(k, train_len):\n",
    "            priority = -distance(p_test, training_set.iloc[i])\n",
    "            if priority > minhp[0][0]:\n",
    "                heapq.heapreplace(minhp, (priority, i))\n",
    "        row_indices = [entry[1] for entry in minhp]\n",
    "        pred = np.mean(training_set['Digit'][row_indices].values)\n",
    "    pred=np.round(pred)\n",
    "    if pred > 9:\n",
    "        return 9\n",
    "    elif pred <0:\n",
    "        return 0\n",
    "    else:\n",
    "        return int(pred)\n",
    "tic = time()\n",
    "Ntests = 100\n",
    "count = 0.0\n",
    "for i in range(Ntests):\n",
    "    pred = knn(15, test_dat.iloc[i], train_dat)\n",
    "    if pred == int(test_dat['Digit'][i]):\n",
    "        count+=1.0\n",
    "print(\"Time cost in minutes:\")\n",
    "print((time()-tic)/60.0)\n",
    "print(\"Success rate:\")\n",
    "print(count/Ntests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 2.9\n",
    "\n",
    "Let's explicitly calculate $\\mathrm{E}[R_{tr}(\\hat{\\beta})]$ and $\\mathrm{E}[R_{te}(\\hat{\\beta})]$ and compare their values. \n",
    "\n",
    "Let's use $\\mathbf{X}$ to denote the $N\\times p$ matrix with its $i$th row being $x_i$ in the training set and $\\mathbf{y}$ being the $N\\times 1$ vector with its $i$th element being $y_i$ in the training set. Let's denote the true value of $\\beta$ as $\\beta_{true}$, then the true model is as follows:\n",
    "$$\\mathbf{y}=\\mathbf{X}\\beta_{\\mbox{true}}+\\mathbf{\\varepsilon},$$\n",
    "where $\\mathbf{\\varepsilon}$ is the random error vector with zero mean and covariance of $\\sigma^2\\mathbf{I}$.\n",
    "\n",
    "The least square estimator of $\\beta_{true}$ is:\n",
    "$$\\hat{\\beta}=(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}.$$\n",
    "\n",
    "Next we can derive $\\mathrm{E}[R_{tr}(\\hat{\\beta})]$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{E}[R_{tr}(\\hat{\\beta})] & =\\mathrm{E}\\left[\\frac{1}{N}\\sum_{i=1}^{N}(y_{i}-x_{i}^{T}\\hat{\\beta})\\right]\\\\\n",
    " & =\\frac{1}{N}\\mathrm{E}\\left[\\sum_{i=1}^{N}(y_{i}-x_{i}^{T}\\hat{\\beta})\\right]\\\\\n",
    " & =\\frac{1}{N}\\mathrm{E}\\left[\\left(\\mathbf{X}\\hat{\\beta}-\\mathbf{y}\\right)^{T}\\left(\\mathbf{X}\\hat{\\beta}-\\mathbf{y}\\right)\\right]\\\\\n",
    " & =\\frac{1}{N}\\mathrm{E}\\left[\\left(\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{\\varepsilon}-\\mathbf{\\varepsilon}\\right)^{T}\\left(\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{\\varepsilon}-\\mathbf{\\varepsilon}\\right)\\right]\\\\\n",
    " & =\\frac{1}{N}\\mathrm{E}\\left[\\mathbf{\\varepsilon}^{T}\\mathbf{\\varepsilon}-\\mathbf{\\varepsilon}^{T}\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{\\varepsilon}\\right]\\\\\n",
    " & =\\frac{1}{N}\\mathrm{E}\\left[\\mathbf{\\varepsilon}^{T}\\mathbf{\\varepsilon}\\right]-\\frac{1}{N}\\mathrm{E}\\left[\\mathbf{\\varepsilon}^{T}\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{\\varepsilon}\\right]\\\\\n",
    " & =\\sigma^{2}-\\frac{1}{N}\\mathrm{E}\\left[\\mathbf{\\varepsilon}^{T}\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{\\varepsilon}\\right]\\\\\n",
    " & =\\sigma^{2}-\\frac{1}{N}\\mathrm{E}\\left[\\mathrm{Tr}\\left(\\left(\\mathbf{X}^{T}\\mathbf{\\varepsilon}\\mathbf{\\varepsilon}^{T}\\mathbf{X}\\right)(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\right)\\right]\\\\\n",
    " & =\\sigma^{2}-\\frac{1}{N}\\mathrm{E}\\left[\\mathrm{Tr}\\left(\\mathbf{X}^{T}\\mathrm{E}\\left[\\mathbf{\\varepsilon}\\mathbf{\\varepsilon}^{T}\\right]\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\right)\\right]\\quad\\mathbf{\\varepsilon}\\mbox{ being independent of }\\mathbf{X}\\\\\n",
    " & =\\sigma^{2}-\\frac{\\sigma^{2}}{N}\\mathrm{E}\\left[\\mathrm{Tr}\\left(\\mathbf{X}^{T}\\cdot\\mathbf{I}\\cdot\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\right)\\right]\\\\\n",
    " & =\\sigma^{2}-\\frac{\\sigma^{2}}{N}\\mathrm{E}\\left[\\mathrm{Tr}\\left(\\mathbf{X}^{T}\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\right)\\right]\\\\\n",
    " & =\\sigma^{2}-\\frac{\\sigma^{2}}{N}\\mathrm{E}\\left[\\mathrm{Tr}(\\mathbf{I})\\right]\\\\\n",
    " & =\\sigma^{2}\\left(1-\\frac{p}{N}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "For $\\mathrm{E}[R_{te}(\\hat{\\beta})]$ we know that $\\mathrm{E}[R_{te}(\\hat{\\beta})]=\\mathrm{E}\\left[\\left(\\tilde{y}_{1}-\\tilde{x_{1}}\\hat{\\beta}\\right)^{2}\\right]$, due to the symmetry of each test point. Using the results in Ex. 2.5 (a), we know that:\n",
    "$$\\mathrm{E}\\left[\\left(\\tilde{y}_{1}-\\tilde{x_{1}}\\hat{\\beta}\\right)^{2}\\right] =\\sigma^{2}\\left(1+\\mathrm{E}\\left[\\tilde{x}_{1}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\tilde{x}_{1}\\right]\\right)\n",
    "$$\n",
    "When $N>p$ we know that the matrix $\\mathbf{X}^{T}\\mathbf{X}$ is almost surely positive definte. Then its inverse is also almost surely positive defnite, so $\\tilde{x}_{1}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\tilde{x}_{1}$ is positive almost surely. In fact we know from Ex. 2.5 (b), that it will converge to $p/N$ as $N$ gets large. Therefore, we come to the following relation:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathrm{E}[R_{te}(\\hat{\\beta})] & =\\sigma^{2}\\left(1+\\mathrm{E}\\left[\\tilde{x}_{1}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\tilde{x}_{1}\\right]\\right)\\\\\n",
    " & >\\sigma^{2}\\\\\n",
    " & >\\sigma^{2}\\left(1-\\frac{p}{N}\\right)\\\\\n",
    " & =\\mathrm{E}[R_{tr}(\\hat{\\beta})]\n",
    "\\end{align*}\n",
    "To complete the proof, let's quickly prove that the inverse of a postive definite matrix is also positive definite. Suppose matrix $A$ is positive definite so invertible, then $\\forall x\\ne0,\\,\\exists y\\ne0,\\,s.t.\\,Ay=x \\mbox{ and } x^{T}A^{-1}x=\\left(y^{T}A^{T}\\right)A^{-1}Ay=y^{T}A^{T}y=y^{T}Ay>0$.\n",
    "\n",
    "In least square regression the Residual Sum of Squares (RSS) is defined as $N\\cdot R_{tr}(\\hat{\\beta})$. We succesfully derived the formula for the expectation of RSS and this will give us an unbias estimator of $\\sigma^2$:\n",
    "$$\\hat{\\sigma}^2=\\frac{\\mathrm{RSS}}{N-p}.$$\n",
    "\n",
    "In Chapter 3.2 and some other text books, you might see the following instead:\n",
    "$$\\hat{\\sigma}^2=\\frac{\\mathrm{RSS}}{N-p-1},$$\n",
    "and in that case, $p$ is defined as the number of independent variables, excluding the intercept. This will result in a total number of independent variables of $p+1$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
