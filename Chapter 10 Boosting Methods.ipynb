{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 10.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $G_m$ plugged in (10.11), the objective we want to minimize becomes:\n",
    "\n",
    "\\begin{align*}\n",
    " & \\left(e^{\\beta}-e^{-\\beta}\\right)\\cdot\\Sigma_{i=1}^{N}w_{i}^{\\left(m\\right)}I\\left(y_{i}\\ne G_{m}\\left(x_{i}\\right)\\right)+e^{-\\beta}\\cdot\\Sigma_{i=1}^{N}w_{i}^{\\left(m\\right)}\\\\\n",
    "= & \\Sigma_{i=1}^{N}w_{i}^{\\left(m\\right)}\\left(\\left(e^{\\beta}-e^{-\\beta}\\right)\\frac{\\Sigma_{i=1}^{N}w_{i}^{\\left(m\\right)}I\\left(y_{i}\\ne G_{m}\\left(x_{i}\\right)\\right)}{\\Sigma_{i=1}^{N}w_{i}^{\\left(m\\right)}}+e^{-\\beta}\\right)\\\\\n",
    "= & \\left(\\left(e^{\\beta}-e^{-\\beta}\\right)\\mbox{err}_{m}+e^{-\\beta}\\right)\\Sigma_{i=1}^{N}w_{i}^{\\left(m\\right)}.\n",
    "\\end{align*}\n",
    "\n",
    "The minimizing $\\beta$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\arg\\min_{\\beta} & \\left\\{ \\left(\\left(e^{\\beta}-e^{-\\beta}\\right)\\mbox{err}_{m}+e^{-\\beta}\\right)\\Sigma_{i=1}^{N}w_{i}^{\\left(m\\right)}\\right\\} \\\\\n",
    "=\\arg\\min_{\\beta} & \\left\\{ \\left(e^{\\beta}-e^{-\\beta}\\right)\\mbox{err}_{m}+e^{-\\beta}\\right\\} \\\\\n",
    "=\\arg\\min_{\\beta} & \\left\\{ \\mbox{err}_{m}e^{\\beta}+\\left(1-\\mbox{err}_{m}\\right)e^{-\\beta}\\right\\} .\n",
    "\\end{align*}\n",
    "\n",
    "Both $e^{\\beta}$ and $e^{-\\beta}$ are convex function, and both $\\mbox{err}_{m}$ and $1-\\mbox{err}_{m}$ are positive, so the objective function is convex. The minimizer is just the root of the derivate:\n",
    "\n",
    "\\begin{align*}\n",
    "0 & =\\mbox{err}_{m}e^{\\beta_{m}}-\\left(1-\\mbox{err}_{m}\\right)e^{-\\beta_{m}}\\\\\n",
    "\\Rightarrow\\mbox{err}_{m}e^{2\\beta_{m}} & =1-\\mbox{err}_{m}\\\\\n",
    "\\Rightarrow e^{2\\beta_{m}} & =\\frac{1-\\mbox{err}_{m}}{\\mbox{err}_{m}}\\\\\n",
    "\\Rightarrow\\beta_{m} & =\\frac{1}{2}\\ln\\frac{1-\\mbox{err}_{m}}{\\mbox{err}_{m}}.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 10.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "f^{*}\\left(x\\right) & =\\arg\\min_{f\\left(x\\right)}\\quad\\mathbb{E}_{Y|x}\\left(e^{-Yf\\left(x\\right)}\\right)\\\\\n",
    " & =\\arg\\min_{f\\left(x\\right)}\\quad\\left\\{ \\mbox{Pr}\\left(Y=1|x\\right)e^{-f\\left(x\\right)}+\\mbox{Pr}\\left(Y=-1|x\\right)e^{f\\left(x\\right)}\\right\\} .\n",
    "\\end{align*}\n",
    "\n",
    "The objective function is obviously convex, so by setting the first derivative with respect to $f(x)$ to zero, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "0 & =-\\mbox{Pr}\\left(Y=1|x\\right)e^{-f^{*}\\left(x\\right)}+\\mbox{Pr}\\left(Y=-1|x\\right)e^{f^{*}\\left(x\\right)}\\\\\n",
    "\\Rightarrow\\mbox{Pr}\\left(Y=1|x\\right)e^{-f^{*}\\left(x\\right)} & =\\mbox{Pr}\\left(Y=-1|x\\right)e^{f^{*}\\left(x\\right)}\\\\\n",
    "\\Rightarrow\\mbox{Pr}\\left(Y=1|x\\right) & =\\mbox{Pr}\\left(Y=-1|x\\right)e^{2f^{*}\\left(x\\right)}\\\\\n",
    "\\Rightarrow f^{*}\\left(x\\right) & =\\frac{1}{2}\\ln\\frac{\\mbox{Pr}\\left(Y=1|x\\right)}{\\mbox{Pr}\\left(Y=-1|x\\right)}.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 10.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginal average (10.47): \n",
    "\n",
    "$f_{\\mathcal{S}}\\left(X_{\\mathcal{S}}\\right)=\\mathbb{E}_{X_{\\mathcal{C}}}f\\left(X_{\\mathcal{S}},X_{\\mathcal{C}}\\right)=\\int f\\left(X_{\\mathcal{S}},X_{\\mathcal{C}}\\right)\\text{d}\\mathbb{P}\\left(X_{\\mathcal{C}}\\right).$\n",
    "\n",
    "Conditional expectation (10.49):\n",
    "\n",
    "$\\tilde{f}_{\\mathcal{S}}\\left(X_{\\mathcal{S}}\\right)=\\mathbb{E}\\left(\\left.f\\left(X_{\\mathcal{S}},X_{\\mathcal{C}}\\right)\\right|X_{\\mathcal{S}}\\right)=\\int f\\left(X_{\\mathcal{S}},X_{\\mathcal{C}}\\right)\\text{d}\\mathbb{P}\\left(\\left.X_{\\mathcal{C}}\\right|X_{\\mathcal{S}}\\right).$\n",
    "\n",
    "Apparently, $\\mathbb{P}\\left(X_{\\mathcal{C}}\\right)\\ne\\mathbb{P}\\left(\\left.X_{\\mathcal{C}}\\right|X_{\\mathcal{S}}\\right)$\n",
    "unless $X_{\\mathcal{C}}$ and $X_{\\mathcal{S}}$ are indepedent.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additive function (10.50):\n",
    "\n",
    "\\begin{align*}\n",
    "f_{\\mathcal{S}}\\left(X_{\\mathcal{S}}\\right) & =\\int\\left(h_{1}\\left(X_{\\mathcal{S}}\\right)+h_{2}\\left(X_{\\mathcal{C}}\\right)\\right)\\text{d}\\mathbb{P}\\left(X_{\\mathcal{C}}\\right)\\\\\n",
    " & =h_{1}\\left(X_{\\mathcal{S}}\\right)\\int\\text{d}\\mathbb{P}\\left(X_{\\mathcal{C}}\\right)+\\int h_{2}\\left(X_{\\mathcal{C}}\\right)\\text{d}\\mathbb{P}\\left(X_{\\mathcal{C}}\\right)\\\\\n",
    " & =h_{1}\\left(X_{\\mathcal{S}}\\right)+\\mathbb{E}\\left[h_{2}\\left(X_{\\mathcal{C}}\\right)\\right],\\\\\n",
    "\\tilde{f}_{\\mathcal{S}}\\left(X_{\\mathcal{S}}\\right) & =\\int\\left(h_{1}\\left(X_{\\mathcal{S}}\\right)+h_{2}\\left(X_{\\mathcal{C}}\\right)\\right)\\text{d}\\mathbb{P}\\left(\\left.X_{\\mathcal{C}}\\right|X_{\\mathcal{S}}\\right)\\\\\n",
    " & =h_{1}\\left(X_{\\mathcal{S}}\\right)\\int\\text{d}\\mathbb{P}\\left(\\left.X_{\\mathcal{C}}\\right|X_{\\mathcal{S}}\\right)+\\int h_{2}\\left(X_{\\mathcal{C}}\\right)\\text{d}\\mathbb{P}\\left(\\left.X_{\\mathcal{C}}\\right|X_{\\mathcal{S}}\\right)\\\\\n",
    " & =h_{1}\\left(X_{\\mathcal{S}}\\right)+\\mathbb{E}\\left[\\left.h_{2}\\left(X_{\\mathcal{C}}\\right)\\right|X_{\\mathcal{S}}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "As we can see, $\\mathbb{E}\\left[h_{2}\\left(X_{\\mathcal{C}}\\right)\\right]$ is a constant, but $\\mathbb{E}\\left[\\left.h_{2}\\left(X_{\\mathcal{C}}\\right)\\right|X_{\\mathcal{S}}\\right]$ is a function of $X_{\\mathcal{S}}$.\n",
    "Therfore, $f_{\\mathcal{S}}\\left(X_{\\mathcal{S}}\\right)$ produces the $h_{1}\\left(X_{\\mathcal{S}}\\right)$ up to an additive constant, but $\\tilde{f}_{\\mathcal{S}}\\left(X_{\\mathcal{S}}\\right)$ cannot.\n",
    "\n",
    "Simiarly, for multiplicative function (10.51):\n",
    "\n",
    "\\begin{align*}\n",
    "f_{\\mathcal{S}}\\left(X_{\\mathcal{S}}\\right) & =\\int h_{1}\\left(X_{\\mathcal{S}}\\right)\\cdot h_{2}\\left(X_{\\mathcal{C}}\\right)\\text{d}\\mathbb{P}\\left(X_{\\mathcal{C}}\\right)\\\\\n",
    " & =h_{1}\\left(X_{\\mathcal{S}}\\right)\\cdot\\int h_{2}\\left(X_{\\mathcal{C}}\\right)\\text{d}\\mathbb{P}\\left(X_{\\mathcal{C}}\\right)\\\\\n",
    " & =h_{1}\\left(X_{\\mathcal{S}}\\right)\\cdot\\mathbb{E}\\left[h_{2}\\left(X_{\\mathcal{C}}\\right)\\right],\\\\\n",
    "\\tilde{f}_{\\mathcal{S}}\\left(X_{\\mathcal{S}}\\right) & =\\int h_{1}\\left(X_{\\mathcal{S}}\\right)\\cdot h_{2}\\left(X_{\\mathcal{C}}\\right)\\text{d}\\mathbb{P}\\left(\\left.X_{\\mathcal{C}}\\right|X_{\\mathcal{S}}\\right)\\\\\n",
    " & =h_{1}\\left(X_{\\mathcal{S}}\\right)\\cdot\\int h_{2}\\left(X_{\\mathcal{C}}\\right)\\text{d}\\mathbb{P}\\left(\\left.X_{\\mathcal{C}}\\right|X_{\\mathcal{S}}\\right)\\\\\n",
    " & =h_{1}\\left(X_{\\mathcal{S}}\\right)\\cdot\\mathbb{E}\\left[\\left.h_{2}\\left(X_{\\mathcal{C}}\\right)\\right|X_{\\mathcal{S}}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "$f_{\\mathcal{S}}\\left(X_{\\mathcal{S}}\\right)$ produces $h_{1}\\left(X_{\\mathcal{S}}\\right)$ up to a multiplicatie constant, but $\\tilde{f}_{\\mathcal{S}}\\left(X_{\\mathcal{S}}\\right)$ cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 10.5 Multiclass exponential loss (Zhu et al., 2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "format": "row"
   },
   "source": [
    "The objective funciton of the population minimizer is: \n",
    "\n",
    "\\begin{align*}\n",
    " & \\mathbb{E}\\left[e^{-\\frac{1}{K}Y^{T}f}\\right]\\\\\n",
    "= & \\sum_{k=1}^{K}P_{k}e^{-\\frac{1}{K}\\left(f_{k}-\\frac{\\sum_{l\\ne k}f_{l}}{K-1}\\right)},\\\\\n",
    "\\text{where }P_{k}= & \\mathbb{P}\\left(\\left.G=\\mathcal{G}_{k}\\right|X\\right).\n",
    "\\end{align*}\n",
    "\n",
    "With the zero-sum constraint, we have: $\\sum_{l\\ne k}f_{l}=f_{k}$, the objective function is:\n",
    "\n",
    "\\begin{align*}\n",
    " & \\mathbb{E}\\left[e^{-\\frac{1}{K}Y^{T}f}\\right]\\\\\n",
    "= & \\sum_{k=1}^{K}P_{k}e^{-\\frac{1}{K}\\left(f_{k}-\\frac{f_{k}}{K-1}\\right)}\\\\\n",
    "= & \\sum_{k=1}^{K}P_{k}e^{-\\frac{f_{k}}{K}\\frac{K-2}{K-1}}\\\\\n",
    "= & \\sum_{k=1}^{K}P_{k}e^{-\\frac{K-2}{K\\left(K-1\\right)}f_{k}}\n",
    "\\end{align*}\n",
    "\n",
    "This is obvious that, the Hessian matrix of this function is diagnonal, where each entry is positive, and thus convex. \n",
    "The first order condition is both necessary and sufficient for optimality.\n",
    "\n",
    "Let's introduce Lagrange multiplier:\n",
    "\n",
    "\\begin{align*}\n",
    "0 & =\\nabla_{f,\\lambda}\\left(\\sum_{k=1}^{K}P_{k}e^{-\\frac{K-2}{K\\left(K-1\\right)}f_{k}}+\\lambda\\sum_{k=1}^{K}f_{k}\\right)\\\\\n",
    "\\Rightarrow & \\begin{cases}\n",
    "P_{k}e^{-\\frac{K-2}{K\\left(K-1\\right)}f_{k}} & =\\gamma,\\qquad\\forall k=1,\\,\\dots,\\,K\\\\\n",
    "\\sum_{k=1}^{K}f_{k}\\sum_{k=1}^{K}f_{k} & =0.\n",
    "\\end{cases}\\\\\n",
    "\\text{where }\\gamma & =\\lambda\\frac{K\\left(K-1\\right)}{K-2}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first solve for $\\gamma$, taking advantage of the zero-sum condition:\n",
    "\n",
    "\\begin{align*}\n",
    " & \\Pi_{k=1}^{K}\\left(P_{k}e^{-\\frac{K-2}{K\\left(K-1\\right)}f_{k}}\\right) & = & \\Pi_{k=1}^{K}\\gamma=\\gamma^{K}\\\\\n",
    "\\Rightarrow & \\Pi_{k=1}^{K}\\left(P_{k}\\right)e^{-\\frac{K-2}{K\\left(K-1\\right)}\\sum_{k=1}^{K}f_{k}} & = & \\gamma^{K}\\\\\n",
    "\\Rightarrow & \\left(\\Pi_{k=1}^{K}P_{k}\\right)e^{-\\frac{K-2}{K\\left(K-1\\right)}\\cdot0} & = & \\gamma^{K}\\\\\n",
    "\\Rightarrow & \\Pi_{k=1}^{K}P_{k} & = & \\gamma^{K}\\\\\n",
    "\\Rightarrow & \\gamma & = & \\left(\\Pi_{k=1}^{K}P_{k}\\right)^{1/K}\\\\\n",
    "\\Rightarrow & \\ln\\gamma & = & \\frac{1}{K}\\Sigma_{k=1}^{K}\\ln P_{k}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the solution is just:\n",
    "\n",
    "\\begin{align*}\n",
    "P_{k}e^{-\\frac{K-2}{K\\left(K-1\\right)}f_{k}} & =\\gamma\\\\\n",
    "\\Rightarrow\\ln P_{k}-\\frac{K-2}{K\\left(K-1\\right)}f_{k} & =\\ln\\gamma\\\\\n",
    "\\Rightarrow\\frac{K-2}{K\\left(K-1\\right)}f_{k} & =\\ln P_{k}-\\ln\\gamma\\\\\n",
    "\\Rightarrow f_{k} & =\\frac{K\\left(K-1\\right)}{K-2}\\left(\\ln P_{k}-\\frac{1}{K}\\Sigma_{l=1}^{K}\\ln P_{l}\\right).\n",
    "\\end{align*}\n",
    "\n",
    "The pupulation minimizor $f^*_k$ is the part of log likelihood of class $k$ above the average of those of all classes, upto a multiplicative constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With boosting, assuming $N$ traning samples, at step $m$, the minimizaiton problem is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\arg\\min_{\\beta_{m},f^{(m)}} & \\quad\\left\\{ \\Sigma_{i=1}^{N}e^{-\\frac{1}{K}Y_{i}^{T}\\left(f^{(m-1)}\\left(x_{i}\\right)+\\beta_{m}f^{(m)}\\left(x_{i}\\right)\\right)}\\right\\} \\\\\n",
    "=\\arg\\min_{\\beta_{m},f^{(m)}} & \\quad\\left\\{ \\Sigma_{i=1}^{N}e^{-\\frac{1}{K}Y_{i}^{T}f^{(m-1)}\\left(x_{i}\\right)}e^{-\\beta_{m}\\frac{1}{K}Y_{i}^{T}f^{(m)}\\left(x_{i}\\right)}\\right\\} .\n",
    "\\end{align*}\n",
    "\n",
    "$e^{-\\frac{1}{K}Y_{i}^{T}f^{(m-1)}\\left(x_{i}\\right)}$ is independent of $\\beta_{m},f^{(m)}$, so we can consider it as the weight at step $m$:\n",
    "\n",
    "$$w_{i}^{(m)}=e^{-\\frac{1}{K}Y_{i}^{T}f^{(m-1)}\\left(x_{i}\\right)}.$$\n",
    "\n",
    "Then the objective funcito of the minimization problem becomes:\n",
    "\n",
    "$$\\Sigma_{i=1}^{N}w_{i}^{(m)}e^{-\\beta_{m}\\frac{1}{K}Y_{i}^{T}f^{(m)}\\left(x_{i}\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the value of $\\frac1KY_{i}^{T}f^{(m)}\\left(x_{i}\\right)$, if trainng sample $i$ is correctly classified by the weak learner $f^{(m)}$, \n",
    "\n",
    "\\begin{align*}\n",
    " & \\frac{1}{K}Y_{i}^{T}f^{(m)}\\left(x_{i}\\right)\\\\\n",
    "= & \\frac{1}{K}\\left(1+\\frac{K-1}{\\left(K-1\\right)^{2}}\\right)\\\\\n",
    "= & \\frac{1}{K}\\frac{K}{K-1}\\\\\n",
    "= & \\frac{1}{K-1},\n",
    "\\end{align*}\n",
    "\n",
    "If sample $i$ is incorrectly classified, \n",
    "\n",
    "\\begin{align*}\n",
    " & \\frac{1}{K}Y_{i}^{T}f^{(m)}\\left(x_{i}\\right)\\\\\n",
    "= & \\frac{1}{K}\\left(-\\frac{2}{K-1}+\\frac{K-2}{\\left(K-1\\right)^{2}}\\right)\\\\\n",
    "= & \\frac{1}{K}\\frac{-K}{\\left(K-1\\right)^{2}}\\\\\n",
    "= & -\\frac{1}{\\left(K-1\\right)^{2}}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    " & e^{-\\frac{\\beta_{m}}{K-1}}\\cdot\\underset{Y_{i}=f^{(m)}\\left(x_{i}\\right)}{\\Sigma}w_{i}^{(m)}+e^{\\frac{\\beta_{m}}{\\left(K-1\\right)^{2}}}\\cdot\\underset{Y_{i}\\ne f^{(m)}\\left(x_{i}\\right)}{\\Sigma}w_{i}^{(m)}\\\\\n",
    "= & e^{-\\frac{\\beta_{m}}{K-1}}\\cdot\\left(\\underset{Y_{i}=f^{(m)}\\left(x_{i}\\right)}{\\Sigma}w_{i}^{(m)}-\\underset{Y_{i}\\ne f^{(m)}\\left(x_{i}\\right)}{\\Sigma}w_{i}^{(m)}+\\underset{Y_{i}\\ne f^{(m)}\\left(x_{i}\\right)}{\\Sigma}w_{i}^{(m)}\\right)+e^{\\frac{\\beta_{m}}{\\left(K-1\\right)^{2}}}\\cdot\\underset{Y_{i}\\ne f^{(m)}\\left(x_{i}\\right)}{\\Sigma}w_{i}^{(m)}\\\\\n",
    "= & e^{-\\frac{\\beta_{m}}{K-1}}\\cdot\\Sigma w_{i}^{(m)}+\\left(e^{\\frac{\\beta_{m}}{\\left(K-1\\right)^{2}}}-e^{-\\frac{\\beta_{m}}{K-1}}\\right)\\cdot\\underset{Y_{i}\\ne f^{(m)}\\left(x_{i}\\right)}{\\Sigma}w_{i}^{(m)}.\n",
    "\\end{align*}\n",
    "\n",
    "For any given $\\beta_m \\gt 0$, \n",
    "\n",
    "$$e^{\\frac{\\beta_{m}}{\\left(K-1\\right)^{2}}}-e^{-\\frac{\\beta_{m}}{K-1}} \\gt 0,$$\n",
    "\n",
    "Therefore, \n",
    "\\begin{align*}\n",
    "\\arg\\min_{f^{(m)}} & \\quad\\left\\{ \\Sigma_{i=1}^{N}w_{i}^{(m)}e^{-\\beta_{m}\\frac{1}{K}Y_{i}^{T}f^{(m)}\\left(x_{i}\\right)}\\right\\} \\quad\\text{with any given }\\beta_{m}>0\\\\\n",
    "=\\arg\\min_{f^{(m)}} & \\quad\\left\\{ \\underset{Y_{i}\\ne f^{(m)}\\left(x_{i}\\right)}{\\Sigma}w_{i}^{(m)}\\right\\} ,\n",
    "\\end{align*}\n",
    "which is just the result of fitting $Y_i$ with $f^{(m)}$ with sample weights $w_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually tells that the minimization problem can be solved first with respect to the $f^{(m)}$, \n",
    "then to $\\beta_m$, and that the first solution is just the fitting result of the weaker learner.\n",
    "\n",
    "Let's asumme this solution is, namely $f^{m}$, we need to find the step size $\\beta_m$ next. \n",
    "Let's define the set of training samples correctly classified by $f^{m}$ as $\\mathcal{C}$, and incorrectly classified by $\\mathcal{I}$.\n",
    "\n",
    "The step size we want to find will minimize: \n",
    "\n",
    "$$e^{-\\frac{\\beta_{m}}{K-1}}\\cdot\\underset{i\\in\\mathcal{C}}{\\Sigma}w_{i}^{(m)}+e^{\\frac{\\beta_{m}}{\\left(K-1\\right)^{2}}}\\cdot\\underset{i\\in\\mathcal{I}}{\\Sigma}w_{i}^{(m)},$$\n",
    "which is a convex combination (after normalizing the weights) of two convex functions of $\\beta_m$.  \n",
    "\n",
    "Then its minimzor is obtained by the first order condition:\n",
    "\n",
    "\\begin{align*}\n",
    "0 & =-\\frac{1}{K-1}e^{-\\frac{\\beta_{m}^{*}}{K-1}}\\cdot\\underset{i\\in\\mathcal{C}}{\\Sigma}w_{i}^{(m)}+\\frac{1}{\\left(K-1\\right)^{2}}e^{\\frac{\\beta_{m}^{*}}{\\left(K-1\\right)^{2}}}\\cdot\\underset{i\\in\\mathcal{I}}{\\Sigma}w_{i}^{(m)}\\\\\n",
    "\\Rightarrow\\frac{1}{\\left(K-1\\right)^{2}}e^{\\frac{\\beta_{m}^{*}}{\\left(K-1\\right)^{2}}}\\cdot\\underset{i\\in\\mathcal{I}}{\\Sigma}w_{i}^{(m)} & =\\frac{1}{K-1}e^{-\\frac{\\beta_{m}^{*}}{K-1}}\\cdot\\underset{i\\in\\mathcal{C}}{\\Sigma}w_{i}^{(m)}\\\\\n",
    "\\Rightarrow e^{\\frac{\\beta_{m}^{*}}{\\left(K-1\\right)^{2}}+\\frac{\\beta_{m}^{*}}{K-1}} & =\\left(K-1\\right)\\frac{\\underset{i\\in\\mathcal{C}}{\\Sigma}w_{i}^{(m)}}{\\underset{i\\in\\mathcal{I}}{\\Sigma}w_{i}^{(m)}}\\\\\n",
    "\\Rightarrow e^{\\frac{K\\beta_{m}^{*}}{\\left(K-1\\right)^{2}}} & =\\left(K-1\\right)\\frac{1-\\text{Err}_{m}}{\\text{Err}_{m}}\\\\\n",
    "\\Rightarrow\\beta_{m}^{*} & =\\frac{\\left(K-1\\right)^{2}}{K}\\ln\\left(\\frac{1-\\text{Err}_{m}}{\\text{Err}_{m}}\\left(K-1\\right)\\right).\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the reweighting stage, the new weight at step $m+1$ is proportional to:\n",
    "\n",
    "\\begin{align*}\n",
    "w_{i}^{(m+1)} & \\propto w_{i}^{(m)}e^{-\\beta_{m}^{*}\\frac{1}{K}Y_{i}^{T}f^{(m)}\\left(x_{i}\\right)}\\\\\n",
    " & \\propto\\begin{cases}\n",
    "w_{i}^{(m)}e^{\\left(\\frac{1}{K}-1\\right)\\ln\\left(\\frac{1-\\text{Err}_{m}}{\\text{Err}_{m}}\\left(K-1\\right)\\right)} & i\\in\\mathcal{C}\\\\\n",
    "w_{i}^{(m)}e^{\\frac{1}{K}\\ln\\left(\\frac{1-\\text{Err}_{m}}{\\text{Err}_{m}}\\left(K-1\\right)\\right)} & i\\in\\mathcal{I}\n",
    "\\end{cases}\\\\\n",
    " & \\propto w_{i}^{(m)}e^{\\frac{1}{K}\\ln\\left(\\frac{1-\\text{Err}_{m}}{\\text{Err}_{m}}\\left(K-1\\right)\\right)}e^{-\\ln\\left(\\frac{1-\\text{Err}_{m}}{\\text{Err}_{m}}\\left(K-1\\right)\\right)I\\left(i\\in\\mathcal{C}\\right)}\\\\\n",
    " & \\propto w_{i}^{(m)}e^{-\\ln\\left(\\frac{1-\\text{Err}_{m}}{\\text{Err}_{m}}\\left(K-1\\right)\\right)I\\left(i\\in\\mathcal{C}\\right)}\\\\\n",
    " & \\propto w_{i}^{(m)}\\left(\\frac{\\text{Err}_{m}/\\left(K-1\\right)}{1-\\text{Err}_{m}}\\right)^{I\\left(i\\in\\mathcal{C}\\right)}\\\\\n",
    " & \\propto\\begin{cases}\n",
    "w_{i}^{(m)}\\frac{\\text{Err}_{m}/\\left(K-1\\right)}{1-\\text{Err}_{m}} & i\\in\\mathcal{C}\\\\\n",
    "w_{i}^{(m)} & i\\in\\mathcal{I}\n",
    "\\end{cases}\\\\\n",
    " & \\propto\\begin{cases}\n",
    "w_{i}^{(m)}\\frac{\\text{Err}_{m}}{K-1} & i\\in\\mathcal{C}\\\\\n",
    "w_{i}^{(m)}\\left(1-\\text{Err}_{m}\\right) & i\\in\\mathcal{I}\n",
    "\\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for correctly classified training sample, the weight is updated by $\\frac{\\text{Err}_{m}}{K-1}$, and for incorrectly classified training sample, the weight is update by $1-\\text{Err}_{m}$.\n",
    "\n",
    "As long as $\\text{Err}_{m} \\lt 1-1/K$, $\\frac{\\text{Err}_{m}}{K-1} \\lt 1-\\text{Err}_{m}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
