{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 5.1\n",
    "Cubic spline is piecewise 3rd order polynormial at each interval, and the derivative is conitnuous at each knots up to the 2nd order. \n",
    "\n",
    "We only need show that any linear combinations of the six functions in (5.3) has continuous derivatives at the two knots up to 2nd order, the polynormial order being 3 is trivial to verify. \n",
    "\n",
    "First of all, we have:\n",
    "\n",
    "\\begin{align*}\n",
    " & \\lim_{X\\rightarrow\\xi_{1}^{+}}h_{5}\\left(X\\right)=\\lim_{X\\rightarrow\\xi_{1}^{+}}h_{5}^{\\prime}\\left(X\\right)=\\lim_{X\\rightarrow\\xi_{1}^{+}}h_{5}^{\\prime\\prime}\\left(X\\right)\\\\\n",
    "= & \\lim_{X\\rightarrow\\xi_{2}^{+}}h_{6}\\left(X\\right)=\\lim_{X\\rightarrow\\xi_{2}^{+}}h_{6}^{\\prime}\\left(X\\right)=\\lim_{X\\rightarrow\\xi_{2}^{+}}h_{6}^{\\prime\\prime}\\left(X\\right)\\\\\n",
    "= & 0\n",
    "\\end{align*}\n",
    "\n",
    "Consider one such linear combination: $f\\left(X\\right)=\\Sigma_{i=1}^{6} \\beta_{i}h_{i}\\left(X\\right)$, then we have:\n",
    "\n",
    "\\begin{align*}\n",
    " & \\lim_{\\mathrm{d}x\\rightarrow0^{+}}f^{(n)}\\left(\\xi_{i}+\\mathrm{d}x\\right)-f^{(n)}\\left(\\xi_{i}-\\mathrm{d}x\\right)\\\\\n",
    "= & \\beta_{i+4}\\lim_{X\\rightarrow\\xi_{i}^{+}}h_{i+4}^{(n)}\\left(X\\right)\\\\\n",
    "= & 0\\quad\\forall i\\in\\{1,2\\},\\,n\\in\\{0,1,2\\}.\n",
    "\\end{align*}\n",
    "\n",
    "This completes the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 5.4\n",
    "\n",
    "Let's start with cubic spline with K knots. Consider one such such spline as a linear combination of the basis:\n",
    "\\begin{align*}\n",
    "f\\left(X\\right) & =\\Sigma_{i=1}^{K+4}\\beta_{i}h_{i}\\left(X\\right)\\\\\n",
    " & =\\Sigma_{i=1}^{4}\\beta_{i}X^{i-1}+\\Sigma_{i=5}^{K+4}\\beta_{i}\\left(X-\\xi_{i-4}\\right)_{+}^{3}.\n",
    "\\end{align*}\n",
    "\n",
    "In the first interval $X\\le\\xi_i$, $f(X)$ is: $\\Sigma_{i=1}^{4}\\beta_{i}X^{i-1}$.\n",
    "Natual spline requires that the first interval being linear, so we know that: $\\beta_3=\\beta_4=0.$ \n",
    "As a result:\n",
    "\n",
    "\\begin{align*}\n",
    "f\\left(X\\right) & =\\Sigma_{i=1}^{K+4}\\beta_{i}h_{i}\\left(X\\right)\\\\\n",
    " & =\\beta_{1}+\\beta_{2}X+\\Sigma_{i=5}^{K+4}\\beta_{i}\\left(X-\\xi_{i-4}\\right)_{+}^{3}\n",
    "\\end{align*}\n",
    "\n",
    "The last interval $X \\ge \\xi_K$ also have to be linear, where:\n",
    "\n",
    "\\begin{align*}\n",
    "f\\left(X\\right) & =\\beta_{1}+\\beta_{2}X+\\Sigma_{i=1}^{K}\\beta_{i+4}\\left(X-\\xi_{i}\\right)^{3}.\n",
    "\\end{align*}\n",
    "\n",
    "$f(X)$ being linear implies that the coefficients for the quadratic and cubit term are all 0:\n",
    "\n",
    "\\begin{align*}\n",
    "\\Sigma_{i=1}^{K}\\beta_{i+4} & =0\\\\\n",
    "\\Sigma_{i=1}^{K}\\beta_{i+4} & \\xi_{i}=0\n",
    "\\end{align*}\n",
    "\n",
    "Next let's try to express the basis of natural spline in terms of basis of cubic splines:\n",
    "\n",
    "\\begin{align*}\n",
    "d_{k}\\left(X\\right) & =\\frac{h_{k+4}\\left(X\\right)-h_{K+4}\\left(X\\right)}{\\xi_{K}-\\xi_{k}},\\\\\n",
    "N_{k+2}\\left(X\\right) & =d_{k}\\left(X\\right)-d_{K-1}\\left(X\\right)\\\\\n",
    " & =\\frac{h_{k+4}\\left(X\\right)-h_{K+4}\\left(X\\right)}{\\xi_{K}-\\xi_{k}}-\\frac{h_{K+3}\\left(X\\right)-h_{K+4}\\left(X\\right)}{\\xi_{K}-\\xi_{K-1}}\\\\\n",
    " & =\\frac{h_{k+4}\\left(X\\right)}{\\xi_{K}-\\xi_{k}}-\\frac{h_{K+3}\\left(X\\right)}{\\xi_{K}-\\xi_{K-1}}-\\left(\\frac{1}{\\xi_{K}-\\xi_{k}}-\\frac{1}{\\xi_{K}-\\xi_{K-1}}\\right)h_{K+4}\\left(X\\right).\n",
    "\\end{align*}\n",
    "\n",
    "We already know that the first two basis are the same, next we want to find the relation betweens the two sets of coefficients such that:\n",
    "\n",
    "\\begin{align*}\n",
    "\\Sigma_{k=1}^{K-2}\\alpha_{k+2}N_{k+2}\\left(X\\right) & =\\Sigma_{k=1}^{K}\\beta_{k+4}h_{k+4}\\left(X\\right)\\\\\n",
    " & =\\Sigma_{k=1}^{K-2}\\alpha_{k+2}\\left\\{ \\frac{h_{k+4}\\left(X\\right)}{\\xi_{K}-\\xi_{k}}-\\frac{h_{K+3}\\left(X\\right)}{\\xi_{K}-\\xi_{K-1}}-\\left(\\frac{1}{\\xi_{K}-\\xi_{k}}-\\frac{1}{\\xi_{K}-\\xi_{K-1}}\\right)h_{K+4}\\left(X\\right)\\right\\} \\\\\n",
    " & =\\Sigma_{k=1}^{K-2}\\frac{\\alpha_{k+2}}{\\xi_{K}-\\xi_{k}}h_{k+4}\\left(X\\right)-\\frac{\\Sigma_{k=1}^{K-2}\\alpha_{k+2}}{\\xi_{K}-\\xi_{K-1}}h_{K+3}\\left(X\\right)+\\left(\\frac{\\Sigma_{k=1}^{K-2}\\alpha_{k+2}}{\\xi_{K}-\\xi_{K-1}}-\\Sigma_{k=1}^{K-1}\\frac{\\alpha_{k+2}}{\\xi_{K}-\\xi_{k}}\\right)h_{K+4}\\left(X\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Then we can express the $\\beta$'s in terms of $\\alpha$'s:\n",
    "\n",
    "\\begin{align*}\n",
    "\\beta_{K+4} & =\\frac{\\Sigma_{k=1}^{K-2}\\alpha_{k+2}}{\\xi_{K}-\\xi_{K-1}}-\\Sigma_{k=1}^{K-2}\\frac{\\alpha_{k+2}}{\\xi_{K}-\\xi_{k}},\\\\\n",
    "\\beta_{K+3} & =-\\frac{\\Sigma_{k=1}^{K-2}\\alpha_{k+2}}{\\xi_{K}-\\xi_{K-1}},\\\\\n",
    "\\beta_{k+4} & =\\frac{\\alpha_{k+2}}{\\xi_{K}-\\xi_{k}}\\quad\\forall i\\in\\left\\{ 1,\\,\\dots,\\,K-2\\right\\} .\n",
    "\\end{align*}\n",
    "\n",
    "Next, we just need to prove that the coefficients satisfis the following constraints:\n",
    "\n",
    "\\begin{align*}\n",
    "\\Sigma_{k=1}^{K}\\beta_{k+4} & =\\Sigma_{k=1}^{K-2}\\frac{\\alpha_{k+2}}{\\xi_{K}-\\xi_{k}}-\\frac{\\Sigma_{k=1}^{K-2}\\alpha_{k+2}}{\\xi_{K}-\\xi_{K-1}}+\\frac{\\Sigma_{k=1}^{K-2}\\alpha_{k+2}}{\\xi_{K}-\\xi_{K-1}}-\\Sigma_{k=1}^{K-2}\\frac{\\alpha_{k+2}}{\\xi_{K}-\\xi_{k}}\\\\\n",
    " & =0\\\\\n",
    "\\Sigma_{i=k}^{K}\\beta_{k+4}\\xi_{k} & =\\Sigma_{k=1}^{K-2}\\frac{\\xi_{k}\\alpha_{k+2}}{\\xi_{K}-\\xi_{k}}-\\frac{\\xi_{K-1}\\Sigma_{k=1}^{K-2}\\alpha_{k+2}}{\\xi_{K}-\\xi_{K-1}}+\\frac{\\xi_{K}\\Sigma_{k=1}^{K-2}\\alpha_{k+2}}{\\xi_{K}-\\xi_{K-1}}-\\Sigma_{k=1}^{K-2}\\frac{\\xi_{K}\\alpha_{k+2}}{\\xi_{K}-\\xi_{k}}\\\\\n",
    " & =\\Sigma_{k=1}^{K-2}\\frac{\\left(\\xi_{k}-\\xi_{K}\\right)\\alpha_{k+2}}{\\xi_{K}-\\xi_{k}}-\\frac{\\left(\\xi_{K-1}-\\xi_{K}\\right)\\Sigma_{k=1}^{K-2}\\alpha_{k+2}}{\\xi_{K}-\\xi_{K-1}}\\\\\n",
    " & =-\\Sigma_{k=1}^{K-2}\\alpha_{k+2}+\\Sigma_{k=1}^{K-2}\\alpha_{k+2}\\\\\n",
    " & =0.\n",
    "\\end{align*}\n",
    "\n",
    "This completes the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 5.9\n",
    "\n",
    "$\\mathbf{N}$ is $N\\times N$ and acutually invertible (amost surely). \n",
    "\n",
    "\\begin{align*}\n",
    "S_{\\lambda} & =\\mathbf{N}\\left(\\mathbf{N}^{T}\\mathbf{N}+\\lambda\\mathbf{\\Omega}\\right)^{-1}\\mathbf{N}^{T}\\\\\n",
    " & =\\left(\\left(\\mathbf{N}^{T}\\mathbf{N}+\\lambda\\mathbf{\\Omega}\\right)\\mathbf{N}^{-1}\\right)^{-1}\\mathbf{N}^{T}\\\\\n",
    " & =\\left(\\mathbf{N}^{T}+\\lambda\\mathbf{\\Omega}\\mathbf{N}^{-1}\\right)^{-1}\\mathbf{N}^{T}\\\\\n",
    " & =\\left(\\mathbf{N}^{-T}\\left(\\mathbf{N}^{T}+\\lambda\\mathbf{\\Omega}\\mathbf{N}^{-1}\\right)\\right)^{-1}\\\\\n",
    " & =\\left(\\mathbf{I}+\\lambda\\mathbf{N}^{-T}\\mathbf{\\Omega}\\mathbf{N}^{-1}\\right),\\\\\n",
    "\\mathbf{K} & =\\mathbf{N}^{-T}\\mathbf{\\Omega}\\mathbf{N}^{-1}.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 5.15\n",
    "\n",
    "This exercise is about reproducing kernel Hilbert space, which, I believe is the most important topic in this chapter. \n",
    "For many classification problems, with a properly chosen kernel, the optimization formula will be significantly simplified. \n",
    "\n",
    "Let's first review how reproducing kernel are defined.\n",
    "A Hilbert space is reproducing kernel Hilbert space when reproducing kernels can be defined. \n",
    "For simplicity, let's consider the Hilbert space $\\mathcal{H} : \\mathbb{R}^p \\rightarrow \\mathbb{R}$.\n",
    "With Reisz representation theorem on Hilbert space $\\mathcal{H}$, we know that for any bounded linear functional $\\phi$ in its dual $\\mathcal{H}^* : (\\mathbb{R}^p \\rightarrow \\mathbb{R})\\rightarrow \\mathbb{R}$, there is an unique vector $u \\in \\mathcal{H}$ that $\\phi(f) = \\langle f,u\\rangle $.\n",
    "We denote such vector $u$ as the representative of $\\phi$ in $\\mathcal{H}$.\n",
    "\n",
    "Now we are interested in the one particular type of linear functional: the Dirac evaluation functional. \n",
    "This functional $\\delta_x$ will map any vector $f \\in \\mathcal{H}$ to its value at the given point $x\\in \\mathbb{R}^p$, that is, $f(x)$.\n",
    "\n",
    "When any Dirac evaluation functional $\\delta_x$ on $\\mathcal{H}$ is bounded, we can have unique representative in $\\mathcal{H}$ for each $\\delta_x$, namely $k_x$.\n",
    "In this case the reproducing kernel is defined as the function $K(x,y)=\\langle k_x, k_y\\rangle _\\mathcal{H}: \\mathbb{R}^p \\times \\mathbb{R}^p \\rightarrow \\mathbb{R}$. \n",
    "When reproducing kernel can be defined on a Hilbert space, we call it reproducing kernel Hilbert space. \n",
    "As we just shown, a Hilbert space is reproducing kernel Hilbert space when all the Dirac evaluation functional on it is bounded. \n",
    "\n",
    "If a function $K(x,y): \\mathbb{R}^p \\times \\mathbb{R}^p \\rightarrow \\mathbb{R}$ is symmetric and positive definite:\n",
    "$$\\sum_{i=1}^n\\sum_{j=1}^n c_i c_j K(x_i, x_j) \\ge 0, \\quad \\forall n \\in \\mathbb{N}, c_i, c_j \\in \\mathbb{R}, \n",
    "x_i, x_j \\in \\mathbb{R}^p,$$\n",
    "\n",
    "$K(x,y)$ uniquely defines a RKHS as the completion of linear space spanned by $\\{K(\\cdot, y)| y \\in \\mathbb{R}^p\\}$.\n",
    "$K(x,y)$ also uniquely defines a self-adjoint operator $\\mathcal{H}_K\\rightarrow \\mathcal{H}_K$:\n",
    "\n",
    "$$T_{K}(f)=\\left\\langle K(x,),f\\right\\rangle _{L_{2}}=\\int_{\\mathbb{R}^{p}}K(x,y)f(y)\\mu(\\mathrm{d}y),$$\n",
    "\n",
    "where $\\mu$ is the Lebesgue measure in $\\mathbb{R}^p$, and $\\langle,\\rangle_{L_2}$ is the inner product in $\\mathcal{H}$ that will induce the $L_2$ norm.\n",
    "\n",
    "By spectral theorem, there exists positive decreasing sequence of eigenvalues $\\{\\gamma_i > \\gamma_{i+1} > 0|\\lim_{i\\rightarrow \\infty} \\gamma_i = 0\\}$, \n",
    "and corresponding eigen vectors $\\phi_i$ such that $K(x,y)$ can be represented by this expansion:\n",
    "\n",
    "$$K(x,y) =\\sum_{i=1}^{\\infty}\\gamma_{i}\\phi_{i}(x)\\phi_{i}(y). $$\n",
    "\n",
    "$\\phi_i$ is a orthonormal basis in $L_2\\left(\\mathbb{R}^p\\right)$:\n",
    "$$\\langle \\phi_i, \\phi_j\\rangle_{L_2}=\\delta_{ij}.$$\n",
    "\n",
    "This helps us to define the inner product in $\\mathcal{H}_K$:\n",
    "\n",
    "$$\\left\\langle f,g\\right\\rangle _{\\mathcal{H}_{K}}=\\sum_{i=1}^{\\infty}\\frac{\\left\\langle f,\\phi_{i}\\right\\rangle _{L_{2}}\\left\\langle g,\\phi_{i}\\right\\rangle _{L_{2}}}{\\gamma_{i}}.$$\n",
    "\n",
    "Suppose $f$ has the following decompostion:\n",
    "$$f=\\sum_{j=1}^{\\infty}c_{j}\\phi_{j},$$\n",
    "then the norm induced by the inner product in $\\mathcal{H}_K$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\Vert f\\right\\Vert _{\\mathcal{H}_{K}}^{2} & =\\left\\langle f,f\\right\\rangle _{\\mathcal{H}_{K}}\\\\\n",
    " & =\\sum_{i=1}^{\\infty}\\frac{\\left\\langle f,\\phi_{i}\\right\\rangle _{L_{2}}\\left\\langle f,\\phi_{i}\\right\\rangle _{L_{2}}}{\\gamma_{i}}\\\\\n",
    " & =\\sum_{i=1}^{\\infty}\\frac{\\left(\\left\\langle \\sum_{j=1}^{\\infty}c_{j}\\phi_{j},\\phi_{i}\\right\\rangle _{L_{2}}\\right)^{2}}{\\gamma_{i}}\\\\\n",
    " & =\\sum_{i=1}^{\\infty}\\frac{\\left(\\sum_{j=1}^{\\infty}c_{j}\\left\\langle \\phi_{j},\\phi_{i}\\right\\rangle _{L_{2}}\\right)^{2}}{\\gamma_{i}}\\\\\n",
    " & =\\sum_{i=1}^{\\infty}\\frac{\\left(\\sum_{j=1}^{\\infty}c_{j}\\delta_{ij}\\right)^{2}}{\\gamma_{i}}\\\\\n",
    " & =\\sum_{i=1}^{\\infty}\\frac{c_{i}^{2}}{\\gamma_{i}}.\n",
    "\\end{align*}\n",
    "\n",
    "This fills the gap of deriving (5.47) in the text book.\n",
    "## (a)\n",
    "\n",
    "\\begin{align*}\n",
    "K(x,y) & =\\langle k_{x},k_{y}\\rangle_{\\mathcal{H}_{K}}=k_{y}(x)=k_{x}(y)\\\\\n",
    "\\Rightarrow K(\\cdot,y) & =k_{y}\\\\\n",
    "\\Rightarrow\\langle K(\\cdot,x_{i}),f\\rangle_{\\mathcal{H}_{K}} & =\\langle k_{x_{i}},f\\rangle_{\\mathcal{H}_{K}}=f(x_{i})\n",
    "\\end{align*}\n",
    "\n",
    "## (b)\n",
    "\n",
    "\\begin{align*}\n",
    "\\langle K(\\cdot,x_{i}),K(\\cdot,x_{j})\\rangle_{\\mathcal{H}_{K}} & =\\langle k_{x_{i}},k_{x_{j}}\\rangle_{\\mathcal{H}_{K}}\\\\\n",
    " & =K(x_{i},x_{j}).\n",
    "\\end{align*}\n",
    "\n",
    "## (c)\n",
    "Since $K(x, x_i)$ has the eigen-decomposition:\n",
    "$$K(x,x_{i})  =\\sum_{l=1}^{\\infty}\\gamma_{l}\\phi_{l}(x)\\phi_{l}(x_{i}). $$\n",
    "\n",
    "We can find the coordinates of $g(x)$ under this eigen basis:\n",
    "\n",
    "\\begin{align*}\n",
    "g(x) & =\\sum_{i=1}^{N}\\alpha_{i}K(x,x_{i})\\\\\n",
    " & =\\sum_{i=1}^{N}\\alpha_{i}\\sum_{l=1}^{\\infty}\\gamma_{l}\\phi_{l}(x)\\phi_{l}(x_{i})\\\\\n",
    " & =\\sum_{l=1}^{\\infty}\\left(\\gamma_{l}\\sum_{i=1}^{N}\\alpha_{i}\\phi_{l}(x_{i})\\right)\\phi_{l}(x)\\\\\n",
    " & =\\sum_{l=1}^{\\infty}c_{l}\\phi_{l}(x), \n",
    "\\end{align*}\n",
    "where \n",
    "$$c_{l}=\\gamma_{l}\\sum_{i=1}^{N}\\alpha_{i}\\phi_{l}(x_{i}).$$\n",
    "\n",
    "By defintion, \n",
    "\n",
    "\\begin{align*}\n",
    "J(g) & =\\sum_{l=1}^{\\infty}\\frac{c_{l}^{2}}{\\gamma_{l}}\\\\\n",
    " & =\\sum_{l=1}^{\\infty}\\frac{\\left(\\gamma_{l}\\sum_{i=1}^{N}\\alpha_{i}\\phi_{l}(x_{i})\\right)^{2}}{\\gamma_{l}}\\\\\n",
    " & =\\sum_{l=1}^{\\infty}\\gamma_{l}\\left(\\sum_{i=1}^{N}\\alpha_{i}\\phi_{l}(x_{i})\\right)^{2}\\\\\n",
    " & =\\sum_{l=1}^{\\infty}\\gamma_{l}\\sum_{i=1}^{N}\\alpha_{i}\\phi_{l}(x_{i})\\sum_{j=1}^{N}\\alpha_{j}\\phi_{l}(x_{j})\\\\\n",
    " & =\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}\\left(\\sum_{l=1}^{\\infty}\\gamma_{l}\\phi_{l}(x_{i})\\phi_{l}(x_{j})\\right)\\\\\n",
    " & =\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}K(x_{i},x_{j}).\n",
    "\\end{align*}\n",
    "\n",
    "Alternatively, we can use the result that this norm is induced by the inner product:\n",
    "\n",
    "\\begin{align*}\n",
    "J(g) & =\\langle g,g\\rangle_{\\mathcal{H}_{K}}\\\\\n",
    " & =\\langle\\sum_{i=1}^{N}\\alpha_{i}K(x,x_{i}),\\sum_{j=1}^{N}\\alpha_{j}K(x,x_{j})\\rangle_{\\mathcal{H}_{K}}\\\\\n",
    " & =\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}\\langle K(x,x_{i}),K(x,x_{j})\\rangle_{\\mathcal{H}_{K}}\\\\\n",
    " & =\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{i}\\alpha_{j}K(x_{i},x_{j}).\n",
    "\\end{align*}\n",
    "\n",
    "## (d)\n",
    "\n",
    "Since $\\rho(x)$ is orthogonal to each $K(x,x_i)$, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\langle K(\\cdot,x_{i}),\\rho\\rangle_{\\mathcal{H}_{K}} & =\\rho(x_{i})=0 & \\Rightarrow\\\\\n",
    "L\\left(y_{i},\\tilde{g}(x_{i})\\right) & =L\\left(y_{i},g(x_{i})+\\rho(x_{i})\\right)=L\\left(y_{i},g(x_{i})\\right) & \\Rightarrow\\\\\n",
    "\\sum_{i=1}^{N}L\\left(y_{i},\\tilde{g}(x_{i})\\right)+\\lambda J\\left(\\tilde{g}\\right) & =\\sum_{i=1}^{N}L\\left(y_{i},g(x_{i})\\right)+\\lambda J\\left(\\tilde{g}\\right).\n",
    "\\end{align*}\n",
    "\n",
    "As we can see, $\\rho(x)$ lives in the perpendicular complement of subspace spanned by $K(x,x_i)$ so natrually it has no contribution to the loss function where only predictions at training set points $x_i$ are relavent. \n",
    "\n",
    "In order to prove the desired result, what remains to be shown is:\n",
    "$$J\\left(\\tilde{g}\\right)\\ge J\\left(g\\right).$$\n",
    "\n",
    "$\\rho(x)$ is orthogonal to the subspace spanned by $K(x,x_i)$, where $g(x)$ lives, so $\\rho(x)$ is also orthogonal to $g(x)$. With Pythagorean theorem, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "J\\left(\\tilde{g}\\right) & =\\langle\\tilde{g},\\tilde{g}\\rangle_{\\mathcal{H}_{K}}\\\\\n",
    " & =\\langle g,g\\rangle_{\\mathcal{H}_{K}}+\\langle\\rho,\\rho\\rangle_{\\mathcal{H}_{K}}\\\\\n",
    " & =J\\left(g\\right)+J\\left(\\rho\\right)\\\\\n",
    " & \\ge J\\left(g\\right),\n",
    "\\end{align*}\n",
    "\n",
    "and the equality holds only when:\n",
    "\n",
    "$$\\langle\\rho,\\rho\\rangle_{\\mathcal{H}_{K}}=0\\Rightarrow\\rho=0.$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
