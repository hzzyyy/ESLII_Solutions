{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 4.1\n",
    "\n",
    "This is a generalized Rayleigh Quotient. $\\mathbf{W}$ is the within-class covariance matrix so it is a Hermitian positive-definite matrix, almost surely. There is an unique Cholesky decomposition of  $\\mathbf{W}$:\n",
    "\n",
    "$$ \\mathbf{W} = \\mathbf{C}\\mathbf{C}^T, $$\n",
    "where $\\mathbf{C}$ is a lower triangular matrix.\n",
    "\n",
    "Let's denote, $\\mathbf{A}=\\mathbf{C}^{-1}B\\mathbf{C}^{-T}$, then the original problem becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{max}\\quad & a^{T}\\mathbf{CAC}^{T}a\\\\\n",
    "\\mbox{subject to}\\quad & a^{T}\\mathbf{CC}^{T}a=1\n",
    "\\end{align*}\n",
    "\n",
    "If we define $y=\\mathbf{C}^{T}a$, the original problem becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{max}\\quad & y^{T}\\mathbf{A}y\\\\\n",
    "\\mbox{subject to}\\quad & y^{T}y=1\n",
    "\\end{align*}\n",
    "\n",
    "This is an ordinary Rayleigh Quotient, and its maximum is the largest eigenvalue of $\\mathbf{A}$. The transfer is valid because $\\mathbf{C}$ is full rank and for any $y$ there exsit a unique $a$ such that $y=\\mathbf{C}^{T}a$. In summery, by using the Cholesky decomposition of $\\mathbf{W}$, we can transfer this problem to a standard eigenvalue problem.\n",
    "\n",
    "A few rearrangement can bring us another ordinary eigenvalue problem that can also solve this problem. Let's denote the solution, i.e. that largest eigenvalue of $\\mathbf{A}$ as $\\lambda$. Then:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{AC}^{T}a & =\\lambda\\mathbf{C}^{T}a\\\\\n",
    "\\Rightarrow\\mathbf{C}^{-1}\\mathbf{BC}^{-T}\\mathbf{C}^{T}a & =\\lambda\\mathbf{C}^{T}a\\\\\n",
    "\\Rightarrow\\mathbf{C}^{-1}\\mathbf{B}a & =\\lambda\\mathbf{C}^{T}a\\\\\n",
    "\\Rightarrow\\mathbf{C}^{-T}\\mathbf{C}^{-1}\\mathbf{B}a & =\\lambda a\\\\\n",
    "\\Rightarrow\\mathbf{W}^{-1}\\mathbf{B}a & =\\lambda a.\n",
    "\\end{align*}\n",
    "\n",
    "We can see that $\\lambda$ is also an eigenvalue of $\\mathbf{W}^{-1}\\mathbf{B}$. When the above condition is satisfied, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "a\\mathbf{B}a & =a\\mathbf{WW}^{-1}\\mathbf{B}a\\\\\n",
    " & =\\lambda a\\mathbf{W}a\\\\\n",
    " & =\\lambda.\n",
    "\\end{align*}\n",
    "\n",
    "This shows us that we can also solve the original problem by finding the largest eigenvalue of $\\mathbf{W}^{-1}\\mathbf{B}$. In summery, the solution is the largest eigenvalue of both $\\mathbf{C}^{-1}\\mathbf{BC}^{-T}$ and $\\mathbf{W}^{-1}\\mathbf{B}$, with correspoding eigenvector $\\mathbf{C}^{T}a$ and $a$, respectively. \n",
    "\n",
    "It is also interesting to show how this is related to lagrangian multiplier. $\\lambda$ being eigenvector of $\\mathbf{W}^{-1}\\mathbf{B}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{W}^{-1}\\mathbf{B}a & =\\lambda a\\\\\n",
    "\\Rightarrow\\mathbf{B}a & =\\lambda\\mathbf{W}a\\\\\n",
    "\\Rightarrow\\nabla\\left(a\\mathbf{B}a\\right) & =\\lambda\\nabla\\left(a\\mathbf{W}a-1\\right),\n",
    "\\end{align*}\n",
    "which is the stationary condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 4.2\n",
    "## (a)\n",
    "\n",
    "Consider the frequency of class 1 and 2 in the training set as the prior:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_{1} & =\\frac{N_{1}}{N},\\\\\n",
    "\\pi_{2} & =\\frac{N_{2}}{N}.\n",
    "\\end{align*}\n",
    "\n",
    "The posterior density is proportional to the likelihood times prior:\n",
    "\n",
    "\\begin{align*}\n",
    "p(1|x) & \\propto\\frac{1}{\\sqrt{\\left|\\hat{\\Sigma}\\right|(2\\pi)^{p}}}e^{-\\frac{(x-\\hat{\\mu_{1}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{1}})}{2}}\\pi_{1},\\\\\n",
    "p(2|x) & \\propto\\frac{1}{\\sqrt{\\left|\\hat{\\Sigma}\\right|(2\\pi)^{p}}}e^{-\\frac{(x-\\hat{\\mu_{2}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{2}})}{2}}\\pi_{2}.\n",
    "\\end{align*}\n",
    "\n",
    "A point $x$ is classified to class 2 if: $\\log p(2|x) > \\log p(1|x)$, that is:\n",
    "\n",
    "\\begin{align*}\n",
    "-\\frac{(x-\\hat{\\mu_{2}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{2}})}{2}+\\log\\frac{N_{2}}{N} & >-\\frac{(x-\\hat{\\mu_{1}})^{T}\\hat{\\Sigma}^{-1}(x-\\hat{\\mu_{1}})}{2}+\\log\\frac{N_{1}}{N}\\\\\n",
    "\\Rightarrow x^{T}\\hat{\\Sigma}^{-1}(\\hat{\\mu_{2}}-\\hat{\\mu_{1}}) & >\\frac{1}{2}\\hat{\\mu_{2}}^{T}\\hat{\\Sigma}^{-1}\\hat{\\mu_{2}}-\\frac{1}{2}\\hat{\\mu_{1}}^{T}\\hat{\\Sigma}^{-1}\\hat{\\mu_{1}}-\\log\\frac{N_{2}}{N_{1}}\\\\\n",
    "\\Rightarrow x^{T}\\hat{\\Sigma}^{-1}(\\hat{\\mu_{2}}-\\hat{\\mu_{1}}) & >\\frac{1}{2}\\left(\\hat{\\mu_{2}}+\\hat{\\mu_{1}}\\right)^{T}\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu_{2}}-\\hat{\\mu_{1}}\\right)-\\log\\frac{N_{2}}{N_{1}}\n",
    "\\end{align*}\n",
    "\n",
    "## (b)\n",
    "Let's denote $1_N$ as the $N\\times 1$ column vector with all entries being $1$. Let $\\mathbf{X}_1$ be defined as the $N_1\\times p$ matrix of training set in class $1$, and $\\mathbf{X}_2$ be defined similarly. Then centroids for class $1$ and $2$ are:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mu}_{1} & =\\frac{1}{N_{1}}\\mathbf{X}_{1}^{T}\\cdot1_{N_{1}},\\\\\n",
    "\\hat{\\mu}_{2} & =\\frac{1}{N_{2}}\\mathbf{X}_{2}^{T}\\cdot1_{N_{2}}.\n",
    "\\end{align*}\n",
    "\n",
    "The normal equation $\\beta_0$ and $\\beta$ satisfy is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left[\\begin{array}{cc}\n",
    "1_{N} & X\\end{array}\\right]^{T}\\left[\\begin{array}{cc}\n",
    "1_{N} & X\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\beta_{0}\\\\\n",
    "\\beta\n",
    "\\end{array}\\right] & =\\left[\\begin{array}{cc}\n",
    "1_{N} & X\\end{array}\\right]^{T}y,\n",
    "\\end{align*}\n",
    "where \n",
    "\n",
    "\\begin{align*}\n",
    "1_{N} & =\\left[\\begin{array}{c}\n",
    "1_{N_{1}}\\\\\n",
    "1_{N_{2}}\n",
    "\\end{array}\\right],\\\\\n",
    "\\mathbf{X} & =\\left[\\begin{array}{c}\n",
    "\\mathbf{X}_{1}\\\\\n",
    "\\mathbf{X}_{2}\n",
    "\\end{array}\\right],\\\\\n",
    "y & =\\left[\\begin{array}{c}\n",
    "-\\frac{N}{N_{1}}1_{N_{1}}\\\\\n",
    "\\frac{N}{N_{2}}1_{N_{2}}\n",
    "\\end{array}\\right].\n",
    "\\end{align*}\n",
    "\n",
    "Notice that:\n",
    "\n",
    "\\begin{align*}\n",
    "1_{N}^{T}y & =\\left[\\begin{array}{c}\n",
    "1_{N_{1}}\\\\\n",
    "1_{N_{2}}\n",
    "\\end{array}\\right]^{T}\\left[\\begin{array}{c}\n",
    "-\\frac{N}{N_{1}}1_{N_{1}}\\\\\n",
    "\\frac{N}{N_{2}}1_{N_{2}}\n",
    "\\end{array}\\right]\\\\\n",
    " & =-1_{N_{1}}^{T}\\frac{N}{N_{1}}1_{N_{1}}+1_{N_{2}}^{T}\\frac{N}{N_{2}}1_{N_{2}}\\\\\n",
    " & =-\\frac{N}{N_{1}}N_{1}+\\frac{N}{N_{2}}N_{2}\\\\\n",
    " & =0,\n",
    "\\end{align*}\n",
    "and that:\n",
    "$$\\left[\\begin{array}{cc}\n",
    "1_{N} & X\\end{array}\\right]^{T}\\left[\\begin{array}{cc}\n",
    "1_{N} & X\\end{array}\\right]=\\left[\\begin{array}{cc}\n",
    "N & 1_{N}^{T}X\\\\\n",
    "X^{T}1_{N} & X^{T}X\n",
    "\\end{array}\\right].$$\n",
    "\n",
    "The noraml equation can be rewritten as:\n",
    "$$\\left[\\begin{array}{cc}\n",
    "N & 1_{N}^{T}X\\\\\n",
    "X^{T}1_{N} & X^{T}X\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\beta_{0}\\\\\n",
    "\\beta\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "X^{T}y\n",
    "\\end{array}\\right].$$\n",
    "\n",
    "We can solve for $\\beta_0$ by the first line of the above system:\n",
    "$$\\beta_{0}=-\\frac{1}{N}1_{N}^{T}X\\beta,$$\n",
    "and plug it into the rest equations:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left(-\\frac{1}{N}X^{T}1_{N}1_{N}^{T}X+X^{T}X\\right)\\beta & =X^{T}y\\\\\n",
    " & =\\left[\\begin{array}{c}\n",
    "\\mathbf{X}_{1}\\\\\n",
    "\\mathbf{X}_{2}\n",
    "\\end{array}\\right]^{T}\\left[\\begin{array}{c}\n",
    "-\\frac{N}{N_{1}}1_{N_{1}}\\\\\n",
    "\\frac{N}{N_{2}}1_{N_{2}}\n",
    "\\end{array}\\right]\\\\\n",
    " & =-\\frac{N}{N_{1}}\\mathbf{X}_{1}^{T}1_{N_{1}}+\\frac{N}{N_{2}}\\mathbf{X}_{2}^{T}1_{N_{2}}\\\\\n",
    " & =-\\frac{N}{N_{1}}N_{1}\\hat{\\mu}_{1}+\\frac{N}{N_{2}}N_{2}\\hat{\\mu}_{2}\\\\\n",
    " & =N\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right).\n",
    "\\end{align*}\n",
    "\n",
    "The RHS is already done, let's continue on the LHS. Observe that:\n",
    "\n",
    "\\begin{align*}\n",
    "X^{T}1_{N} & =\\left[\\begin{array}{c}\n",
    "\\mathbf{X}_{1}\\\\\n",
    "\\mathbf{X}_{2}\n",
    "\\end{array}\\right]^{T}\\left[\\begin{array}{c}\n",
    "1_{N_{1}}\\\\\n",
    "1_{N_{2}}\n",
    "\\end{array}\\right]\\\\\n",
    " & =\\mathbf{X}_{1}^{T}1_{N_{1}}+\\mathbf{X}_{2}^{T}1_{N_{2}}\\\\\n",
    " & =N_{1}\\hat{\\mu}_{1}+N_{2}\\hat{\\mu}_{2}.\n",
    "\\end{align*}\n",
    "Therefore, \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{N}X^{T}1_{N}1_{N}^{T}X & =\\frac{1}{N}\\left(N_{1}\\hat{\\mu}_{1}+N_{2}\\hat{\\mu}_{2}\\right)\\left(N_{1}\\hat{\\mu}_{1}+N_{2}\\hat{\\mu}_{2}\\right)^{T}\\\\\n",
    " & =\\frac{1}{N}\\left(N_{1}^{2}\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}+2N_{1}N_{2}\\hat{\\mu}_{1}\\hat{\\mu}_{2}^{T}+N_{2}^{2}\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}\\right)\n",
    "\\end{align*}\n",
    "In addition,\n",
    "\n",
    "\\begin{align*}\n",
    "(N-2)\\hat{\\Sigma} & =\\left(\\mathbf{X}_{1}-1_{N_{1}}\\hat{\\mu}_{1}^{T}\\right)^{T}\\left(\\mathbf{X}_{1}-1_{N_{1}}\\hat{\\mu}_{1}^{T}\\right)+\\left(\\mathbf{X}_{2}-1_{N_{2}}\\hat{\\mu}_{2}^{T}\\right)^{T}\\left(\\mathbf{X}_{2}-1_{N_{2}}\\hat{\\mu}_{2}^{T}\\right)\\\\\n",
    " & =\\mathbf{X}_{1}^{T}\\mathbf{X}_{1}+\\mathbf{X}_{2}^{T}\\mathbf{X}_{2}-2\\mathbf{X}_{1}^{T}1_{N_{1}}\\hat{\\mu}_{1}^{T}+\\hat{\\mu}_{1}1_{N_{1}}^{T}1_{N_{1}}\\hat{\\mu}_{1}^{T}-2\\mathbf{X}_{2}^{T}1_{N_{2}}\\hat{\\mu}_{2}^{T}+\\hat{\\mu}_{2}1_{N_{2}}^{T}1_{N_{2}}\\hat{\\mu}_{2}^{T}\\\\\n",
    " & =\\mathbf{X}^{T}\\mathbf{X}-N_{1}\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}-N_{2}\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}.\n",
    "\\end{align*}\n",
    "\n",
    "The LHS of the normal equation can be reorganized as:\n",
    "\n",
    "\\begin{align*}\n",
    " & \\left(-\\frac{1}{N}X^{T}1_{N}1_{N}^{T}X+X^{T}X\\right)\\beta\\\\\n",
    " & =\\left(-\\frac{1}{N}\\left(N_{1}^{2}\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}+2N_{1}N_{2}\\hat{\\mu}_{1}\\hat{\\mu}_{2}^{T}+N_{2}^{2}\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}\\right)+N_{1}\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}+N_{2}\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}+(N-2)\\hat{\\Sigma}\\right)\\beta\\\\\n",
    " & =\\left(\\frac{1}{N}\\left(\\left(N_{1}N-N_{1}^{2}\\right)\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}-2N_{1}N_{2}\\hat{\\mu}_{1}\\hat{\\mu}_{2}^{T}+\\left(N_{2}N-N_{2}^{2}\\right)\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}\\right)+(N-2)\\hat{\\Sigma}\\right)\\beta\\\\\n",
    " & =\\left(\\frac{1}{N}\\left(N_{1}N_{2}\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}-2N_{1}N_{2}\\hat{\\mu}_{1}\\hat{\\mu}_{2}^{T}+N_{1}N_{2}\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}\\right)+(N-2)\\hat{\\Sigma}\\right)\\beta\\\\\n",
    " & =\\left(\\frac{N_{1}N_{2}}{N}\\left(\\hat{\\mu}_{1}\\hat{\\mu}_{1}^{T}-2\\hat{\\mu}_{1}\\hat{\\mu}_{2}^{T}+\\hat{\\mu}_{2}\\hat{\\mu}_{2}^{T}\\right)+(N-2)\\hat{\\Sigma}\\right)\\beta\\\\\n",
    " & =\\left(\\frac{N_{1}N_{2}}{N}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)^{T}+(N-2)\\hat{\\Sigma}\\right)\\beta\\\\\n",
    " & =\\left(N\\hat{\\Sigma}_{B}+(N-2)\\hat{\\Sigma}\\right)\\beta.\n",
    "\\end{align*}\n",
    "\n",
    "This completes the proof.\n",
    "\n",
    "## (c)\n",
    "$\\hat{\\Sigma}_{B}\\beta$ is in direction of $\\hat{\\mu}_{2}-\\hat{\\mu}_{1}$, becuase it is a scaler times $\\hat{\\mu}_{2}-\\hat{\\mu}_{1}$, where the scaler is:\n",
    "$$\\frac{N_{1}N_{2}}{N^2}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)^{T}\\beta.$$\n",
    "Therefore, \n",
    "\n",
    "\\begin{align*}\n",
    "\\left(N-2\\right)\\hat{\\Sigma}\\beta & =N\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)-N\\hat{\\Sigma}_{B}\\beta\\\\\n",
    "\\Rightarrow\\hat{\\Sigma}\\beta & =\\frac{N}{N-2}\\left(\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)-\\hat{\\Sigma}_{B}\\beta\\right)\\\\\n",
    "\\Rightarrow\\beta & =\\frac{N}{N-2}\\hat{\\Sigma}^{-1}\\left(\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)-\\hat{\\Sigma}_{B}\\beta\\right)\\\\\n",
    " & =\\frac{N}{N-2}\\left(1-\\frac{N_{1}N_{2}}{N^{2}}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)^{T}\\beta\\right)\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)\\\\\n",
    "\\Rightarrow\\beta & \\propto\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "As shown in (a), the normal vector of the affine decision boundary of LDA is:\n",
    "$\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right).$\n",
    "\n",
    "## (d)\n",
    "Lets code the value of class $1$ and $2$ as $k_1$ and $k_2$ respectively. The normal equation becomes:\n",
    "\n",
    "$\\left[\\begin{array}{cc}\n",
    "N & 1_{N}^{T}X\\\\\n",
    "X^{T}1_{N} & X^{T}X\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\beta_{0}\\\\\n",
    "\\beta\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "1_{N}^{T}y\\\\\n",
    "X^{T}y\n",
    "\\end{array}\\right].$\n",
    "\n",
    "In this case, $1_{N}^{T}y=N_{1}k_{1}+N_{2}k_{2}$ , and $X^{T}y=N_{1}k_{1}\\hat{\\mu}_{1}+N_{2}k_{2}\\hat{\\mu}_{2}$. After eliminating $\\beta_0$ folliwng the same approach as in (b), we have:\n",
    "\n",
    "\\begin{align*}\n",
    " & \\left(-\\frac{1}{N}X^{T}1_{N}1_{N}^{T}X+X^{T}X\\right)\\beta\\\\\n",
    " & =X^{T}y-\\frac{1}{N}\\mathbf{X}^{T}1_{N}1_{N}^{T}y\\\\\n",
    " & =N_{1}k_{1}\\hat{\\mu}_{1}+N_{2}k_{2}\\hat{\\mu}_{2}-\\frac{1}{N}\\left(N_{1}\\hat{\\mu}_{1}+N_{2}\\hat{\\mu}_{2}\\right)\\left(N_{1}k_{1}+N_{2}k_{2}\\right)\\\\\n",
    " & =N_{2}\\hat{\\mu}_{2}\\left(k_{2}-\\frac{N_{1}k_{1}+N_{2}k_{2}}{N}\\right)-N_{1}\\hat{\\mu}_{1}\\left(\\frac{N_{1}k_{1}+N_{2}k_{2}}{N}-k_{1}\\right)\\\\\n",
    " & =\\frac{N_{1}N_{2}}{N}\\hat{\\mu}_{2}\\left(k_{2}-k_{1}\\right)-\\frac{N_{1}N_{2}}{N}\\hat{\\mu}_{1}\\left(k_{2}-k_{1}\\right)\\\\\n",
    " & =\\frac{N_{1}N_{2}}{N}\\left(k_{2}-k_{1}\\right)\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Compared with previous case, the LHS stays the same, and the RHS is only different up to a scalar multiple. So the property in (c) still holds.\n",
    "\n",
    "## (e)\n",
    "\n",
    "Let's first prove that when $N_1 = N_2$, LDA and least square have the same decission boundary. The boundary for LDA is:\n",
    "$$x^{T}\\hat{\\Sigma}^{-1}(\\hat{\\mu_{2}}-\\hat{\\mu_{1}})=\\frac{1}{2}\\left(\\hat{\\mu_{2}}+\\hat{\\mu_{1}}\\right)^{T}\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu_{2}}-\\hat{\\mu_{1}}\\right).$$\n",
    "\n",
    "For linear least square with class coding in (a) and $N_1 = N_2$, the decision boundary is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{f}(x)=\\hat{\\beta_{0}}+x^{T}\\hat{\\beta} & =0\\\\\n",
    "\\Rightarrow\\left(x^{T}-\\frac{1}{N}1_{N}^{T}X\\right)\\hat{\\beta} & =0\\\\\n",
    "\\Rightarrow\\left(x-\\left(\\frac{N_{1}}{N}\\hat{\\mu}_{1}+\\frac{N_{2}}{N}\\hat{\\mu}_{2}\\right)\\right)^{T}\\hat{\\beta} & =0\\\\\n",
    "\\Rightarrow\\left(x-\\frac{1}{2}\\left(\\hat{\\mu}_{1}+\\hat{\\mu}_{2}\\right)\\right)^{T}\\hat{\\beta} & =0\n",
    "\\end{align*}\n",
    "\n",
    "From (c) we know that\n",
    "$$\\hat{\\beta}=\\lambda\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right), $$\n",
    "where $\\lambda$ is a scalar. If we plug it into the previous equation we obtain the decision boundary:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left(x-\\frac{1}{2}\\left(\\hat{\\mu}_{1}+\\hat{\\mu}_{2}\\right)\\right)^{T}\\lambda\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right) & =0\\\\\n",
    "\\Rightarrow\\left(x-\\frac{1}{2}\\left(\\hat{\\mu}_{1}+\\hat{\\mu}_{2}\\right)\\right)^{T}\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right) & =0\\\\\n",
    "\\Rightarrow x^{T}\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right) & =\\frac{1}{2}\\left(\\hat{\\mu}_{1}+\\hat{\\mu}_{2}\\right)^{T}\\hat{\\Sigma}^{-1}\\left(\\hat{\\mu}_{2}-\\hat{\\mu}_{1}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "We can see the decision boundary in these two methods are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 4.3\n",
    "\n",
    "The solution for this problem can be found at the following two papers:\n",
    "\n",
    "Hastie, Tibshirani, and Buja. 1994, flexible discriminant analysis by optimal scoring. Journal of the American Statistical Association.\n",
    "\n",
    "Breiman and Ihaka. 1984, Nolinear discriminant analysis via scaling and ACE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex. 4.6\n",
    "### (a)\n",
    "For any point $x$, it is classified to $1$ if $f(x) = \\beta^Tx^* > 0 $ and to $-1$ if $f(x) = \\beta^Tx^* < 0 $. As we can see, for correctly classified points, we have:\n",
    "$yf(x) >0,$\n",
    "and for misclassified points, we have:\n",
    "$yf(x) < 0.$\n",
    "\n",
    "Suppose those $N$ points are linearly separable and $\\beta$ defines such a seperating hyperplane, then we have:\n",
    "\n",
    "\\begin{align*}\n",
    "y_{i}\\beta^{T}x_{i}^{*} & >0,\\quad\\forall i\\,\\Rightarrow\\\\\n",
    "y_{i}\\beta^{T}\\frac{x_{i}^{*}}{\\left\\Vert x_{i}^{*}\\right\\Vert } & >0,\\quad\\forall i\\,\\Rightarrow\\\\\n",
    "\\exists C>0\\,\\mbox{s. t. }y_{i}\\beta^{T}\\frac{x_{i}^{*}}{\\left\\Vert x_{i}^{*}\\right\\Vert } & \\ge C,\\quad\\forall i\\,\\Rightarrow\\\\\n",
    "\\exists C>0\\,\\mbox{s. t. }y_{i}\\frac{\\beta^{T}}{C}\\frac{x_{i}^{*}}{\\left\\Vert x_{i}^{*}\\right\\Vert } & \\ge1,\\quad\\forall i\\,\\Rightarrow\\\\\n",
    "\\exists\\beta_{\\mbox{sep}}^{T}=\\frac{\\beta^{T}}{C}\\,\\mbox{s. t. }y_{i}\\beta_{\\mbox{sep}}^{T}\\frac{x_{i}^{*}}{\\left\\Vert x_{i}^{*}\\right\\Vert } & \\ge1,\\quad\\forall i\n",
    "\\end{align*}\n",
    "\n",
    "### (b)\n",
    "It will be less confusing to rephrase the perceptron algorithm in this way. Suppose $x_i$ is misclassified, then update $\\beta$ by:\n",
    "$$\\beta_{\\mbox{new}} \\leftarrow\\beta_{\\mbox{old}}+\\rho y_{i}x_{i}^{*},$$\n",
    "where $\\rho=1/\\left\\Vert x_i^*\\right\\Vert.$\n",
    "\n",
    "Let $\\beta_{\\mbox{sep}}$ be defined exactly the same as in (a), we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\beta_{\\mbox{new}} & =\\beta_{\\mbox{old}}+y_{i}\\frac{x_{i}^{*}}{\\left\\Vert x_{i}^{*}\\right\\Vert }=\\beta_{\\mbox{old}}+y_{i}z_{i}\\,\\Rightarrow\\\\\n",
    "\\beta_{\\mbox{new}}-\\beta_{\\mbox{sep}} & =\\beta_{\\mbox{old}}-\\beta_{\\mbox{sep}}+y_{i}z_{i}\\,\\Rightarrow\\\\\n",
    "\\left\\Vert \\beta_{\\mbox{new}}-\\beta_{\\mbox{sep}}\\right\\Vert ^{2} & =\\left\\Vert \\beta_{\\mbox{old}}-\\beta_{\\mbox{sep}}+y_{i}z_{i}\\right\\Vert ^{2}\\,\\Rightarrow\\\\\n",
    "\\left\\Vert \\beta_{\\mbox{new}}-\\beta_{\\mbox{sep}}\\right\\Vert ^{2} & =\\left\\Vert \\beta_{\\mbox{old}}-\\beta_{\\mbox{sep}}\\right\\Vert ^{2}+2y_{i}\\left(\\beta_{\\mbox{old}}-\\beta_{\\mbox{sep}}\\right)^{T}z_{i}+y_{i}^{2}\\left\\Vert z_{i}\\right\\Vert ^{2}\\,\\Rightarrow\\\\\n",
    "\\left\\Vert \\beta_{\\mbox{new}}-\\beta_{\\mbox{sep}}\\right\\Vert ^{2} & =\\left\\Vert \\beta_{\\mbox{old}}-\\beta_{\\mbox{sep}}\\right\\Vert ^{2}+2\\left(y_{i}\\beta_{\\mbox{old}}^{T}z_{i}-y_{i}\\beta_{\\mbox{sep}}^{T}z_{i}\\right)+1.\n",
    "\\end{align*}\n",
    "\n",
    "Since $x_i$ is misclassified by $\\beta_{\\mbox{old}}$, we have: $y_i\\beta_{\\mbox{old}}^Tx_i < 0$. We just proved in (a) that $y_{i}\\beta_{\\mbox{sep}}^{T}z_{i} \\ge 1$, which leads to \n",
    "$$2\\left(y_{i}\\beta_{\\mbox{old}}^{T}z_{i}-y_{i}\\beta_{\\mbox{sep}}^{T}z_{i}\\right) < -2,$$ and finally we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\left\\Vert \\beta_{\\mbox{new}}-\\beta_{\\mbox{sep}}\\right\\Vert ^{2}-\\left\\Vert \\beta_{\\mbox{old}}-\\beta_{\\mbox{sep}}\\right\\Vert ^{2} & =2\\left(y_{i}\\beta_{\\mbox{old}}^{T}z_{i}-y_{i}\\beta_{\\mbox{sep}}^{T}z_{i}\\right)+1\\\\\n",
    " & \\le-2+1\\\\\n",
    " & =-1.\n",
    "\\end{align*}\n",
    "This completes the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9\n",
    "\n",
    "We first import the data and find the sample means and covariance matrices for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import pandas as pd\n",
    "from os import getcwd\n",
    "fname_train = getcwd()+'/data/Chapter04/vowel.train'\n",
    "train_dat = pd.read_table(fname_train,sep=',',index_col=0)\n",
    "train_dat.columns = ['y', 'x01' , 'x02', 'x03', 'x04', 'x05', 'x06', 'x07', 'x08', 'x09', 'x10']\n",
    "col_xs = train_dat.columns.values[1:]\n",
    "train_gb = train_dat.groupby('y')\n",
    "train_means = train_gb.mean()\n",
    "train_cov = train_gb.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in the training set, the occurencies of each class are the same. So the priors are all the same. We only need to compute in the likelihood for each point in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x01</th>\n",
       "      <th>x02</th>\n",
       "      <th>x03</th>\n",
       "      <th>x04</th>\n",
       "      <th>x05</th>\n",
       "      <th>x06</th>\n",
       "      <th>x07</th>\n",
       "      <th>x08</th>\n",
       "      <th>x09</th>\n",
       "      <th>x10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x01  x02  x03  x04  x05  x06  x07  x08  x09  x10\n",
       "y                                                   \n",
       "1    48   48   48   48   48   48   48   48   48   48\n",
       "2    48   48   48   48   48   48   48   48   48   48\n",
       "3    48   48   48   48   48   48   48   48   48   48\n",
       "4    48   48   48   48   48   48   48   48   48   48\n",
       "5    48   48   48   48   48   48   48   48   48   48\n",
       "6    48   48   48   48   48   48   48   48   48   48\n",
       "7    48   48   48   48   48   48   48   48   48   48\n",
       "8    48   48   48   48   48   48   48   48   48   48\n",
       "9    48   48   48   48   48   48   48   48   48   48\n",
       "10   48   48   48   48   48   48   48   48   48   48\n",
       "11   48   48   48   48   48   48   48   48   48   48"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gb.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import solve, det\n",
    "fname_test = getcwd()+'/data/Chapter04/vowel.train'\n",
    "test_dat = pd.read_table(fname_train,sep=',',index_col=0)\n",
    "test_dat.columns = ['y', 'x01' , 'x02', 'x03', 'x04', 'x05', 'x06', 'x07', 'x08', 'x09', 'x10']\n",
    "num_xs = len(col_xs)\n",
    "num_test = test_dat.shape[0]\n",
    "num_class = train_means.shape[0]\n",
    "loglkhd = np.zeros([num_class, num_test])\n",
    "for i in range(num_class):\n",
    "    offsets = pd.DataFrame.transpose(test_dat[col_xs] - train_means.loc[i+1])\n",
    "    dist = np.einsum('ij,ij->j', offsets, solve(train_cov.loc[i+1], offsets, sym_pos=True))\n",
    "    loglkhd[i] = -0.5*(np.log(det(train_cov.loc[i+1])) + num_class * np.log(2.0*np.pi) + dist)\n",
    "pred = np.argmax(loglkhd, axis=0) + 1\n",
    "test_dat.insert(1, 'pred', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification success rate is: \n",
      "0.988636363636\n",
      "Prediction for each test data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>pred</th>\n",
       "      <th>x01</th>\n",
       "      <th>x02</th>\n",
       "      <th>x03</th>\n",
       "      <th>x04</th>\n",
       "      <th>x05</th>\n",
       "      <th>x06</th>\n",
       "      <th>x07</th>\n",
       "      <th>x08</th>\n",
       "      <th>x09</th>\n",
       "      <th>x10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.639</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.670</td>\n",
       "      <td>1.779</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>1.627</td>\n",
       "      <td>-0.388</td>\n",
       "      <td>0.529</td>\n",
       "      <td>-0.874</td>\n",
       "      <td>-0.814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.327</td>\n",
       "      <td>0.496</td>\n",
       "      <td>-0.694</td>\n",
       "      <td>1.365</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>1.933</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>0.510</td>\n",
       "      <td>-0.621</td>\n",
       "      <td>-0.488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.120</td>\n",
       "      <td>0.894</td>\n",
       "      <td>-1.576</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.707</td>\n",
       "      <td>1.559</td>\n",
       "      <td>-0.579</td>\n",
       "      <td>0.676</td>\n",
       "      <td>-0.809</td>\n",
       "      <td>-0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.287</td>\n",
       "      <td>1.809</td>\n",
       "      <td>-1.498</td>\n",
       "      <td>1.012</td>\n",
       "      <td>-1.053</td>\n",
       "      <td>1.060</td>\n",
       "      <td>-0.567</td>\n",
       "      <td>0.235</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.598</td>\n",
       "      <td>1.938</td>\n",
       "      <td>-0.846</td>\n",
       "      <td>1.062</td>\n",
       "      <td>-1.633</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.394</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.277</td>\n",
       "      <td>-0.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.852</td>\n",
       "      <td>1.914</td>\n",
       "      <td>-0.755</td>\n",
       "      <td>0.825</td>\n",
       "      <td>-1.588</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.217</td>\n",
       "      <td>-0.246</td>\n",
       "      <td>0.238</td>\n",
       "      <td>-0.365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.482</td>\n",
       "      <td>2.524</td>\n",
       "      <td>-0.433</td>\n",
       "      <td>1.048</td>\n",
       "      <td>-1.995</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-3.941</td>\n",
       "      <td>2.305</td>\n",
       "      <td>0.124</td>\n",
       "      <td>1.771</td>\n",
       "      <td>-1.815</td>\n",
       "      <td>0.593</td>\n",
       "      <td>-0.435</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.575</td>\n",
       "      <td>-0.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>-3.860</td>\n",
       "      <td>2.116</td>\n",
       "      <td>-0.939</td>\n",
       "      <td>0.688</td>\n",
       "      <td>-0.675</td>\n",
       "      <td>1.679</td>\n",
       "      <td>-0.512</td>\n",
       "      <td>0.928</td>\n",
       "      <td>-0.167</td>\n",
       "      <td>-0.434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-3.648</td>\n",
       "      <td>1.812</td>\n",
       "      <td>-1.378</td>\n",
       "      <td>1.578</td>\n",
       "      <td>0.065</td>\n",
       "      <td>1.577</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>-3.032</td>\n",
       "      <td>1.739</td>\n",
       "      <td>-1.141</td>\n",
       "      <td>0.737</td>\n",
       "      <td>-0.834</td>\n",
       "      <td>1.386</td>\n",
       "      <td>-0.575</td>\n",
       "      <td>0.679</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.653</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>1.705</td>\n",
       "      <td>-0.222</td>\n",
       "      <td>1.765</td>\n",
       "      <td>-0.353</td>\n",
       "      <td>0.537</td>\n",
       "      <td>-0.797</td>\n",
       "      <td>-0.813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.237</td>\n",
       "      <td>0.436</td>\n",
       "      <td>-0.860</td>\n",
       "      <td>1.363</td>\n",
       "      <td>-0.251</td>\n",
       "      <td>1.915</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>0.751</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>-0.327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.135</td>\n",
       "      <td>0.954</td>\n",
       "      <td>-1.632</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>1.600</td>\n",
       "      <td>-0.628</td>\n",
       "      <td>0.713</td>\n",
       "      <td>-0.903</td>\n",
       "      <td>-0.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.304</td>\n",
       "      <td>1.784</td>\n",
       "      <td>-1.506</td>\n",
       "      <td>0.981</td>\n",
       "      <td>-0.961</td>\n",
       "      <td>0.806</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.540</td>\n",
       "      <td>2.144</td>\n",
       "      <td>-1.024</td>\n",
       "      <td>0.933</td>\n",
       "      <td>-1.567</td>\n",
       "      <td>1.024</td>\n",
       "      <td>0.188</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>0.309</td>\n",
       "      <td>-0.633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.826</td>\n",
       "      <td>2.003</td>\n",
       "      <td>-0.738</td>\n",
       "      <td>0.801</td>\n",
       "      <td>-1.669</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.245</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>0.256</td>\n",
       "      <td>-0.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.582</td>\n",
       "      <td>2.374</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>1.162</td>\n",
       "      <td>-1.953</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.415</td>\n",
       "      <td>-0.259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-3.951</td>\n",
       "      <td>2.250</td>\n",
       "      <td>0.127</td>\n",
       "      <td>1.772</td>\n",
       "      <td>-1.906</td>\n",
       "      <td>0.567</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>1.045</td>\n",
       "      <td>0.598</td>\n",
       "      <td>-0.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>-3.783</td>\n",
       "      <td>1.974</td>\n",
       "      <td>-1.200</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-0.650</td>\n",
       "      <td>1.504</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.392</td>\n",
       "      <td>-0.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-3.673</td>\n",
       "      <td>1.811</td>\n",
       "      <td>-1.405</td>\n",
       "      <td>1.621</td>\n",
       "      <td>0.044</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-0.453</td>\n",
       "      <td>0.745</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>-2.946</td>\n",
       "      <td>1.649</td>\n",
       "      <td>-1.167</td>\n",
       "      <td>0.788</td>\n",
       "      <td>-0.909</td>\n",
       "      <td>1.300</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>0.902</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.665</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.641</td>\n",
       "      <td>1.791</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>1.686</td>\n",
       "      <td>-0.359</td>\n",
       "      <td>0.570</td>\n",
       "      <td>-0.676</td>\n",
       "      <td>-0.841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.165</td>\n",
       "      <td>0.408</td>\n",
       "      <td>-0.971</td>\n",
       "      <td>1.207</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>1.921</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.723</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>-0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.105</td>\n",
       "      <td>1.035</td>\n",
       "      <td>-1.705</td>\n",
       "      <td>0.231</td>\n",
       "      <td>-0.558</td>\n",
       "      <td>1.554</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>0.710</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>-0.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.312</td>\n",
       "      <td>1.746</td>\n",
       "      <td>-1.510</td>\n",
       "      <td>1.019</td>\n",
       "      <td>-0.990</td>\n",
       "      <td>0.941</td>\n",
       "      <td>-0.488</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.635</td>\n",
       "      <td>2.147</td>\n",
       "      <td>-1.129</td>\n",
       "      <td>0.911</td>\n",
       "      <td>-1.407</td>\n",
       "      <td>1.095</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-0.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.887</td>\n",
       "      <td>2.131</td>\n",
       "      <td>-0.830</td>\n",
       "      <td>0.682</td>\n",
       "      <td>-1.557</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.448</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>0.207</td>\n",
       "      <td>-0.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.635</td>\n",
       "      <td>2.250</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>1.012</td>\n",
       "      <td>-1.693</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-3.986</td>\n",
       "      <td>2.325</td>\n",
       "      <td>0.102</td>\n",
       "      <td>1.633</td>\n",
       "      <td>-2.014</td>\n",
       "      <td>0.576</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>1.003</td>\n",
       "      <td>0.566</td>\n",
       "      <td>-0.245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.581</td>\n",
       "      <td>1.188</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.449</td>\n",
       "      <td>-0.676</td>\n",
       "      <td>0.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.828</td>\n",
       "      <td>2.367</td>\n",
       "      <td>-0.836</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>0.116</td>\n",
       "      <td>1.172</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-3.820</td>\n",
       "      <td>1.705</td>\n",
       "      <td>-1.130</td>\n",
       "      <td>0.450</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.298</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.473</td>\n",
       "      <td>0.848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.949</td>\n",
       "      <td>2.990</td>\n",
       "      <td>-0.643</td>\n",
       "      <td>-0.986</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.803</td>\n",
       "      <td>-1.479</td>\n",
       "      <td>0.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-4.471</td>\n",
       "      <td>4.461</td>\n",
       "      <td>-1.887</td>\n",
       "      <td>-0.799</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-1.036</td>\n",
       "      <td>-0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>-4.580</td>\n",
       "      <td>4.116</td>\n",
       "      <td>-0.753</td>\n",
       "      <td>-0.849</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-1.256</td>\n",
       "      <td>-0.119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-5.049</td>\n",
       "      <td>4.846</td>\n",
       "      <td>-0.678</td>\n",
       "      <td>-0.877</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.253</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>-4.115</td>\n",
       "      <td>1.856</td>\n",
       "      <td>-0.738</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.329</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.530</td>\n",
       "      <td>0.456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.143</td>\n",
       "      <td>2.051</td>\n",
       "      <td>1.216</td>\n",
       "      <td>1.898</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>-0.466</td>\n",
       "      <td>-0.842</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.403</td>\n",
       "      <td>1.409</td>\n",
       "      <td>0.551</td>\n",
       "      <td>1.496</td>\n",
       "      <td>0.673</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>-0.787</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>-0.696</td>\n",
       "      <td>1.139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-4.316</td>\n",
       "      <td>1.272</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>1.285</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.463</td>\n",
       "      <td>-0.294</td>\n",
       "      <td>0.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.622</td>\n",
       "      <td>1.111</td>\n",
       "      <td>-0.972</td>\n",
       "      <td>0.399</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.373</td>\n",
       "      <td>-0.716</td>\n",
       "      <td>0.943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.850</td>\n",
       "      <td>2.280</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.913</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>0.260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-3.962</td>\n",
       "      <td>1.724</td>\n",
       "      <td>-1.290</td>\n",
       "      <td>0.531</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.282</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.603</td>\n",
       "      <td>1.139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.894</td>\n",
       "      <td>2.910</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>-0.671</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.708</td>\n",
       "      <td>-1.515</td>\n",
       "      <td>0.121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-4.469</td>\n",
       "      <td>4.348</td>\n",
       "      <td>-1.740</td>\n",
       "      <td>-0.886</td>\n",
       "      <td>0.447</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.577</td>\n",
       "      <td>-0.908</td>\n",
       "      <td>-0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>-4.661</td>\n",
       "      <td>4.235</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>-1.021</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.457</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.345</td>\n",
       "      <td>-1.355</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-5.040</td>\n",
       "      <td>5.074</td>\n",
       "      <td>-1.721</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.609</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>-4.169</td>\n",
       "      <td>1.799</td>\n",
       "      <td>-0.621</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.362</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.105</td>\n",
       "      <td>1.968</td>\n",
       "      <td>1.140</td>\n",
       "      <td>1.719</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>-0.432</td>\n",
       "      <td>-0.969</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.544</td>\n",
       "      <td>1.672</td>\n",
       "      <td>0.643</td>\n",
       "      <td>1.281</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.803</td>\n",
       "      <td>-0.709</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>1.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-4.426</td>\n",
       "      <td>1.167</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1.315</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.244</td>\n",
       "      <td>-0.423</td>\n",
       "      <td>0.684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.622</td>\n",
       "      <td>0.988</td>\n",
       "      <td>-1.108</td>\n",
       "      <td>0.685</td>\n",
       "      <td>-0.693</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.166</td>\n",
       "      <td>-0.458</td>\n",
       "      <td>0.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.597</td>\n",
       "      <td>2.146</td>\n",
       "      <td>-1.492</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>-0.297</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.870</td>\n",
       "      <td>-0.490</td>\n",
       "      <td>-0.471</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-4.122</td>\n",
       "      <td>1.735</td>\n",
       "      <td>-1.340</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.345</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-4.065</td>\n",
       "      <td>2.876</td>\n",
       "      <td>-0.856</td>\n",
       "      <td>-0.221</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.633</td>\n",
       "      <td>-1.452</td>\n",
       "      <td>0.272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-4.513</td>\n",
       "      <td>4.265</td>\n",
       "      <td>-1.477</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.693</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>-0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>-4.651</td>\n",
       "      <td>4.246</td>\n",
       "      <td>-0.823</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.546</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-1.343</td>\n",
       "      <td>0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-5.034</td>\n",
       "      <td>4.993</td>\n",
       "      <td>-1.633</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.181</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>-0.508</td>\n",
       "      <td>-0.283</td>\n",
       "      <td>0.304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>-4.261</td>\n",
       "      <td>1.827</td>\n",
       "      <td>-0.482</td>\n",
       "      <td>-0.194</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.354</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>528 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      y  pred    x01    x02    x03    x04    x05    x06    x07    x08    x09  \\\n",
       "1     1     1 -3.639  0.418 -0.670  1.779 -0.168  1.627 -0.388  0.529 -0.874   \n",
       "2     2     2 -3.327  0.496 -0.694  1.365 -0.265  1.933 -0.363  0.510 -0.621   \n",
       "3     3     3 -2.120  0.894 -1.576  0.147 -0.707  1.559 -0.579  0.676 -0.809   \n",
       "4     4     4 -2.287  1.809 -1.498  1.012 -1.053  1.060 -0.567  0.235 -0.091   \n",
       "5     5     6 -2.598  1.938 -0.846  1.062 -1.633  0.764  0.394 -0.150  0.277   \n",
       "6     6     6 -2.852  1.914 -0.755  0.825 -1.588  0.855  0.217 -0.246  0.238   \n",
       "7     7     7 -3.482  2.524 -0.433  1.048 -1.995  0.902  0.322  0.450  0.377   \n",
       "8     8     8 -3.941  2.305  0.124  1.771 -1.815  0.593 -0.435  0.992  0.575   \n",
       "9     9     9 -3.860  2.116 -0.939  0.688 -0.675  1.679 -0.512  0.928 -0.167   \n",
       "10   10    10 -3.648  1.812 -1.378  1.578  0.065  1.577 -0.466  0.702  0.060   \n",
       "11   11    11 -3.032  1.739 -1.141  0.737 -0.834  1.386 -0.575  0.679 -0.018   \n",
       "12    1     1 -3.653  0.373 -0.600  1.705 -0.222  1.765 -0.353  0.537 -0.797   \n",
       "13    2     2 -3.237  0.436 -0.860  1.363 -0.251  1.915 -0.395  0.751 -0.774   \n",
       "14    3     3 -2.135  0.954 -1.632  0.121 -0.704  1.600 -0.628  0.713 -0.903   \n",
       "15    4     4 -2.304  1.784 -1.506  0.981 -0.961  0.806 -0.294 -0.002  0.119   \n",
       "16    5     5 -2.540  2.144 -1.024  0.933 -1.567  1.024  0.188 -0.047  0.309   \n",
       "17    6     6 -2.826  2.003 -0.738  0.801 -1.669  0.939  0.245 -0.257  0.256   \n",
       "18    7     7 -3.582  2.374 -0.358  1.162 -1.953  0.621  0.339  0.355  0.415   \n",
       "19    8     8 -3.951  2.250  0.127  1.772 -1.906  0.567 -0.432  1.045  0.598   \n",
       "20    9     9 -3.783  1.974 -1.200  0.606 -0.650  1.504 -0.134  0.528  0.392   \n",
       "21   10    10 -3.673  1.811 -1.405  1.621  0.044  1.572 -0.453  0.745 -0.066   \n",
       "22   11    11 -2.946  1.649 -1.167  0.788 -0.909  1.300 -0.562  0.902 -0.070   \n",
       "23    1     1 -3.665  0.337 -0.641  1.791 -0.194  1.686 -0.359  0.570 -0.676   \n",
       "24    2     2 -3.165  0.408 -0.971  1.207 -0.298  1.921 -0.215  0.723 -0.492   \n",
       "25    3     3 -2.105  1.035 -1.705  0.231 -0.558  1.554 -0.649  0.710 -0.855   \n",
       "26    4     4 -2.312  1.746 -1.510  1.019 -0.990  0.941 -0.488  0.208  0.033   \n",
       "27    5     5 -2.635  2.147 -1.129  0.911 -1.407  1.095 -0.071  0.118  0.139   \n",
       "28    6     6 -2.887  2.131 -0.830  0.682 -1.557  0.818  0.448 -0.382  0.207   \n",
       "29    7     7 -3.635  2.250 -0.394  1.012 -1.693  0.117  0.665  0.281  0.343   \n",
       "30    8     8 -3.986  2.325  0.102  1.633 -2.014  0.576 -0.344  1.003  0.566   \n",
       "..   ..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "499   4     4 -3.581  1.188 -0.958  0.189 -0.077  0.451  0.049  0.449 -0.676   \n",
       "500   5     5 -3.828  2.367 -0.836 -0.343 -0.466  0.116  1.172 -0.175 -0.275   \n",
       "501   6     6 -3.820  1.705 -1.130  0.450 -0.471  0.665  0.298 -0.175 -0.473   \n",
       "502   7     7 -3.949  2.990 -0.643 -0.986 -0.175  0.768  0.249  0.803 -1.479   \n",
       "503   8     8 -4.471  4.461 -1.887 -0.799  0.719  0.938  0.239  0.342 -1.036   \n",
       "504   9     9 -4.580  4.116 -0.753 -0.849  0.516  0.498  0.117  0.181 -1.256   \n",
       "505  10    10 -5.049  4.846 -0.678 -0.877 -0.090  0.005  0.253 -0.078 -0.178   \n",
       "506  11    11 -4.115  1.856 -0.738  0.132  0.658  0.329 -0.252 -0.123 -0.530   \n",
       "507   1     1 -5.143  2.051  1.216  1.898 -0.555 -0.466 -0.842  0.011  0.461   \n",
       "508   2     2 -4.403  1.409  0.551  1.496  0.673 -0.616 -0.787 -0.382 -0.696   \n",
       "509   3     3 -4.316  1.272 -0.087  1.285  0.126 -0.128 -0.286 -0.463 -0.294   \n",
       "510   4     4 -3.622  1.111 -0.972  0.399 -0.402  0.527  0.271  0.373 -0.716   \n",
       "511   5     5 -3.850  2.280 -1.055 -0.335 -0.164  0.410  0.913 -0.391 -0.338   \n",
       "512   6     6 -3.962  1.724 -1.290  0.531 -0.170  0.528  0.282 -0.149 -0.603   \n",
       "513   7     7 -3.894  2.910 -0.968 -0.671 -0.131  0.519  0.591  0.708 -1.515   \n",
       "514   8     8 -4.469  4.348 -1.740 -0.886  0.447  0.883  0.374  0.577 -0.908   \n",
       "515   9     9 -4.661  4.235 -0.497 -1.021  0.295  0.457 -0.019  0.345 -1.355   \n",
       "516  10    10 -5.040  5.074 -1.721 -0.264  0.279  0.176  0.003 -0.609 -0.177   \n",
       "517  11    11 -4.169  1.799 -0.621  0.025  0.706  0.362 -0.343 -0.049 -0.339   \n",
       "518   1     1 -5.105  1.968  1.140  1.719 -0.443 -0.432 -0.969 -0.173  0.551   \n",
       "519   2     2 -4.544  1.672  0.643  1.281  0.504 -0.803 -0.709 -0.229 -0.546   \n",
       "520   3     3 -4.426  1.167  0.008  1.315  0.189 -0.297 -0.124 -0.244 -0.423   \n",
       "521   4     4 -3.622  0.988 -1.108  0.685 -0.693  0.754  0.442  0.166 -0.458   \n",
       "522   5     5 -3.597  2.146 -1.492 -0.142 -0.297  0.815  0.870 -0.490 -0.471   \n",
       "523   6     6 -4.122  1.735 -1.340  0.615  0.030  0.353  0.345 -0.104 -0.540   \n",
       "524   7     7 -4.065  2.876 -0.856 -0.221 -0.533  0.232  0.855  0.633 -1.452   \n",
       "525   8     8 -4.513  4.265 -1.477 -1.090  0.215  0.829  0.342  0.693 -0.601   \n",
       "526   9     9 -4.651  4.246 -0.823 -0.831  0.666  0.546 -0.300  0.094 -1.343   \n",
       "527  10    10 -5.034  4.993 -1.633 -0.285  0.398  0.181 -0.211 -0.508 -0.283   \n",
       "528  11    11 -4.261  1.827 -0.482 -0.194  0.731  0.354 -0.478  0.050 -0.112   \n",
       "\n",
       "       x10  \n",
       "1   -0.814  \n",
       "2   -0.488  \n",
       "3   -0.049  \n",
       "4   -0.795  \n",
       "5   -0.396  \n",
       "6   -0.365  \n",
       "7   -0.366  \n",
       "8   -0.301  \n",
       "9   -0.434  \n",
       "10  -0.836  \n",
       "11  -0.823  \n",
       "12  -0.813  \n",
       "13  -0.327  \n",
       "14  -0.027  \n",
       "15  -0.760  \n",
       "16  -0.633  \n",
       "17  -0.458  \n",
       "18  -0.259  \n",
       "19  -0.293  \n",
       "20  -0.580  \n",
       "21  -0.733  \n",
       "22  -0.842  \n",
       "23  -0.841  \n",
       "24  -0.425  \n",
       "25  -0.151  \n",
       "26  -0.847  \n",
       "27  -0.685  \n",
       "28  -0.402  \n",
       "29  -0.003  \n",
       "30  -0.245  \n",
       "..     ...  \n",
       "499  0.919  \n",
       "500  0.117  \n",
       "501  0.848  \n",
       "502  0.168  \n",
       "503 -0.060  \n",
       "504 -0.119  \n",
       "505  0.131  \n",
       "506  0.456  \n",
       "507  0.193  \n",
       "508  1.139  \n",
       "509  0.691  \n",
       "510  0.943  \n",
       "511  0.260  \n",
       "512  1.139  \n",
       "513  0.121  \n",
       "514 -0.089  \n",
       "515  0.062  \n",
       "516  0.422  \n",
       "517  0.220  \n",
       "518  0.176  \n",
       "519  1.058  \n",
       "520  0.684  \n",
       "521  0.738  \n",
       "522  0.410  \n",
       "523  0.988  \n",
       "524  0.272  \n",
       "525 -0.056  \n",
       "526  0.185  \n",
       "527  0.304  \n",
       "528  0.321  \n",
       "\n",
       "[528 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The classification success rate is: ')\n",
    "print((test_dat['y'] == test_dat['pred']).sum() / num_test)\n",
    "print('Prediction for each test data')\n",
    "test_dat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
