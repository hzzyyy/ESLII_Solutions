{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 11.2\n",
    "\n",
    "Consider a neural network for a quantitative outcome as in (11.5), using squared-error loss and identity output function $g_k(t) = t$. \n",
    "Suppose that the weights $\\alpha_m$ from the input to hidden layer are nearly zero. Show that the resulting model is nearly linear in the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without loss of generality consider only one output $(K=1)$. \n",
    "Our goal is to show that funciton: $g\\circ T\\circ\\sigma\\left(X\\right)$ is close to a linear function of $X$, when $\\alpha$ is small.\n",
    "\n",
    "$g$ is identity and $T$ is linear, so we only need to show that $\\sigma\\left(X\\right)$ is close to linear. \n",
    "We calculate the gradient and Hessian of that function with respect to $\\alpha$, and show that the Hessian is higher order infinitesimal with respect to $\\alpha$ than the gradient.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial\\sigma}{\\partial X_{i}} & =\\frac{\\partial\\sigma}{\\partial v}\\alpha_{i},\\\\\n",
    "\\frac{\\partial^{2}\\sigma}{\\partial X_{i}\\partial X_{j}} & =\\left(\\frac{\\partial}{\\partial X_{j}}\\frac{\\partial\\sigma}{\\partial v}\\right)\\alpha_{i}\\\\\n",
    " & =\\frac{\\partial^{2}\\sigma}{\\partial v^{2}}\\alpha_{i}\\alpha_{j}.\n",
    "\\end{align*}\n",
    "\n",
    "The result is obvious, when $\\alpha \\rightarrow 0$, the Hessian matrix goes to $0$ with higher order than the gradient does, making it close to linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 11.4\n",
    "\n",
    "Consider a neural network for a K class outcome that uses cross- entropy loss. If the network has no hidden layer, show that the model is equivalent to the multinomial logistic model described in Chapter 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no hidden layer, $K = M$ the final softmax output is: \n",
    "\n",
    "\\begin{align*}\n",
    "g_{k}\\left(X\\right) & =\\frac{e^{\\alpha_{0k}+\\alpha_{k}^{T}X}}{\\Sigma_{k=1}^{K}e^{\\alpha_{0k}+\\alpha_{k}^{T}X}}\\\\\n",
    " & =\\frac{e^{-\\alpha_{0K}-\\alpha_{K}^{T}X}\\cdot e^{\\alpha_{0k}+\\alpha_{k}^{T}X}}{e^{-\\alpha_{0K}-\\alpha_{K}^{T}X}\\cdot\\Sigma_{k=1}^{K}e^{\\alpha_{0k}+\\alpha_{k}^{T}X}}\\\\\n",
    " & =\\frac{e^{\\left(\\alpha_{0k}-\\alpha_{0K}\\right)+\\left(\\alpha_{k}^{T}-\\alpha_{K}^{T}\\right)X}}{\\Sigma_{k=1}^{K}e^{\\left(\\alpha_{0k}-\\alpha_{0K}\\right)+\\left(\\alpha_{k}^{T}-\\alpha_{K}^{T}\\right)X}}\\\\\n",
    " & =\\frac{e^{\\beta_{0k}+\\beta_{k}^{T}X}}{\\Sigma_{k=1}^{K}e^{e^{\\beta_{0k}+\\beta_{k}^{T}X}}},\\\\\n",
    "\\text{where}\\quad\\beta_{0K} & =0,\\\\\n",
    "\\beta_{k} & =\\mathbf{0},\\\\\n",
    "e^{\\beta_{0K}+\\beta_{K}^{T}X} & =1.\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
