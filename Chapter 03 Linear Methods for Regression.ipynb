{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 3.2\n",
    "\n",
    "This excercise covers the statistical tests for the parameters and the prediction. We are going to derive the relationship, under the same confidence level, between the interval projected by the confidence parameter set and the confidence interval itself. The conclusion is: \n",
    "\n",
    "**In linear regression model assuming iid normal errors, with the same confidence level, the  interval of expected prediction generated by the confidence paramter set strictly covers the confidence interval of the expected prediction symmetrically.**\n",
    "\n",
    "By expected prediction we mean the true value of the dependent variable less the error term, which is $\\mathrm{E}[y_0]=f(x_0)=x_0^T\\beta$.\n",
    "\n",
    "The proof is done without any assumption on the problem setup. Namely lets denote the true model as:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{X} \\beta + \\mathbf{\\varepsilon},$$\n",
    "where $\\mathbf{y}$ and $\\mathbf{X}$ are the training set with $N$ entries. $\\mathbf{X}$ is a $N\\times p$ matrix, and in this perticular problem, the first colume of $\\mathbf{X}$ is all one, the second column is $X_i$, the third column is $X_i^2$, etc. We further define deviation of the fitted parameters from its true value by $\\beta^* = \\hat{\\beta}-\\beta$, then the deviation of the prediction is $\\hat{f}(x_0)-f(x_0)=x_0^T\\beta^*$. We further use this operator: $(\\cdot)^{(1-\\alpha)}$ to denote the $1-\\alpha$ quantile of a random variable.\n",
    "We know $x_0^T\\beta^*$ is normally distributed with $0$ mean, and the confidence interval of $f(x_0)$ is simply:\n",
    "$$\\left(\\hat{f}(x_0)-\\left(x_0^T\\beta^*\\right)^{(1-\\alpha/2)},\\,\n",
    "\\hat{f}(x_0)+\\left(x_0^T\\beta^*\\right)^{(1-\\alpha/2)}\\right).$$\n",
    "\n",
    "The prediction interval generated by confidence parameter set is slighly more complicated. One may already noticed that there is a typo in 3.15, where the $\\hat{\\sigma}^2$ should be replaced by $\\sigma^2$. The $1-\\alpha$ confidence set of $\\beta$ is:\n",
    "\n",
    "$$C_{\\beta}=\\left\\{ \\beta\\vert\\beta^{*T}\\mathbf{X}^{T}\\mathbf{X}\\beta^{*}\\le\\left(\\sigma^{2}\\chi_{p}^{2}\\right)^{(1-\\alpha)}=\\hat{\\sigma}^{2}pF_{p,N-p}^{(1-\\alpha)}\\right\\},$$\n",
    "and the interval of expected prediction generated by this confidence set is:\n",
    "$$\\left(\\hat{f}(x_{0})+\\min_{\\beta^{*}\\in C_{\\beta}}\\left(x_{0}^{T}\\beta^{*}\\right),\\,\\hat{f}(x_{0})+\\max_{\\beta^{*}\\in C_{\\beta}}\\left(x_{0}^{T}\\beta^{*}\\right)\\right).$$\n",
    "\n",
    "The two end points of the above interval are determined by optimization on the linear objective function $x_{0}^{T}\\beta^{*}$ with quadratic constraint $C_\\beta$. Since the objective function is linear, we know it obtains both its mininum and maximum at the boundary of the contraint. The constraint set is symmetric about the origin, we further know that the minimum and maximum are symmetric about the origin as well. Namely, let's denote the boundary of $C_\\beta$ as:\n",
    "$$B_{\\beta}=\\left\\{ \\beta\\vert\\beta^{*T}\\mathbf{X}^{T}\\mathbf{X}\\beta^{*}=\\hat{\\sigma}^{2}pF_{p,N-p}^{(1-\\alpha)}\\right\\}.$$\n",
    "\n",
    "We simplify the second interval as:\n",
    "$$\\left(\\hat{f}(x_{0})-\\max_{\\beta^{*}\\in B_{\\beta}}\\left(x_{0}^{T}\\beta^{*}\\right),\\,\\hat{f}(x_{0})+\\max_{\\beta^{*}\\in B_{\\beta}}\\left(x_{0}^{T}\\beta^{*}\\right)\\right).$$\n",
    "\n",
    "The plane $x_{0}^{T}\\beta^{*}=\\max_{\\beta^{*}\\in B_{\\beta}}\\left(x_{0}^{T}\\beta^{*}\\right)$ must be tangential to $B_\\beta$, we can use the following two equations to solve for\n",
    "$\\max_{\\beta^{*}\\in B_{\\beta}}\\left(x_{0}^{T}\\beta^{*}\\right)$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\beta^{*T}\\mathbf{X}^{T}\\mathbf{X}\\beta^{*} & =\\hat{\\sigma}^{2}pF_{p,N-p}^{(1-\\alpha)}\\\\\n",
    "\\frac12\\nabla_{\\beta^{*}}\\left(\\beta^{*T}\\mathbf{X}^{T}\\mathbf{X}\\beta^{*}\\right)=\\mathbf{X}^{T}\\mathbf{X}\\beta^{*} & =\\lambda x_{0}=\\lambda\\nabla_{\\beta^{*}}\\left(x_{0}^{T}\\beta^{*}\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "The solution is obtained by eliminating $\\lambda$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{X}^{T}\\mathbf{X}\\beta^{*} & =\\lambda x_{0}\\\\\n",
    "\\Rightarrow\\beta^{*} & =\\lambda\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}\\\\\n",
    "\\Rightarrow\\beta^{*T}\\mathbf{X}^{T}\\mathbf{X}\\beta^{*} & =\\lambda^{2}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}\\\\\n",
    " & =\\lambda^{2}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}\\\\\n",
    " & =\\hat{\\sigma}^{2}pF_{p,N-p}^{(1-\\alpha)}\\\\\n",
    "\\Rightarrow x_{0}^{T}\\beta^{*} & =\\lambda x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}\\\\\n",
    " & =\\sqrt{\\hat{\\sigma}^{2}pF_{p,N-p}^{(1-\\alpha)}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "On the other hand, we know that \n",
    "$$x_{0}^{T}\\beta^{*}=x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\varepsilon,$$ \n",
    "and that \n",
    "$$\n",
    "\\mathrm{Var}\\left(x_{0}^{T}\\beta^{*}\\right)=x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}\\mathrm{Cov}(\\varepsilon)\\mathbf{X}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}=\\sigma^{2}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}.\n",
    "$$\n",
    "\n",
    "The distribution of $x_{0}^{T}\\beta^{*}$ is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_{0}^{T}\\beta^{*} & \\sim\\mathcal{N}\\left(0,\\,\\sigma^{2}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}\\right)\n",
    "\\end{align*}.\n",
    "$$\n",
    "$\\sigma^2$ is unkown, so the confidence interval of $x_{0}^{T}\\beta^{*}$ is based on $t$ distribution:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{x_{0}^{T}\\beta^{*}}{\\sqrt{\\hat{\\sigma}^{2}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}}} & \\sim t_{N-p}\\\\\n",
    "\\left(x_{0}^{T}\\beta^{*}\\right)^{(1-\\alpha/2)} & =t_{N-p}^{(1-\\alpha/2)}\\sqrt{\\hat{\\sigma}^{2}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}}\\\\\n",
    " & =\\sqrt{\\hat{\\sigma}^{2}\\left(t_{N-p}^{(1-\\alpha/2)}\\right)^{2}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}}\\\\\n",
    " & =\\sqrt{\\hat{\\sigma}^{2}F_{1,N-p}^{(1-\\alpha)}x_{0}^{T}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}x_{0}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since\n",
    "$$\n",
    "\\begin{align*}\n",
    "pF_{p,N-p}^{(1-\\alpha)} & =\\left(\\frac{\\chi_{p}^{2}}{\\chi_{N-p}^{2}/(N-p)}\\right)^{(1-\\alpha)}\\\\\n",
    " & >\\left(\\frac{\\chi_{1}^{2}}{\\chi_{N-p}^{2}/(N-p)}\\right)^{(1-\\alpha)}\\\\\n",
    " & =F_{1,N-p}^{(1-\\alpha)}\n",
    "\\end{align*}\n",
    "$$\n",
    "We know:\n",
    "$$\n",
    "\\max_{\\beta^{*}\\in B_{\\beta}}\\left(x_{0}^{T}\\beta^{*}\\right)>\\left(x_{0}^{T}\\beta^{*}\\right)^{(1-\\alpha/2)}.\n",
    "$$\n",
    "This completes the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 3.6\n",
    "\n",
    "With Bayes' rule, we can find the relation that the $\\beta$ maximizing the posterior density is the ridge regression of $\\beta$ with a parameter of $\\lambda = \\frac{\\sigma^2}{\\tau^2}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\beta|\\mathbf{y}) & =\\frac{p(\\mathbf{y}|\\beta)}{p(\\mathbf{y})}\\cdot p(\\beta)\\\\\n",
    " & =\\frac{1}{p(\\mathbf{y})}\\cdot p(\\mathbf{y}|\\beta)\\prod_{i=1}^{p}\\frac{1}{\\sqrt{2\\pi\\tau^{2}}}e^{-\\frac{\\beta_{i}^{2}}{2\\tau^{2}}}\\\\\n",
    " & =\\frac{1}{p(\\mathbf{y})}\\cdot\\prod_{j=1}^{p}\\frac{1}{\\sqrt{2\\pi\\tau^{2}}}e^{-\\frac{\\beta_{j}^{2}}{2\\tau^{2}}}\\prod_{i=1}^{N}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(y_{i}-\\mathbf{X}\\beta)^{2}}{2\\sigma^{2}}}\\\\\n",
    "\\Rightarrow-\\ln p(\\beta|\\mathbf{y}) & =\\ln p(\\mathbf{y})+\\frac{p}{2}\\ln\\left(2\\pi\\tau^{2}\\right)+\\frac{N}{2}\\ln\\left(2\\pi\\sigma^{2}\\right)+\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{X}\\beta\\right)^{2}+\\frac{1}{2\\tau^{2}}\\sum_{j=1}^{p}\\beta_{j}^{2}\\\\\n",
    " & =\\ln p(\\mathbf{y})+\\frac{p}{2}\\ln\\left(2\\pi\\tau^{2}\\right)+\\frac{N}{2}\\ln\\left(2\\pi\\sigma^{2}\\right)+\\frac{1}{2\\sigma^{2}}\\left(\\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{X}\\beta\\right)^{2}+\\frac{\\sigma^{2}}{\\tau^{2}}\\sum_{j=1}^{p}\\beta_{j}^{2}\\right)\\\\\n",
    "\\Rightarrow\\mathrm{argmax}_{\\beta}p(\\beta|\\mathbf{y}) & =\\mathrm{argmin}_{\\beta}\\left(\\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{X}\\beta\\right)^{2}+\\frac{\\sigma^{2}}{\\tau^{2}}\\sum_{j=1}^{p}\\beta_{j}^{2}\\right)\\\\\n",
    "\\mbox{together with }\\hat{\\beta}(\\lambda)^{\\mathrm{ridge}} & =\\mathrm{argmin}_{\\beta}\\left(\\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{X}\\beta\\right)^{2}+\\lambda\\sum_{j=1}^{p}\\beta_{j}^{2}\\right)\\\\\n",
    "\\Rightarrow\\mathrm{argmax}_{\\beta}p(\\beta|\\mathbf{y}) & =\\hat{\\beta}(\\frac{\\sigma^{2}}{\\tau^{2}})^{\\mathrm{ridge}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We know that $p(\\beta|\\mathbf{y})$ is Gaussian, therefore, the density reaches its maximum (mode) at its mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 3.12\n",
    "\n",
    "In the augmented regression, the true model is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\left[\\begin{array}{c}\n",
    "\\mathbf{y}\\\\\n",
    "0\n",
    "\\end{array}\\right]= & \\left[\\begin{array}{c}\n",
    "\\mathbf{X}\\\\\n",
    "\\sqrt{\\lambda}\\mathbf{I}\n",
    "\\end{array}\\right]\\beta+\\mathbf{\\varepsilon}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This will produce the least square estimator of $\\beta$ as the \n",
    "following:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\beta} & =\\left(\\left[\\begin{array}{cc}\n",
    "\\mathbf{X}^{T} & \\sqrt{\\lambda}\\mathbf{I}\\end{array}\\right]\\cdot\\left[\\begin{array}{c}\n",
    "\\mathbf{X}\\\\\n",
    "\\sqrt{\\lambda}\\mathbf{I}\n",
    "\\end{array}\\right]\\right)^{-1}\\cdot\\left[\\begin{array}{cc}\n",
    "\\mathbf{X}^{T} & \\sqrt{\\lambda}\\mathbf{I}\\end{array}\\right]\\cdot\\left[\\begin{array}{c}\n",
    "\\mathbf{y}\\\\\n",
    "0\n",
    "\\end{array}\\right]\\\\\n",
    " & =\\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\mathbf{I}\\right)^{-1}\\cdot\\left(\\mathbf{X}^{T}\\mathbf{y}+\\sqrt{\\lambda}\\mathbf{I}\\right)\\\\\n",
    " & =\\left(\\mathbf{X}^{T}\\mathbf{X}+\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{X}^{T}\\mathbf{y}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This is exactly the same as the formula in ridge regression (3.44)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 3.23\n",
    "## (a)\n",
    "$\\mathbf{x}_j$ is the $j$th column of $\\mathbf{X}$, also the realizations of the $j$th regressor. Let's define $\\mathbf{e}_j$ as the column vector with the $j$th entry being $1$ and all the rest $p-1$ entries $0$. In this way we can denote $\\mathbf{x}_j$ as $\\mathbf{Xe}_j$.\n",
    "\n",
    "The covariances of each $\\mathbf{x}_j$ with the residuals can be conveniently written as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1}{N}\\left|\\left\\langle \\mathbf{x}_{j},\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right\\rangle \\right| & =\\frac{1}{N}\\left|\\left\\langle \\mathbf{x}_{j},\\,\\mathbf{y}-\\alpha\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}\\right\\rangle \\right|\\\\\n",
    " & =\\frac{1}{N}\\left|\\left\\langle \\mathbf{x}_{j},\\,\\left(\\mathbf{I}-\\alpha\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\right)\\mathbf{y}\\right\\rangle \\right|\\\\\n",
    " & =\\frac{1}{N}\\left|\\mathbf{x}_{j}^{T}\\left(\\mathbf{I}-\\alpha\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\right)\\mathbf{y}\\right|\\\\\n",
    " & =\\frac{1}{N}\\left|\\left(\\mathbf{x}_{j}^{T}\\mathbf{y}-\\alpha(\\mathbf{Xe}_{j})^{T}\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}\\right)\\right|\\\\\n",
    " & =\\frac{1}{N}\\left|\\left(\\mathbf{x}_{j}^{T}\\mathbf{y}-\\alpha\\mathbf{e}_{j}^{T}\\mathbf{X}^{T}\\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}\\right)\\right|\\\\\n",
    " & =\\frac{1}{N}\\left|\\left(\\mathbf{x}_{j}^{T}\\mathbf{y}-\\alpha\\mathbf{e}_{j}^{T}\\mathbf{X}^{T}\\mathbf{y}\\right)\\right|\\\\\n",
    " & =\\frac{1}{N}\\left|\\left(\\mathbf{x}_{j}^{T}\\mathbf{y}-\\alpha\\mathbf{x}_{j}^{T}\\mathbf{y}\\right)\\right|\\\\\n",
    " & =(1-\\alpha)\\frac{1}{N}\\left|\\left(\\mathbf{x}_{j}^{T}\\mathbf{y}\\right)\\right|\\\\\n",
    " & =(1-\\alpha)\\frac{1}{N}\\left|\\left\\langle \\mathbf{x}_{j},\\,\\mathbf{y}\\right\\rangle \\right|\\\\\n",
    " & =(1-\\alpha)\\lambda.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## (b)\n",
    "We know that the response $\\mathbf{y}$ and each $\\mathbf{x}_j$ has been normalized to have zero mean. So it is obvious that $\\mathbf{u}(\\alpha)$ also has zero mean, because it is a linear combination of zero-mean vectors. Let's futher denote $\\mathbf{P}$ as the matrix: $\\mathbf{P}=\\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T$, and it is easy to varify that it is idempotent: $\\mathbf{PP}=\\mathbf{P}$. Then we know that:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{Var}\\left(\\mathbf{y}-\\mathbf{u}(\\alpha)\\right) & =\\mathrm{Var}\\left(\\mathbf{y}-\\alpha\\mathbf{Py}\\right)\\\\\n",
    " & =\\frac{1}{N}\\left(\\mathbf{y}^{T}\\left(\\mathbf{I}-\\alpha\\mathbf{P}\\right)\\left(\\mathbf{I}-\\alpha\\mathbf{P}\\right)\\mathbf{y}\\right)\\\\\n",
    " & =\\frac{1}{N}\\left(\\mathbf{y}^{T}\\left(\\mathbf{I}-2\\alpha\\mathbf{P}+\\alpha^{2}\\mathbf{P}\\right)\\mathbf{y}\\right)\\\\\n",
    " & =\\frac{1}{N}\\left(\\mathbf{y}^{T}\\left(\\alpha\\left(2-\\alpha\\right)\\mathbf{I}-\\alpha\\left(2-\\alpha\\right)\\mathbf{P}+(1-\\alpha)^{2}\\mathbf{I}\\right)\\mathbf{y}\\right)\\\\\n",
    " & =\\frac{1}{N}\\left(\\alpha\\left(2-\\alpha\\right)\\mathbf{y}^{T}\\left(\\mathbf{I}-\\mathbf{P}\\right)\\mathbf{y}+(1-\\alpha)^{2}\\mathbf{y}^{T}\\mathbf{y}\\right)\\\\\n",
    " & =\\frac{1}{N}\\alpha\\left(2-\\alpha\\right)\\mathrm{RSS}+(1-\\alpha)^{2}\\frac{\\mathbf{y}^{T}\\mathbf{y}}{N}\\\\\n",
    " & =(1-\\alpha)^{2}+\\frac{\\alpha\\left(2-\\alpha\\right)}{N}\\cdot\\mathrm{RSS}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Finally, we proved that:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{Cor}\\left(\\mathbf{x}_{j},\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right) & =\\frac{\\mathrm{Cov}\\left(\\mathbf{x}_{j},\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right)}{\\sqrt{\\mathrm{Var}\\left(\\mathbf{x}_{j}\\right)}\\cdot\\sqrt{\\mathrm{Var}\\left(\\mathbf{y}-\\mathbf{u}(\\alpha)\\right)}}\\\\\n",
    " & =\\frac{(1-\\alpha)\\lambda}{1\\cdot\\sqrt{\\alpha\\left(2-\\alpha\\right)\\frac{\\mathrm{RSS}}{N}+(1-\\alpha)^{2}}}\\\\\n",
    " & =\\frac{(1-\\alpha)}{\\sqrt{(1-\\alpha)^{2}+\\frac{\\alpha\\left(2-\\alpha\\right)}{N}\\cdot\\mathrm{RSS}}}\\cdot\\lambda.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## (c)\n",
    "\n",
    "We know that the corelations between the $p$ regressors $\\mathbf{x}_j$ and the residul as a function of $\\alpha$ are the same, which is formlated in (b). Let's reformulate it as:\n",
    "$$\\frac{\\lambda}{\\sqrt{(1-\\frac{\\mathrm{RSS}}{N})+\\frac{1}{(1-\\alpha)^{2}}\\frac{\\mathrm{RSS}}{N}}}$$\n",
    "\n",
    "It is obvious that it is monotonically decreasing as $\\alpha$ goes from $0$ to $1$.\n",
    "\n",
    "In the LAR algorithm, as the corelations between the residual and regressors in the active set monotonically decreases, at some point, this corelation will become equal to the largest corelation between the residual and remaining inactive regressors. When this happens, we move this regressor from inactive set to active set, initialize its coefficient as $0$, and recompute the moving direction with the updated active regressors. As we can see, during this whole process, the corelations between the residual and active regressors are kept same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 3.24\n",
    "In Ex. 3.23, we just proved that, in LAR algorithm, the residual $\\mathbf{y}-\\mathbf{u}(\\alpha)$ has the same corelation with all the current predictor vectors $\\mathbf{x}_j$. In geometry, this corelation is just the cosine of the angle between $\\mathbf{y}-\\mathbf{u}(\\alpha)$ and $\\mathbf{x}_j$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{Cor}\\left(\\mathbf{x}_{j},\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right) & =\\frac{\\mathrm{Cov}\\left(\\mathbf{x}_{j},\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right)}{\\sqrt{\\mathrm{Var}\\left(\\mathbf{x}_{j}\\right)}\\cdot\\sqrt{\\mathrm{Var}\\left(\\mathbf{y}-\\mathbf{u}(\\alpha)\\right)}}\\\\\n",
    " & =\\frac{\\left\\langle \\mathbf{x}_{j},\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right\\rangle }{\\sqrt{\\left\\langle \\mathbf{x}_{j},\\,\\mathbf{x}_{j}\\right\\rangle }\\cdot\\sqrt{\\left\\langle \\mathbf{y}-\\mathbf{u}(\\alpha),\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right\\rangle }}\\\\\n",
    " & =\\cos\\left(\\mathrm{angle}\\left(\\mathbf{x}_{j},\\,\\mathbf{y}-\\mathbf{u}(\\alpha)\\right)\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In this problem, we try to prove that the new direction has the same angle to all the active regressors. We know that the variances of $\\mathbf{x}_j$ are all $1$ hence independent of $j$. All we need to do is to prove that the covariances between $\\mathbf{x}_j$ and $\\mathbf{u}_k$ are independent of $j$.\n",
    "\n",
    "At the $k$th step, this covariance is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{Cov}\\left(\\mathbf{x}_{j},\\,\\mathbf{u}_{k}\\right) & =\\frac{1}{N}\\left(\\mathbf{X}_{\\mathcal{A}_{k}}\\mathbf{e}_{j}\\right)^{T}\\mathbf{u}_{k}\\\\\n",
    " & =\\frac{1}{N}\\mathbf{e}_{j}^{T}\\mathbf{X}_{\\mathcal{A}_{k}}^{T}\\mathbf{X}_{\\mathcal{A}_{k}}\\left(\\mathbf{X}_{\\mathcal{A}_{k}}^{T}\\mathbf{X}_{\\mathcal{A}_{k}}\\right)^{-1}\\mathbf{X}_{\\mathcal{A}_{k}}^{T}\\mathbf{r}_{k}\\\\\n",
    " & =\\frac{1}{N}\\mathbf{e}_{j}^{T}\\mathbf{X}_{\\mathcal{A}_{k}}^{T}\\mathbf{r}_{k}\\\\\n",
    " & =\\mathrm{Cov}\\left(\\mathbf{x}_{j},\\,\\mathbf{r}_{k}\\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "$\\mathbf{r}_k$ is the residual of at the end of the $k-1$th step, we know from Ex. 3.23 that its corelations with all the $k-1$ regressors in the previous step are the same. As for the newly added regressor, it is also the same because that's how it is picked. We just proved that, at the begining of a step, the covariances between the moving direction of the predictor $\\hat{\\mathbf{y}}$ and the active regressors and the covariances between the residual and the active regressors are the same. Geometrically speaking, the moving direction has the same angle with all the active regressors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 3.25\n",
    "Let's first denote a regressor in the inactive set as $\\mathbf{x}_i$ and a regressor in the active set as $\\mathbf{x}_j$. Let's assume the covariance between the residual at the begnning of the $k$th step and the active regressors to be $\\lambda$. As we gradually increase the value of $\\alpha$ we know that this covariance is gradually decreasing as $(1-\\alpha)\\lambda$.\n",
    "\n",
    "The goal is to find the inactive regressor, with the samllest $\\alpha$, that will have the equal corelation between the changing residual, and that between the active regressor and the changing residual:\n",
    "$$\\mathrm{argmin}_{i}\\left\\{ \\alpha\\vert\\mathrm{Cor}\\left(\\mathbf{x}_{i},\\,\\mathbf{r}_{k}(\\alpha)\\right)=\\mathrm{Cor}\\left(\\mathbf{x}_{j},\\,\\mathbf{r}_{k}(\\alpha)\\right)\\right\\}.$$\n",
    "\n",
    "We further know that $\\mathrm{Var}(\\mathbf{x}_{i})=\\mathrm{Var}(\\mathbf{x}_{j})=1$, then equal corelation implies equal covariance:\n",
    "$$\\mathrm{argmin}_{i}\\left\\{ \\alpha\\vert\\mathrm{Cov}\\left(\\mathbf{x}_{i},\\,\\mathbf{r}_{k}(\\alpha)\\right)=\\mathrm{Cov}\\left(\\mathbf{x}_{j},\\,\\mathbf{r}_{k}(\\alpha)\\right)=(1-\\alpha)\\lambda\\right\\}. $$\n",
    "\n",
    "Namely let's denote the predicton matrix constructed by the active regressor at $k$th step as:\n",
    "$$\\mathbf{P}_{k}=\\mathbf{X}_{\\mathcal{A}_{k}}\\left(\\mathbf{X}_{\\mathcal{A}_{k}}^{T}\\mathbf{X}_{\\mathcal{A}_{k}}\\right)^{-1}\\mathbf{X}_{\\mathcal{A}_{k}}^{T}.$$\n",
    "\n",
    "Then the changing residual at step $k$ as a function of $\\alpha$ is:\n",
    "$$\\mathbf{r}_{k}(\\alpha)=(\\mathbf{I}-\\alpha\\mathbf{P}_{k})\\mathbf{r}_{k}.$$\n",
    "\n",
    "At the beginning of $k$th step, when regressing the residual $\\mathbf{r}_{k}$ on active regressors $\\mathbf{X}_{\\mathcal{A}_{k}}$, we also regress all the inactive regressors on the active regressors and obtain the their predictions. We denote them as $\\hat{\\mathbf{x}}_i=\\mathbf{P}_k\\mathbf{x}_i$.\n",
    "\n",
    "Finally, we can solve for the enterning regressor at the $k+1$th step as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{argmin}_{i}\\left\\{ \\alpha\\vert\\mathrm{Cov}\\left(\\mathbf{x}_{i},\\,\\mathbf{r}_{k}(\\alpha)\\right)=(1-\\alpha)\\lambda\\right\\}  & =\\mathrm{argmin}_{i}\\left\\{ \\alpha\\vert\\mathrm{Cov}\\left(\\mathbf{x}_{i},\\,(\\mathbf{I}-\\alpha\\mathbf{P}_{k})\\mathbf{r}_{k}\\right)=(1-\\alpha)\\lambda\\right\\} \\\\\n",
    " & =\\mathrm{argmin}_{i}\\left\\{ \\alpha\\vert\\mathrm{Cov}\\left(\\mathbf{x}_{i},\\,\\mathbf{r}_{k}\\right)-\\alpha\\mathrm{Cov}\\left(\\mathbf{x}_{i},\\,\\mathbf{P}_{k}\\mathbf{r}_{k}\\right)=(1-\\alpha)\\lambda\\right\\} \\\\\n",
    " & =\\mathrm{argmin}_{i}\\left\\{ \\alpha\\vert\\mathrm{Cov}\\left(\\mathbf{x}_{i},\\,\\mathbf{r}_{k}\\right)-\\frac{\\alpha}{N}\\left(\\mathbf{x}_{i}^{T}\\mathbf{P}_{k}\\mathbf{r}_{k}\\right)=(1-\\alpha)\\lambda\\right\\} \\\\\n",
    " & =\\mathrm{argmin}_{i}\\left\\{ \\alpha\\vert\\mathrm{Cov}\\left(\\mathbf{x}_{i},\\,\\mathbf{r}_{k}\\right)-\\frac{\\alpha}{N}\\left(\\left(\\mathbf{P}_{k}\\mathbf{x}_{i}\\right)^{T}\\mathbf{r}_{k}\\right)=(1-\\alpha)\\lambda\\right\\} \\\\\n",
    " & =\\mathrm{argmin}_{i}\\left\\{ \\alpha\\vert\\mathrm{Cov}\\left(\\mathbf{x}_{i},\\,\\mathbf{r}_{k}\\right)-\\frac{\\alpha}{N}\\left(\\mathbf{\\hat{x}}_{i}^{T}\\mathbf{r}_{k}\\right)=(1-\\alpha)\\lambda\\right\\} \\\\\n",
    " & =\\mathrm{argmin}_{i}\\left\\{ \\alpha\\vert\\mathrm{Cov}\\left(\\mathbf{x}_{i},\\,\\mathbf{r}_{k}\\right)-\\alpha\\mathrm{Cov}\\left(\\mathbf{\\hat{x}}_{i},\\,\\mathbf{r}_{k}\\right)=(1-\\alpha)\\lambda\\right\\} \\\\\n",
    " & =\\mathrm{argmin}_{i}\\left\\{ \\frac{\\lambda-\\mathrm{Cov}\\left(\\mathbf{x}_{i},\\,\\mathbf{r}_{k}\\right)}{\\lambda-\\mathrm{Cov}\\left(\\mathbf{\\hat{x}}_{i},\\,\\mathbf{r}_{k}\\right)}\\right\\}. \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The $\\alpha$ is just the minimized value in the above equation: \n",
    "$$\\alpha_\\mathrm{min}=\\mathrm{min}_{i}\\left\\{ \\frac{\\lambda-\\mathrm{Cov}\\left(\\mathbf{x}_{i},\\,\\mathbf{r}_{k}\\right)}{\\lambda-\\mathrm{Cov}\\left(\\mathbf{\\hat{x}}_{i},\\,\\mathbf{r}_{k}\\right)}\\right\\}. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 3.27\n",
    "\n",
    "The primal problem is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\underset{\\mathbf{\\beta}}{\\mathrm{minimize}}\\quad & \\frac12\\left(\\mathbf{y}-\\mathbf{X\\beta}\\right)^{T}\\left(\\mathbf{y}-\\mathbf{X\\beta}\\right)\\\\\n",
    "\\mbox{subject to}\\quad & \\left\\Vert \\mathbf{\\beta}\\right\\Vert _{1} \\le t\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{\\beta}$ is a $p\\times 1$ vector of parameters, and $\\mathbf{X}$ is a $N\\times p$ matrix. We can first quickly show that strong duality holds for the primal problem.\n",
    "\n",
    "The objective function $\\frac12\\left(\\mathbf{y}-\\mathbf{X\\beta}\\right)^{T}\\left(\\mathbf{y}-\\mathbf{X\\beta}\\right)$, is a quadratic form and has the constant Hessian matrix $\\mathbf{X}^T\\mathbf{X}$. We know that this matrix is positive semidefinite, and thus the objective function is convex. We have only one inequity constraint, and the function is $L_1$ norm of $\\mathbf{\\beta}$ minus a constant. By triangle inequitlity and absolute scalability of a norm function, we know that any norm function is a convex function. A convex function minus a constant is still a convex function, so we conclude that the function in the inequality constraint is also convex. In addition, the domain for both the objective funciton and constraint function is $\\mathbb{R}^p$ and thus convex. We conclue that the primal problem is convex. \n",
    "\n",
    "We can easily verify that the point $\\mathbf{\\beta}=\\mathbf{0}$ is an interior point of the primal domain $\\mathbb{R}^p$, and that it is strictly primal feasible: $\\left\\Vert \\mathbf{0}\\right\\Vert _{1}-t = -t < 0$. The Slater condition is satisfied for the primal problem, together with the fact that it is convex, we can conclude that strong duality holds for this problem. With strong duality, KKT condition is necessary and sufficient for optimality.\n",
    "\n",
    "## (a)\n",
    "The goal here is to find an equivalent primal problem so that all the constraint functions are differentiable on their own domain. For each of $\\beta_j$, let's introduce a pair of two predictors $\\beta_j^+$ and $\\beta_j^-$, such that:\n",
    "\n",
    "\\begin{align*}\n",
    "\\beta_{j}^{+} & =\\begin{cases}\n",
    "\\beta_{j} & \\beta_{j}>0\\\\\n",
    "0 & \\beta_{j}\\le0\n",
    "\\end{cases}\\\\\n",
    "\\beta_{j}^{-} & =\\begin{cases}\n",
    "0 & \\beta_{j}>0\\\\\n",
    "-\\beta_{j} & \\beta_{j}\\le0\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "In this way, we can write both $\\beta_j$ and its absolute value as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\beta_{j} & =\\beta_{j}^{+}-\\beta_{j}^{-},\\\\\n",
    "\\left|\\beta_{j}\\right| & =\\beta_{j}^{+}+\\beta_{j}^{-}.\n",
    "\\end{align*}\n",
    "\n",
    "The original primal problem becomes: \n",
    "\n",
    "\\begin{align*}\n",
    "\\underset{\\mathbf{\\beta}^{+},\\mathbf{\\beta}^{-}}{\\mathrm{minimize}}\\quad & \\left(\\mathbf{y}-\\mathbf{X\\left(\\mathbf{\\beta}^{+}-\\mathbf{\\beta}^{-}\\right)}\\right)^{T}\\left(\\mathbf{y}-\\mathbf{X\\left(\\mathbf{\\beta}^{+}-\\mathbf{\\beta}^{-}\\right)}\\right)\\\\\n",
    "\\mbox{subject to}\\quad & \\begin{array}{ccc}\n",
    "\\mathbf{1}^{T}\\left(\\mathbf{\\beta}^{+}+\\mathbf{\\beta}^{-}\\right)-t & \\le0\\\\\n",
    "-\\mathbf{\\beta}^{+} & \\le0\\\\\n",
    "-\\mathbf{\\beta}^{-} & \\le0\\\\\n",
    "\\mathbf{\\beta}_{j}^{+}\\mathbf{\\beta}_{j}^{-} & =0 & j=1,\\,\\dots,\\,p\n",
    "\\end{array}\n",
    "\\end{align*}\n",
    "\n",
    "The last three conditions are a direct result of $\\beta^+$ and $\\beta^-$ being the postive and negative part of $\\beta$. The last condition is not convex, complicating this problem. Let's see what happens if we drop the last condition. \n",
    "\n",
    "Without the last condition, we get a convex primal problem. In addition, all the constraint functions are affine, so the strong duality holds. If there exits a solution satifying all the KKT conditions, then it must be optimal. Let's first assume that such a solution does exit and we obtain the minimum value of objective function without the last constraint, namely $\\hat{p}^\\star$. Let's denote the optimal of the problem with the last constraint as $p^\\star$, then we must have $\\hat{p}^\\star \\le p^\\star$ because the former is the minimum on a larger feasible set. By larger, we mean one set contrains the other.\n",
    "\n",
    "If the solution producing $\\hat{p}^\\star$ satisfies the last condition then we have $\\hat{p}^\\star = p^\\star$. If it doesn't satisfy the last condition, then in this solution, there is at least one pair of positive $\\beta_j^+$ and $\\beta_j^-$. Without loss of generality, let's assume  $\\beta_j^+ =a > \\beta_j^-=b>0$. The exciting fact is that if we update this pair with new values: $\\beta_j^+ = a - b$ and $\\beta_j^-=0$, the value of the objective function doesn't change, and all the previously satisfied constraints are still satisfied. For the last constraint, we have one less pair of $\\beta_j^+$ and $\\beta_j^-$ violating it. By repeating such process for all the pairs violating the last constaint, we can modify the solution producing $\\hat{p}^\\star$ to become a feasible solution for the problem with the last constraint. Since this modifying process doesn't change the objective function value, we proved that there is a feasible solution in the problem with the last constraint producing an objective of $\\hat{p}^\\star$. $p^\\star$ must be less or equal to the value produced by any of its feasible solution, so $p^\\star \\le \\hat{p}^\\star$. \n",
    "\n",
    "With the arguements in the above two paragraphs combined, we proved that $p^\\star = \\hat{p}^\\star$. This is good news, becaue the problem can be solved in this way: find the solution of the problem without last constraint by KKT conditions. If this solution happens to satisfies the last constraint as well, we are done. If it doesn't, modify it by the process mentioned above to make it satisfy the last constraint and the solution is obtained.\n",
    "\n",
    "Can we happily ignore the last constaint? Yes and no. It can be ignored in the sense that the solution obtained by KKT conditions will always satisfy the last constraint, as we are going to prove very soon. It cannot be ignored because the optimal solution without the last constraint is not always unique, and in that case, the solution produced by KKT is one of them, or the 'best' of them. Let's consider the case when $t$ is sufficiently large, and in this case, the constaint on the norm of the parameters doesn't kick in, so all parameters are active. In this case the solution is just the full least square solution. If we pick any of the pair $\\beta_j^+$ and $\\beta_j^-$ and add the same positive constant to them, the problem is still optimal with all the constraints satisfied. \n",
    "\n",
    "We notice that the value of $\\beta$ doesn't change at all during such kind of shift of $\\beta_j^+$ and $\\beta_j^-$, but the solution is getting closer to the boundary where the first constraint is violated. So if we define the set of $\\beta_j^+$ and $\\beta_j^-$ producing the same $\\beta$ as being equivalent, then the solution of the problem without the last constraint defines an equivalent class, and we can choose the pair of $\\beta_j^+$ and $\\beta_j^-$ such that one of them is $0$ to represent this equivalent class. This representitive is, not suprisingly, the optimal solution of the problem with the last constraint, and it is the 'best' in the sense that it is most distant from the boundary of the first constraint (the norm constraint).\n",
    "\n",
    "The above discussion is for the purpose of rigorously proving that the KKT condition on the above optimazation problem without the last constraint can be used to find the optimal solution of the original problem. The Lagrangian is:\n",
    "$$L(\\lambda^{+},\\,\\lambda^{-},\\,\\lambda;\\,\\beta^{+},\\,\\beta^{-})=f\\left(\\mathbf{\\beta}^{+}-\\mathbf{\\beta}^{-}\\right)+\\lambda\\left(\\mathbf{1}^{T}\\left(\\mathbf{\\beta}^{+}+\\mathbf{\\beta}^{-}\\right)-t\\right)-\\sum_{j=1}^{p}\\lambda_{j}^{+}\\beta_{j}^{+}-\\sum_{j=1}^{p}\\lambda_{j}^{-}\\beta_{j}^{-}$$\n",
    "where \n",
    "$$f(\\beta)=f\\left(\\mathbf{\\beta}^{+}-\\mathbf{\\beta}^{-}\\right)=\\frac12\\left(\\mathbf{y}-\\mathbf{X\\beta}\\right)^{T}\\left(\\mathbf{y}-\\mathbf{X\\beta}\\right)$$\n",
    "is the objective function.\n",
    "\n",
    "Let's list the itemized KKT conditions. Stationary condition:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial\\beta_{j}^{+}} & =\\nabla f\\left(\\mathbf{\\beta}^{+}-\\mathbf{\\beta}^{-}\\right)_{j}+\\lambda-\\lambda_{j}^{+}=0\\\\\n",
    "\\frac{\\partial L}{\\partial\\beta_{j}^{-}} & =-\\nabla f\\left(\\mathbf{\\beta}^{+}-\\mathbf{\\beta}^{-}\\right)_{j}+\\lambda-\\lambda_{j}^{-}=0\n",
    "\\end{align*}\n",
    "\n",
    "Complementary slackness:\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda\\left(\\mathbf{1}^{T}\\left(\\mathbf{\\beta}^{+}+\\mathbf{\\beta}^{-}\\right)-t\\right) & =0\\\\\n",
    "\\lambda_{j}^{+}\\beta_{j}^{+} & =0\\\\\n",
    "\\lambda_{j}^{-}\\beta_{j}^{-} & =0\n",
    "\\end{align*}\n",
    "\n",
    "Dual feasible:\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda & \\ge0\\\\\n",
    "\\lambda_{j}^{+} & \\ge0\\\\\n",
    "\\lambda_{j}^{-} & \\ge0\n",
    "\\end{align*}\n",
    "\n",
    "## b)\n",
    "When $\\lambda = 0$, with the stationary condition we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla f\\left(\\mathbf{\\beta}\\right)_{j} & =\\lambda_{j}^{+}\\ge0\\\\\n",
    "\\nabla f\\left(\\mathbf{\\beta}\\right)_{j} & =-\\lambda_{j}^{-}\\le0\n",
    "\\end{align*}\n",
    "\n",
    "So $\\nabla f\\left(\\mathbf{\\beta}\\right)_{j} = 0$.\n",
    "\n",
    "When $\\lambda > 0$ and $\\beta_{j}^{+} > 0$, by complementary slackness, we have $\\lambda_{j}^{+} = 0$. By first stationary condition we have: $\\nabla f\\left(\\mathbf{\\beta}\\right)_{j}=-\\lambda$. Then we plug it into the second stationary condition to have: $\\lambda_{j}^{-}=2\\lambda>0$. With third complementary slackness condition we have: $\\beta_{j}^{-} = 0$.\n",
    "\n",
    "When $\\lambda > 0$ and $\\beta_{j}^{-} > 0$, by complementary slackness, we have $\\lambda_{j}^{-} = 0$. By second stationary condition we have: $\\nabla f\\left(\\mathbf{\\beta}\\right)_{j}=\\lambda$. Then we plug it into the first stationary condition to have: $\\lambda_{j}^{+}=2\\lambda>0$. With second complementary slackness condition we have: $\\beta_{j}^{+} = 0$.\n",
    "\n",
    "As we can see, the value of $\\nabla f\\left(\\mathbf{\\beta}\\right)_{j}$ is $0$, or $\\pm\\lambda$, so $|\\nabla f\\left(\\mathbf{\\beta}\\right)_{j}| \\le \\lambda$. \n",
    "\n",
    "## c)\n",
    "The gradient of objective function with respect to $\\beta$ is: \n",
    "$\\nabla f\\left(\\mathbf{\\beta}\\right)=\\mathbf{X}^{T}\\mathbf{X\\beta}-\\mathbf{X}^{T}\\mathbf{y}$. Let's denote $\\hat{\\mathbf{X}}$ as the $N\\times q$ matrix of active regressors, and $\\mathbf{e}$ as the $q\\times1$ colume vector with its entries taking values of only $\\pm1$. Then if from $\\lambda$ to $\\lambda_0$, the active predictors are the same, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{X}}^{T}\\mathbf{\\hat{X}\\hat{\\beta}(\\lambda)}-\\hat{\\mathbf{X}}^{T}\\mathbf{y} & =\\lambda\\mathbf{e}\\\\\n",
    "\\hat{\\mathbf{X}}^{T}\\mathbf{\\hat{X}\\hat{\\beta}(\\lambda_{0})}-\\hat{\\mathbf{X}}^{T}\\mathbf{y} & =\\lambda_{0}\\mathbf{e}\n",
    "\\end{align*}\n",
    "\n",
    "Subtracting the second equation from the first we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{X}}^{T}\\mathbf{\\hat{X}\\left(\\hat{\\beta}(\\lambda)-\\hat{\\beta}(\\lambda_{0})\\right)} & =\\left(\\lambda-\\lambda_{0}\\right)\\mathbf{e}\\\\\n",
    "\\Rightarrow\\hat{\\beta}(\\lambda)-\\hat{\\beta}(\\lambda_{0}) & =\\left(\\hat{\\mathbf{X}}^{T}\\hat{\\mathbf{X}}\\right)^{-1}\\left(\\lambda-\\lambda_{0}\\right)\\mathbf{e}\\\\\n",
    "\\Rightarrow\\hat{\\beta}(\\lambda) & =\\hat{\\beta}(\\lambda_{0})+\\left(\\lambda-\\lambda_{0}\\right)\\left(\\hat{\\mathbf{X}}^{T}\\hat{\\mathbf{X}}\\right)^{-1}\\mathbf{e}\\\\\n",
    "\\Rightarrow\\exists\\gamma_{0} & =-\\left(\\hat{\\mathbf{X}}^{T}\\hat{\\mathbf{X}}\\right)^{-1}\\mathbf{e}\\\\\n",
    "\\mbox{such that }\\hat{\\beta}(\\lambda) & =\\hat{\\beta}(\\lambda_{0})-\\left(\\lambda-\\lambda_{0}\\right)\\gamma_{0}.\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
